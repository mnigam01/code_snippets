{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Mean\n",
    "\n",
    "The mean is the sum of all values divided by the number of values.\n",
    "\n",
    "```python\n",
    "# Example: Mean calculation\n",
    "data = [3, 5, 1, 2]\n",
    "mean = sum(data) / len(data)\n",
    "print(mean)  # Output: 2.75\n",
    "```\n",
    "\n",
    "#### Trimmed Mean\n",
    "\n",
    "The trimmed mean eliminates the influence of extreme values.\n",
    "\n",
    "```python\n",
    "# Example: Trimmed Mean calculation\n",
    "sorted_data = sorted([3, 5, 1, 2])\n",
    "p = 1  # Number of values to trim from each end\n",
    "trimmed_mean = sum(sorted_data[p:-p]) / (len(sorted_data) - 2 * p)\n",
    "print(trimmed_mean)  # Output: 2.5\n",
    "```\n",
    "\n",
    "#### Weighted Mean\n",
    "\n",
    "The weighted mean multiplies each data value by a weight and divides by the sum of the weights.\n",
    "\n",
    "```python\n",
    "# Example: Weighted Mean calculation\n",
    "data = [3, 5, 1, 2]\n",
    "weights = [1, 2, 1, 1]\n",
    "weighted_mean = sum(w * d for w, d in zip(weights, data)) / sum(weights)\n",
    "print(weighted_mean)  # Output: 3.0\n",
    "```\n",
    "\n",
    "#### Median and Robust Estimates\n",
    "\n",
    "The median is the middle number on a sorted list of the data.\n",
    "\n",
    "```python\n",
    "# Example: Median calculation\n",
    "data = [3, 5, 1, 2]\n",
    "sorted_data = sorted(data)\n",
    "median = (sorted_data[1] + sorted_data[2]) / 2\n",
    "print(median)  # Output: 2.5\n",
    "```\n",
    "\n",
    "# Chapter 1: Exploratory Data Analysis - Summary\n",
    "\n",
    "## 2. Elements of Structured Data\n",
    "\n",
    "Data comes in various forms (sensor readings, events, text, images, videos).  Much is unstructured, requiring processing into structured forms like tables for statistical analysis.  Structured data is categorized into:\n",
    "\n",
    "* **Numeric:**\n",
    "    * **Continuous:**  Values within an interval (e.g., wind speed).\n",
    "    * **Discrete:** Integer values (e.g., event counts).\n",
    "* **Categorical:** A fixed set of values (e.g., tv screen types or country names).\n",
    "    * **Binary:** Two categories (e.g., 0/1, true/false).\n",
    "    * **Ordinal:** Categories with an order (e.g., rating scales).\n",
    "\n",
    "Explicitly identifying data types helps software optimize processing and statistical procedures.  In Python, `pandas` allows explicit categorical data type specification.\n",
    "\n",
    "In Python, scikit-learn supports ordinal data with the sklearn.preprocessing.OrdinalEncoder.\n",
    "\n",
    "## 3. Rectangular Data\n",
    "\n",
    "Rectangular data, represented as a two-dimensional matrix (data frame in Python), is the standard format for data analysis and modeling.  Rows represent records (cases, observations), and columns represent features (attributes, variables). Unstructured Data often needs transformation into rectangular form.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Data frame:** The basic rectangular data structure with rows and columns.\n",
    "* **Feature:** A column.\n",
    "* **Outcome:** The variable to be predicted (dependent variable).\n",
    "* **Record:** A row.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Data Frames and Indexes\n",
    "\n",
    "In Python, `pandas` DataFrames have a default integer index, but multilevel indexes can be created for efficiency.\n",
    "In pandas, it is also possible to set multilevel/hierarchical indexes to improve the efficiency of certain operations.\n",
    "\n",
    "## 5. Terminology Differences\n",
    "\n",
    "Different fields use varying terminology for the same concepts (e.g., \"predictor variables\" vs. \"features\").\n",
    "\n",
    "\n",
    "## 6. Nonrectangular Data Structures\n",
    "\n",
    "Other data structures include:\n",
    "\n",
    "Time series data records successive measurements of the same variable. It is the raw material for statistical forecasting methods, and it is also a key component of the data produced by devices—the Internet of Things.\n",
    "Spatial data structures, which are used in mapping and location analytics, are more complex and varied than rectangular data structures. In the object representation, the focus of the data is an object (e.g., a house) and its spatial coordinates. The field view, by contrast, focuses on small units of space and the value of a relevant metric (pixel brightness, for example).\n",
    "Graph (or network) data structures are used to represent physical, social, and abstract relationships. For example, a graph of a social network, such as Facebook or LinkedIn, may represent connections between people on the network. Distribution hubs connected by roads are an example of a physical network. Graph structures are useful for certain types of problems, such as network optimization and recommender systems\n",
    "\n",
    "\n",
    "## 7. Estimates of Location\n",
    "\n",
    "an estimate of where most of the data is located (i.e., its central tendency).\n",
    "\n",
    "* **Mean:** The average value.\n",
    "* **Weighted mean:**  The sum of all values times a weight divided by the sum of the weights.\n",
    "* **Median:** The middle value in a sorted dataset. or The value such that one-half of the data lies above and below.\n",
    "* **Weighted median:** Accounts for different weights or The value such that one-half of the sum of the weights lies above and below the sorted data.\n",
    "* **Trimmed mean:**  Average after removing extreme values.\n",
    "* **Outlier:** A value significantly different from most others.\n",
    "\n",
    "percentile: The value such that P percent of the data lies below.\n",
    "robust : Not sensitive to extreme values.\n",
    "\n",
    "You will encounter the symbol x ¯  (pronounced “x-bar”) being used to represent the mean of a sample from a population\n",
    "\n",
    "The mean is easily calculated but sensitive to outliers.  The median and trimmed mean are more robust.\n",
    "\n",
    "N (or n) refers to the total number of records or observations. In statistics it is capitalized if it is referring to a population, and lowercase if it refers to a sample from a population. In data science, that distinction is not vital, so you may see it both ways.\n",
    "\n",
    "weighted mean, which you calculate by multiplying each data value xi  by a user-specified weight wi and dividing their sum by the sum of the weights.\n",
    "\n",
    "There are two main motivations for using a weighted mean:\n",
    "• Some values are intrinsically more variable than others, and highly variable\n",
    "observations are given a lower weight. For example, if we are taking the average\n",
    "from multiple sensors and one of the sensors is less accurate, then we might\n",
    "downweight the data from that sensor.\n",
    "• The data collected does not equally represent the different groups that we are\n",
    "interested in measuring. For example, because of the way an online experiment\n",
    "was conducted, we may not have a set of data that accurately reflects all groups in\n",
    "the user base. To correct that, we can give a higher weight to the values from the\n",
    "groups that were underrepresented.\n",
    "\n",
    "As with the median, we first sort the data, although each data value\n",
    "has an associated weight. Instead of the middle number, the weighted median is a\n",
    "value such that the sum of the weights is equal for the lower and upper halves of the\n",
    "sorted list. Like the median, the weighted median is robust to outliers.\n",
    "\n",
    "**Python Code Examples:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from scipy.stats import trim_mean\n",
    "import numpy as np\n",
    "import wquantiles\n",
    "\n",
    "# Load data\n",
    "state = pd.read_csv('state.csv')\n",
    "\n",
    "# Mean\n",
    "state['Population'].mean()\n",
    "\n",
    "# Trimmed mean\n",
    "trim_mean(state['Population'], 0.1)    # what is this i don't know  remove some examples from sorted array and take mean of rest.\n",
    "\n",
    "# Median\n",
    "state['Population'].median()\n",
    "\n",
    "# Weighted mean (murder rate, weighted by population)\n",
    "np.average(state['Murder.Rate'], weights=state['Population'])\n",
    "\n",
    "# Weighted median (murder rate, weighted by population)\n",
    "wquantiles.median(state['Murder.Rate'], weights=state['Population'])\n",
    "\n",
    "```\n",
    "\n",
    "**Note:**  The code requires the `pandas`, `scipy`, `numpy` and `wquantiles` libraries.  The `state.csv` file should contain the data shown in Table 1-2.  This code directly addresses the example provided in the text and includes the necessary imports and functions.\n",
    "\n",
    ", outliers are often the result of data errors such as mixing data of different\n",
    "units (kilometers versus meters) or bad readings from a sensor. When outliers are the\n",
    "result of bad data, the mean will result in a poor estimate of location, while the\n",
    "median will still be valid. In any case, outliers should be identified and are usually\n",
    "worthy of further investigation.\n",
    "\n",
    "===========================================================================\n",
    "\n",
    "summary 2\n",
    "\n",
    "## Estimates of Variability\n",
    "\n",
    "Location is just one dimension in summarizing a feature. A second dimension, variability, also referred to as dispersion, measures whether the data values are tightly clustered or spread out.\n",
    "\n",
    "### Key Terms for Variability Metrics\n",
    "\n",
    "- **Deviations**: Differences between observed values and the estimate of location. Synonyms: errors, residuals.\n",
    "- **Variance**: Sum of squared deviations from the mean divided by \\( n - 1 \\), where \\( n \\) is the number of data values. Synonym: mean-squared-error.\n",
    "- **Standard Deviation**: Square root of the variance.\n",
    "- **Mean Absolute Deviation**: Mean of the absolute values of the deviations from the mean. Synonyms: l1-norm, Manhattan norm.\n",
    "- **Median Absolute Deviation from the Median**: Median of the absolute values of the deviations from the median.\n",
    "- **Range**: Difference between the largest and smallest value in a data set.\n",
    "- **Order Statistics**: Metrics based on sorted data values. Synonym: ranks.\n",
    "- **Percentile**: Value such that \\( P \\) percent of the values take on this value or less and \\( (100 - P) \\) percent take on this value or more. Synonym: quantile.\n",
    "- **Interquartile Range (IQR)**: Difference between the 75th percentile and the 25th percentile.\n",
    "\n",
    "\n",
    "## Standard Deviation and Related Estimates\n",
    "\n",
    "Variability is measured by examining deviations from a central tendency (mean or median).  Averaging deviations directly is uninformative (positive and negative deviations cancel).  Therefore, alternative approaches are used:\n",
    "\n",
    "* **Mean Absolute Deviation:** Averages the absolute values of the deviations from the mean. \n",
    "\n",
    "* **Variance and Standard Deviation:** Based on squared deviations.  Variance is the average of squared deviations (divided by n-1 for unbiased sample estimate). The standard deviation is the square root of the variance and is easier to interpret because it uses the original data's units.\n",
    "\n",
    "\n",
    "## Degrees of Freedom, and n or n – 1?\n",
    "\n",
    "consider n people and n balls, a person has to choose only one ball. first person has n choices but last person has no choice, so n-1 people have choices so degreee of freedom is n-1. \n",
    "\n",
    "Using (n-1) in the variance denominator instead of 'n' provides an *unbiased* estimate of the population variance.  The (n-1) accounts for the *degrees of freedom*, reflecting the constraint of using the sample mean in the calculation.  For large datasets, the difference is negligible.\n",
    "\n",
    "## Robust Estimates of Variability\n",
    "\n",
    "The variance and standard deviation are sensitive to outliers.  A more robust alternative is the **Median Absolute Deviation (MAD)**:\n",
    "\n",
    "* **MAD Formula:** `Median(|x1 - m|, |x2 - m|, ..., |xn - m|)` where 'm' is the median. # so median of absolute error.\n",
    "\n",
    "The variance, the standard deviation, the mean absolute deviation,\n",
    "and the median absolute deviation from the median are not equiv‐\n",
    "alent estimates, even in the case where the data comes from a nor‐\n",
    "mal distribution. In fact, the standard deviation is always greater\n",
    "than the mean absolute deviation, which itself is greater than the\n",
    "median absolute deviation. Sometimes, the median absolute devia‐\n",
    "tion is multiplied by a constant scaling factor to put the MAD on\n",
    "the same scale as the standard deviation in the case of a normal dis‐\n",
    "tribution. The commonly used factor of 1.4826 means that 50% of\n",
    "the normal distribution fall within the range ±MAD\n",
    "\n",
    "## Estimates Based on Percentiles\n",
    "Statistics based on sorted (ranked) data are referred to as order statistics.\n",
    "This section describes variability estimation using order statistics (sorted data):\n",
    "\n",
    "* **Range:** The difference between the maximum and minimum values.  Extremely sensitive to outliers.\n",
    "* **Percentiles (Quantiles):**  The Pth percentile is a value where at least P% of the data is less than or equal to it, and at least (100-P)% is greater than or equal to it.\n",
    "\n",
    "* **Interquartile Range (IQR):** The difference between the 75th and 25th percentiles. A robust measure of spread.\n",
    "\n",
    "**Example in Python (using pandas):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.Series([3, 1, 5, 3, 6, 7, 2, 9])\n",
    "sorted_data = data.sort_values()\n",
    "percentile_25 = sorted_data.quantile(0.25)  #2.5\n",
    "percentile_75 = sorted_data.quantile(0.75)  #6.5\n",
    "iqr = percentile_75 - percentile_25  #4.0\n",
    "print(f\"IQR: {iqr}\")\n",
    "```\n",
    "\n",
    "## Example: Variability Estimates of State Population\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.robust.scale import mad #you need to install statsmodels package\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = {'State': ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California'],\n",
    "        'Population': [4779736, 710231, 6392017, 2915918, 37253956]}\n",
    "state = pd.DataFrame(data)\n",
    "\n",
    "std_dev = state['Population'].std()\n",
    "iqr = state['Population'].quantile(0.75) - state['Population'].quantile(0.25)\n",
    "mad_value = mad(state['Population'])\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev}\")  # sensitive to outliers\n",
    "print(f\"IQR: {iqr}\")\n",
    "print(f\"MAD: {mad_value}\")  # not sensitive to outliers\n",
    "```\n",
    "\n",
    "## Key Ideas (Variability)\n",
    "\n",
    "* Variance and standard deviation are commonly used but sensitive to outliers.\n",
    "* Mean absolute deviation and median absolute deviation from the median are more robust.\n",
    "* Percentiles offer insights into data spread at different percentages.\n",
    "\n",
    "## Exploring the Data Distribution\n",
    "\n",
    "This section covers methods to visualize data distributions:\n",
    "\n",
    "## Key Terms for Exploring the Distribution\n",
    "\n",
    "* **Boxplot:** A visual representation of data distribution using percentiles (quartiles, IQR, outliers).\n",
    "* **Frequency table:** Data values categorized into intervals (bins) with their counts.\n",
    "* **Histogram:** A bar chart visualizing a frequency table.\n",
    "* **Density plot:** A smoothed representation of a histogram, often using kernel density estimation.\n",
    "\n",
    "\n",
    "## Percentiles and Boxplots\n",
    "\n",
    "Percentiles (quartiles, deciles) summarize data distribution. Boxplots visualize this using percentiles:  the box represents the interquartile range, the line inside the box is the median, and whiskers extend to show the range of the data (typically up to 1.5 times the IQR). Points beyond the whiskers are considered outliers.\n",
    "\n",
    "state['Murder.Rate'].quantile([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "\n",
    "## Frequency Tables and Histograms\n",
    "\n",
    "Frequency tables and histograms summarize data by grouping values into bins.  Histograms visualize frequency tables, showing counts or proportions for each bin.\n",
    "\n",
    "It is important to include the empty bins; the fact that there are no values in those bins is useful information. It can also be useful to experiment with different bin sizes. If they are too large, important features of the distribution can be obscured. If they are too small, the result is too granular, and the ability to see the bigger picture is lost.\n",
    "\n",
    "**Example in Python (using pandas):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = {'Population': [4779736, 710231, 6392017, 2915918, 37253956, 5029196, 3574097, 897934]}\n",
    "state = pd.DataFrame(data)\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(state['Population'], bins=5) #Adjust number of bins as needed\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of State Populations')\n",
    "plt.show()\n",
    "\n",
    "#Create frequency table using cut and value_counts\n",
    "bins = pd.cut(state['Population'], bins=5)\n",
    "frequency_table = bins.value_counts()\n",
    "print(frequency_table)\n",
    "\n",
    "# Create a boxplot\n",
    "state['Population'].plot.box()\n",
    "plt.ylabel('Population')\n",
    "plt.title('Boxplot of State Populations')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Statistical Moments\n",
    "\n",
    "In statistical theory, location and variability are referred to as the first and second moments of a distribution. The third and fourth moments are called skewness and kurtosis. Skewness refers to whether the data is skewed to larger or smaller values, and kurtosis indicates the propensity of the data to have extreme values. \n",
    "\n",
    "Beyond location and variability (first and second moments), skewness (third moment) and kurtosis (fourth moment) describe data asymmetry and tail weight respectively.  These are usually analyzed visually (histograms, boxplots).\n",
    "\n",
    "\n",
    "## Density Plots and Estimates\n",
    "\n",
    "Density plots provide a smoothed view of data distributions.  They are often created using kernel density estimation.\n",
    "\n",
    "**Example in Python (using pandas):**\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # you will need seaborn installed\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = {'Murder.Rate': [5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8]}\n",
    "state = pd.DataFrame(data)\n",
    "\n",
    "# Create a density plot using seaborn\n",
    "sns.kdeplot(state['Murder.Rate'])\n",
    "plt.xlabel('Murder Rate')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Murder Rates')\n",
    "plt.show()\n",
    "\n",
    "# Create histogram with density plot overlay\n",
    "plt.hist(state['Murder.Rate'], density=True, alpha=0.5) #alpha adds some transparency\n",
    "sns.kdeplot(state['Murder.Rate'])\n",
    "plt.xlabel('Murder Rate')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram with Density Overlay')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "the total area under the density curve = 1, and instead of counts in bins you calculate areas under the curve between any two points on the x-axis, which correspond to the proportion of the distribution lying between those two points.\n",
    "\n",
    "## Key Ideas (Data Distribution)\n",
    "\n",
    "* Histograms and frequency tables show data distribution by grouping into bins.\n",
    "* Boxplots provide a concise visual summary using percentiles.\n",
    "* Density plots offer a smoothed view of the distribution.\n",
    "\n",
    "=========================================================================\n",
    "\n",
    "\n",
    "summary 2\n",
    "\n",
    "## Exploring Binary and Categorical Data\n",
    "\n",
    "This section explores methods for summarizing and visualizing categorical data, including binary data (e.g., yes/no).  It also introduces the concept of expected value and touches upon probability.\n",
    "\n",
    "### Key Terms for Exploring Categorical Data\n",
    "\n",
    "* **Mode:** The most frequent category.\n",
    "* **Expected Value:**  When the categories can be associated with a numeric value, this gives an average value based on a category’s probability of occurrence.\n",
    "* **Bar Charts:** Visual representation of category frequencies or proportions using bars.\n",
    "* **Pie Charts:** Visual representation of category proportions using pie slices.\n",
    "\n",
    "\n",
    "### Summarizing Categorical Data\n",
    "\n",
    "Summarizing categorical data often involves calculating proportions or percentages for each category.  For example, Table 1-6 (shown below) displays the percentage of flight delays at Dallas/Fort Worth Airport categorized by cause.\n",
    "\n",
    "**Table 1-6. Percentage of delays by cause at Dallas/Fort Worth Airport**\n",
    "\n",
    "| Cause          | Percentage |\n",
    "|-----------------|------------|\n",
    "| Carrier         | 23.02      |\n",
    "| ATC             | 30.40      |\n",
    "| Weather         | 4.03       |\n",
    "| Security        | 0.12       |\n",
    "| Inbound Aircraft| 42.43      |\n",
    "\n",
    "\n",
    "### Visualizing Categorical Data: Bar Charts and Pie Charts\n",
    "\n",
    "Bar charts are commonly used to visualize categorical data.  Categories are on the x-axis, and frequencies or proportions are on the y-axis.\n",
    "\n",
    "\n",
    "Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale. In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data. In a bar chart, the bars are shown separate from one another.\n",
    "\n",
    "**Python Code for Bar Charts (using pandas):**\n",
    "\n",
    "```python\n",
    "# Assuming 'dfw' is a pandas DataFrame with delay causes and counts\n",
    "ax = dfw.transpose().plot.bar(figsize=(4, 4), legend=False) # Creates the bar chart\n",
    "ax.set_xlabel('Cause of delay')\n",
    "ax.set_ylabel('Count')\n",
    "```\n",
    "\n",
    "Pie charts are an alternative, but less informative than bar charts according to data visualization experts.\n",
    "\n",
    "\n",
    "### Numerical Data as Categorical Data\n",
    "\n",
    "Numeric data can be converted to categorical data through binning (grouping into ranges).  This simplifies the data and can aid in identifying relationships between variables.  Histograms and bar charts are similar in this regard, but bar charts use unordered categories on the x-axis.\n",
    "\n",
    "\n",
    "### Mode\n",
    "\n",
    "The mode is the most frequent value in a dataset or values in case of tie.  \n",
    "\n",
    "\n",
    "### Expected Value\n",
    "\n",
    "The expected value is a weighted average of possible outcomes, where the weights are their probabilities.\n",
    "\n",
    "A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month. The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service, and 80% will not sign up for anything. This data can be summed up, for financial purposes, in a single “expected value,” which is a form of weighted mean, in which the weights are probabilities.\n",
    "\n",
    "The expected value is calculated as follows:\n",
    "1.Multiply each outcome by its probability of occurrence.\n",
    "2.Sum these values.\n",
    "\n",
    "**Example:** A cloud service provider has two plans: $300/month (5% probability) and $50/month (15% probability).  The expected value per customer is:\n",
    "\n",
    "```\n",
    "EV = (0.05 * 300) + (0.15 * 50) + (0.80 * 0) = $22.50\n",
    "```\n",
    "\n",
    "\n",
    "### Probability\n",
    "\n",
    "Probability is the likelihood of an event occurring, often expressed as a proportion or percentage.\n",
    "\n",
    "\n",
    "### Correlation\n",
    "\n",
    "This section discusses methods for measuring and visualizing the relationship between numerical variables.\n",
    "\n",
    "### Key Terms for Correlation\n",
    "\n",
    "* **Correlation Coefficient:**  A measure of the linear association between two numerical variables, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation). 0 indicates no linear correlation.\n",
    "* **Correlation Matrix:** A table showing the correlation coefficients between all pairs of variables.\n",
    "* **Scatterplot:** A plot showing the relationship between two numerical variables; each point represents a data record.\n",
    "\n",
    "the correlation coefficient, which gives an esti‐\n",
    "mate of the correlation between two variables that always lies on the same scale. To\n",
    "compute Pearson’s correlation coefficient, we multiply deviations from the mean for\n",
    "variable 1 times those for variable 2, and divide by the product of the standard\n",
    "deviations. (n-1) degree of freedom\n",
    "\n",
    "\n",
    "**Pearson's Correlation Coefficient Formula:**\n",
    "\n",
    "```\n",
    "r = Σᵢ₌₁ⁿ (xᵢ - x̄)(yᵢ - ȳ) / ((n-1)sxsy) \n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "*  `xᵢ` and `yᵢ` are individual data points.\n",
    "*  `x̄` and `ȳ` are the means of x and y.\n",
    "*  `sx` and `sy` are the standard deviations of x and y.\n",
    "*  `n` is the number of data points.\n",
    "\n",
    "\n",
    "**Python code for Correlation Matrix and Heatmap (using seaborn and pandas):**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'etfs' is a pandas DataFrame of ETF returns\n",
    "\n",
    "# Correlation Matrix\n",
    "correlation_matrix = etfs.corr()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(correlation_matrix, vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True))\n",
    "```\n",
    "\n",
    "\n",
    "**Scatterplot (using pandas):**\n",
    "\n",
    "```python\n",
    "# Assuming 'telecom' is a pandas DataFrame with ATT and Verizon returns\n",
    "ax = telecom.plot.scatter(x='T', y='VZ', figsize=(4, 4), marker='$◯$')\n",
    "ax.set_xlabel('ATT (T)')\n",
    "ax.set_ylabel('Verizon (VZ)')\n",
    "ax.axhline(0, color='grey', lw=1)\n",
    "ax.axvline(0, color='grey', lw=1)\n",
    "\n",
    "```\n",
    "\n",
    "Like the mean and standard deviation, the correlation coefficient is sensitive to outli‐\n",
    "ers in the data\n",
    "\n",
    "Other correlation estimates exist (Spearman's rho, Kendall's tau) These are correla‐\n",
    "tion coefficients based on the rank of the data. Since they work\n",
    "with ranks rather than values, these estimates are robust to outliers\n",
    "and can handle certain types of nonlinearities, but Pearson's and robust alternatives are generally sufficient for exploratory analysis. \n",
    "\n",
    "\n",
    "### Exploring Two or More Variables\n",
    "\n",
    "This section covers visualizing relationships between multiple variables.\n",
    "Familiar estimators like mean and variance look at variables one at a time (univariate\n",
    "analysis). Correlation analysis (see “Correlation” on page 30) is an important method\n",
    "that compares two variables (bivariate analysis). In this section we look at additional\n",
    "estimates and plots, and at more than two variables (multivariate analysis).\n",
    "\n",
    "### Key Terms for Exploring Two or More Variables\n",
    "\n",
    "* **Contingency Table:** A table summarizing the counts of different combinations of categories across two or more categorical variables.\n",
    "* **Hexagonal Binning:**  A method for visualizing the density of points in a scatterplot by binning the data into hexagons.\n",
    "* **Contour Plot:** A topographical-like representation of the density of two numeric variables.\n",
    "* **Violin Plot:** Similar to a box plot but shows the probability density of the data.\n",
    "\n",
    "### Two Categorical Variables\n",
    "\n",
    "A useful way to summarize two categorical variables is a contingency table—a table of counts by category.\n",
    "\n",
    "#### Table 1-8: Contingency Table of Loan Grade and Status\n",
    "\n",
    "| Grade | Charged off | Current | Fully paid | Late | Total |\n",
    "|-------|-------------|---------|------------|------|-------|\n",
    "| **A** | 1562        | 50051   | 20408      | 469  | 72490 |\n",
    "| **B** | 5302        | 93852   | 31160      | 2056 | 132370|\n",
    "| **C** | 6023        | 88928   | 23147      | 2777 | 120875|\n",
    "| **D** | 5007        | 53281   | 13681      | 2308 | 74277 |\n",
    "| **E** | 2842        | 24639   | 5949       | 1374 | 34804 |\n",
    "| **F** | 1526        | 8444    | 2328       | 606  | 12904 |\n",
    "| **G** | 409         | 1990    | 643        | 199  | 3241  |\n",
    "| **Total**| 22671   | 321185  | 97316      | 9789 | 450961|\n",
    "\n",
    "#### Python Code for Contingency Table\n",
    "```python\n",
    "crosstab = lc_loans.pivot_table(index='grade', columns='status',\n",
    "                                aggfunc=lambda x: len(x), margins=True)\n",
    "\n",
    "df = crosstab.loc['A':'G',:].copy()\n",
    "df.loc[:,'Charged Off':'Late'] = df.loc[:,'Charged Off':'Late'].div(df['All'],\n",
    "                                                                    axis=0)\n",
    "df['All'] = df['All'] / sum(df['All'])\n",
    "perc_crosstab = df\n",
    "```\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Contingency tables can look only at counts, or they can also include column and total percentages.\n",
    "- Hexagonal binning and contour plots give a visual representation of a two-dimensional density.\n",
    "\n",
    "**Python code for Hexagonal Binning (using pandas):**\n",
    "\n",
    "```python\n",
    "# Assuming 'kc_tax0' is a pandas DataFrame with square footage and tax assessed value.\n",
    "\n",
    "ax = kc_tax0.plot.hexbin(x='SqFtTotLiving', y='TaxAssessedValue', gridsize=30, sharex=False, figsize=(5, 4))\n",
    "ax.set_xlabel('Finished Square Feet')\n",
    "ax.set_ylabel('Tax-Assessed Value')\n",
    "```\n",
    "\n",
    "**Python code for Contour Plot (using seaborn):**\n",
    "\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'kc_tax0' is a pandas DataFrame with square footage and tax assessed value.\n",
    "\n",
    "ax = sns.kdeplot(kc_tax0.SqFtTotLiving, kc_tax0.TaxAssessedValue, ax=ax)\n",
    "ax.set_xlabel('Finished Square Feet')\n",
    "ax.set_ylabel('Tax-Assessed Value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "These visualization techniques help to understand the relationships between variables in larger datasets where simple scatterplots are too dense.\n",
    "\n",
    "Heat maps, hexagonal binning, and contour plots all give\n",
    "a visual representation of a two-dimensional density. In this way, they are natural\n",
    "analogs to histograms and density plots.\n",
    "\n",
    "===============================================================================\n",
    "\n",
    "\n",
    "summary 2\n",
    "\n",
    "## Categorical and Numeric Data Visualization in Python\n",
    "\n",
    "This section explores visualizing the relationship between categorical and numeric data using boxplots and violin plots in Python.\n",
    "\n",
    "### Boxplots\n",
    "\n",
    "Boxplots offer a simple way to compare the distribution of a numeric variable across different categories. \n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "# Assuming 'airline_stats' is a Pandas DataFrame with 'airline' and 'pct_carrier_delay' columns.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Using pandas' built-in boxplot function\n",
    "airline_stats.boxplot(by='airline', column='pct_carrier_delay')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Daily % of Delayed Flights')\n",
    "plt.suptitle('')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#there is boxplot in matplotlib as well. \n",
    "```\n",
    "\n",
    "The example highlights that Alaska has fewer delays (lower quartile is higher) than American (whose lower quartile is higher than Alaska's upper quartile).\n",
    "\n",
    "### Violin Plots\n",
    "\n",
    "Violin plots enhance boxplots by displaying the probability density of the data at different values, providing a richer understanding of the distribution's shape.\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.violinplot(airline_stats.airline, airline_stats.pct_carrier_delay, inner='quartile', color='white')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Daily % of Delayed Flights')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "**Explanation:** This code uses the `seaborn` library to create a violin plot. The `inner='quartile'` argument adds a boxplot inside the violin, showing the quartiles. The resulting plot reveals the concentration of data points near zero for Alaska and Delta, a detail less apparent in the boxplot.\n",
    "\n",
    "\n",
    "### Visualizing Multiple Variables\n",
    "\n",
    "This section shows how to extend visualization techniques to handle more than two variables by using conditioning (faceting). The example analyzes the relationship between house square footage, tax-assessed value, and zip code.\n",
    "\n",
    "**Code:**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "zip_codes = [98188, 98105, 98108, 98126]\n",
    "kc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes),:]\n",
    "\n",
    "def hexbin(x, y, color, **kwargs):\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)\n",
    "\n",
    "g = sns.FacetGrid(kc_tax_zip, col='ZipCode', col_wrap=2)\n",
    "g.map(hexbin, 'SqFtTotLiving', 'TaxAssessedValue', extent=[0, 3500, 0, 700000])\n",
    "g.set_axis_labels('Finished Square Feet', 'Tax-Assessed Value')\n",
    "g.set_titles('Zip code {col_name:.0f}')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:** This code uses `seaborn.FacetGrid` to create a set of hexagonal binning plots, each conditioned on a specific zip code.  This reveals that tax-assessed values vary significantly across zip codes, explaining clusters seen in a simpler two-variable plot (not shown here, but referenced in the original text).  The `hexbin` function creates the hexagonal bin plots, efficiently visualizing the density of data points.  `FacetGrid` arranges these plots based on the `ZipCode` column, providing a clear comparison across different zip codes.\n",
    "\n",
    "\n",
    "**Key Ideas Summarized:**\n",
    "\n",
    "*   Boxplots and violin plots effectively visualize the relationship between one numeric and one categorical variable.\n",
    "*   Conditioning variables (faceting) extend visualizations to handle multiple variables by creating separate plots for each category of the conditioning variable.  This allows for deeper insight into data relationships.\n",
    "*   Hexagonal binning is a useful technique to visualize the density of points in a scatterplot, especially for large datasets.\n",
    "\n",
    "\n",
    "==================================================================================\n",
    "## Chapter 2. Data and Sampling Distributions\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The chapter discusses the importance of sampling in data science, even in the era of big data. Sampling is essential for working efficiently with a variety of data and minimizing bias. Predictive models are often developed and piloted with samples, and samples are used in various tests, such as comparing the effect of web page designs on clicks.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Data and Sampling Distributions**:\n",
    "  - **Population**: A large, defined set of data.\n",
    "  - **Sample**: A subset of data from a larger data set.\n",
    "  - **Sampling Procedure**: The process of drawing elements into a sample at random.\n",
    "  - **Empirical Distribution**: The distribution of sample data.\n",
    "\n",
    "- **Random Sampling and Sample Bias**:\n",
    "  - **Random Sampling**: Each member of the population has an equal chance of being chosen.\n",
    "  - **Sample Bias**: A sample that misrepresents the population.\n",
    "  - **Stratified Sampling**: Dividing the population into strata and randomly sampling from each stratum.\n",
    "  - **Bias**: Systematic error.\n",
    "\n",
    "- **Self-Selection Sampling Bias**:\n",
    "  - Bias in reviews due to self-selection of reviewers.\n",
    "\n",
    "- **Bias**:\n",
    "  - **Statistical Bias**: Systematic errors in measurement or sampling.\n",
    "  - **Random Error**: Errors due to random chance.\n",
    "\n",
    "- **Random Selection**:\n",
    "  - Methods to achieve representativeness, such as stratified sampling.\n",
    "\n",
    "- **Size Versus Quality**:\n",
    "  - Smaller samples can be better for reducing bias and allowing greater attention to data quality.\n",
    "  - Massive amounts of data are needed for sparse data problems, such as search queries.\n",
    "\n",
    "- **Sample Mean Versus Population Mean**:\n",
    "  - **x ¯**: Mean of a sample.\n",
    "  - **μ**: Mean of a population.\n",
    "\n",
    "- **Selection Bias**:\n",
    "  - Bias resulting from the way observations are selected.\n",
    "  - **Data Snooping**: Extensive hunting through data.\n",
    "  - **Vast Search Effect**: Bias from repeated data modeling.\n",
    "\n",
    "- **Regression to the Mean**:\n",
    "  - Extreme observations tend to be followed by more central ones.\n",
    "\n",
    "- **Sampling Distribution of a Statistic**:\n",
    "  - The distribution of a sample statistic over many samples.\n",
    "  - **Central Limit Theorem**: The tendency of the sampling distribution to take on a normal shape as sample size rises.\n",
    "  - **Standard Error**: The variability of a sample statistic over many samples.\n",
    "\n",
    "### Code Snippets\n",
    "\n",
    "#### Python Code for Sampling Distribution\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming loans_income is a predefined list of income values\n",
    "sample_data = pd.DataFrame({\n",
    "    'income': loans_income.sample(1000),\n",
    "    'type': 'Data',\n",
    "})\n",
    "sample_mean_05 = pd.DataFrame({\n",
    "    'income': [loans_income.sample(5).mean() for _ in range(1000)],\n",
    "    'type': 'Mean of 5',\n",
    "})\n",
    "sample_mean_20 = pd.DataFrame({\n",
    "    'income': [loans_income.sample(20).mean() for _ in range(1000)],\n",
    "    'type': 'Mean of 20',\n",
    "})\n",
    "results = pd.concat([sample_data, sample_mean_05, sample_mean_20])\n",
    "\n",
    "g = sns.FacetGrid(results, col='type', col_wrap=1, height=2, aspect=2)\n",
    "g.map(plt.hist, 'income', range=[0, 200000], bins=40)\n",
    "g.set_axis_labels('Income', 'Count')\n",
    "g.set_titles('{col_name}')\n",
    "```\n",
    "\n",
    "### Examples and Key Points\n",
    "\n",
    "- **Literary Digest Poll of 1936**:\n",
    "  - Predicted a victory of Alf Landon over Franklin Roosevelt.\n",
    "  - The sample was biased towards high socioeconomic status.\n",
    "\n",
    "- **George Gallup**:\n",
    "  - Conducted biweekly polls of just 2,000 people and accurately predicted a Roosevelt victory.\n",
    "\n",
    "- **Regression to the Mean**:\n",
    "  - Example: \"Rookie of the year, sophomore slump\" phenomenon in sports.\n",
    "  - Identified by Francis Galton in 1886.\n",
    "\n",
    "### Tips\n",
    "\n",
    "- **Data Quality**:\n",
    "  - Data quality involves completeness, consistency of format, cleanliness, and accuracy of individual data points.\n",
    "  - Statistics adds the notion of representativeness.\n",
    "\n",
    "- **Random Sampling**:\n",
    "  - Proper definition of an accessible population is key.\n",
    "  - Consider timing and stratification for representative samples.\n",
    "\n",
    "- **Data Snooping**:\n",
    "  - Extensive hunting through data can lead to misleading conclusions.\n",
    "  - Use holdout sets and target shuffling to validate performance.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **Sampling Procedures**:\n",
    "  - Ronald Fricker’s chapter “Sampling Methods for Online Surveys” in The SAGE Handbook of Online Research Methods, 2nd ed.\n",
    "\n",
    "- **Literary Digest Poll Failure**:\n",
    "  - Story on the Capital Century website.\n",
    "\n",
    "- **Selection Bias**:\n",
    "  - Christopher J. Pannucci and Edwin G. Wilkins’ article “Identifying and Avoiding Bias in Research” in Plastic and Reconstructive Surgery (August 2010).\n",
    "  - Michael Harris’s article “Fooled by Randomness Through Selection Bias” from the perspective of traders.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Key Ideas**:\n",
    "  - Random sampling remains important in data science.\n",
    "  - Bias occurs when measurements or observations are systematically in error.\n",
    "  - Data quality is often more important than data quantity.\n",
    "  - The sampling distribution of a statistic tells us how a metric would turn out differently from sample to sample.\n",
    "  - The central limit theorem and standard error are key concepts in understanding sampling distributions.\n",
    "\n",
    "This structured format ensures that all information is presented clearly and logically, maintaining the flow of the original text.\n",
    "\n",
    "\n",
    "\n",
    "summary 2\n",
    "\n",
    "# Chapter 2: Data and Sampling Distributions - Summary\n",
    "\n",
    "This chapter emphasizes the importance of sampling, even in the \"big data\" era, to ensure efficient data handling and minimize bias.  It covers random sampling, bias types, sampling distributions, and the central limit theorem.\n",
    "\n",
    "## 1. Random Sampling and Sample Bias\n",
    "\n",
    "**Key Concept:**  A sample is a smaller subset of a larger dataset (the population). Random sampling ensures every population member has an equal chance of selection.  This avoids **sample bias**, where the sample misrepresents the population in a systematic way (e.g., the Literary Digest poll).\n",
    "\n",
    "**Example:** The Literary Digest's 1936 poll failed because it sampled from a biased population (subscribers and car/telephone owners), leading to an inaccurate prediction. George Gallup's smaller, more representative sample gave a correct prediction.\n",
    "\n",
    "**Types of Sampling:**\n",
    "\n",
    "*   **Simple Random Sampling:** Each member has an equal chance of being selected.  Can be done with or without replacement.\n",
    "*   **Stratified Sampling:** Population is divided into subgroups (strata), and random samples are taken from each. Useful for ensuring representation of minority groups.\n",
    "\n",
    "**Data Quality:**  Data quality (completeness, consistency, cleanliness, accuracy, and representativeness) is crucial, often outweighing quantity.\n",
    "\n",
    "\n",
    "## 2. Self-Selection Sampling Bias\n",
    "\n",
    "**Key Concept:**  Self-selection bias occurs when individuals select themselves into the sample (e.g., Yelp reviews). Those motivated to participate might not represent the entire population. While unreliable for generalizing to the whole population, self-selection samples can be useful for *comparisons* between similar entities.\n",
    "\n",
    "\n",
    "## 3. Bias\n",
    "\n",
    "**Key Concept:** Bias refers to systematic errors in measurement or sampling, distinct from random error.  It often indicates a misspecified model or missing variables.\n",
    "\n",
    "\n",
    "## 4. Random Selection\n",
    "\n",
    "Proper definition of the accessible population and the sampling procedure are vital for avoiding bias.  Consider the challenges in defining “customer” for a customer survey (past customers, refunds, test purchases, etc.). Timing of sampling also matters (e.g., website traffic varies by time of day and week).\n",
    "\n",
    "\n",
    "## 5. Size Versus Quality\n",
    "\n",
    "Smaller, high-quality samples are often preferable to massive, low-quality datasets.  Random sampling reduces bias and allows for better data exploration and quality improvement.\n",
    "\n",
    "**Exception:** \"Big data\" is valuable when data is sparse (e.g., Google search queries).  The sheer volume allows for effective predictions even for infrequent search terms.\n",
    "\n",
    "\n",
    "## 6. Sample Mean Versus Population Mean\n",
    "\n",
    "x̄ represents the sample mean, while μ represents the population mean. The distinction is important because sample statistics are observed, while population parameters are often inferred from samples.\n",
    "\n",
    "\n",
    "## 7. Selection Bias\n",
    "\n",
    "**Key Concept:** Selection bias involves choosing data in a way that leads to misleading conclusions.  This includes data snooping (searching for patterns until something interesting is found) and the vast search effect (repeated modeling leading to spurious findings).\n",
    "\n",
    "**Mitigation:** Use a holdout set (or multiple sets) to validate model performance and protect against the vast search effect.  Target shuffling (permutation tests) can also help assess the validity of findings.\n",
    "\n",
    "\n",
    "## 8. Regression to the Mean\n",
    "\n",
    "**Key Concept:** Extreme observations tend to be followed by more central ones. This is a type of selection bias, where focusing on extreme values can lead to misinterpretations (e.g., \"rookie of the year, sophomore slump\").  It's caused by the combination of skill and luck influencing initial extreme performance; luck typically regresses towards the average in subsequent measurements.\n",
    "\n",
    "**Caution:** Regression to the mean is *not* the same as linear regression in statistical modeling.\n",
    "\n",
    "\n",
    "## 9. Sampling Distribution of a Statistic\n",
    "\n",
    "**Key Concept:** The sampling distribution of a statistic is the distribution of that statistic (e.g., mean) across many samples from the same population. It shows how much the statistic might vary from sample to sample.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "*   **Sample statistic:** A metric calculated from a sample (e.g., sample mean).\n",
    "*   **Data distribution:** The distribution of individual data values.\n",
    "*   **Sampling distribution:** The distribution of a sample statistic over many samples.\n",
    "*   **Central limit theorem:** The tendency of sampling distributions to become normal as sample size increases.\n",
    "*   **Standard error:** The variability (standard deviation) of a sample statistic over many samples.\n",
    "\n",
    "**Example (Python Code):**  The following Python code demonstrates the central limit theorem by showing how the distribution of sample means becomes more normal and narrower as sample size increases.  It uses the `seaborn` library for visualization.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'loans_income' is a Pandas Series containing income data (replace with your data)\n",
    "#For demonstration purposes:\n",
    "loans_income = pd.Series(range(10000))\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'income': loans_income.sample(1000),\n",
    "    'type': 'Data',\n",
    "})\n",
    "sample_mean_05 = pd.DataFrame({\n",
    "    'income': [loans_income.sample(5).mean() for _ in range(1000)],\n",
    "    'type': 'Mean of 5',\n",
    "})\n",
    "sample_mean_20 = pd.DataFrame({\n",
    "    'income': [loans_income.sample(20).mean() for _ in range(1000)],\n",
    "    'type': 'Mean of 20',\n",
    "})\n",
    "results = pd.concat([sample_data, sample_mean_05, sample_mean_20])\n",
    "\n",
    "g = sns.FacetGrid(results, col='type', col_wrap=1, height=2, aspect=2)\n",
    "g.map(plt.hist, 'income', range=[0, 200000], bins=40)\n",
    "g.set_axis_labels('Income', 'Count')\n",
    "g.set_titles('{col_name}')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "## 10. Central Limit Theorem\n",
    "\n",
    "The central limit theorem states that the distribution of sample means will approximate a normal distribution, regardless of the original population distribution, as the sample size increases. This is crucial for statistical inference (confidence intervals, hypothesis tests).  However, bootstrapping (not covered in this excerpt's python code) offers a modern, assumption-free alternative for estimating sampling distributions.\n",
    "\n",
    "\n",
    "## 11. Standard Error\n",
    "\n",
    "Standard error measures the variability of a sample statistic (e.g., the mean). A smaller standard error indicates a more precise estimate. It is inversely related to the sample size (square root of n rule: quadrupling sample size halves the standard error).  Bootstrapping is a preferred method for estimating standard error over methods relying on the central limit theorem in modern statistics.\n",
    "\n",
    "\n",
    "## 12. Standard Deviation Versus Standard Error\n",
    "\n",
    "Standard deviation measures the variability of *individual data points*, while standard error measures the variability of a *sample statistic*.  Do not confuse the two.\n",
    "\n",
    "========================================================================\n",
    "\n",
    "\n",
    "# The Bootstrap\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The bootstrap is a powerful statistical technique that estimates the sampling distribution of a statistic or model parameters by repeatedly drawing samples with replacement from the original data set. This method does not require any assumptions about the data or the sample statistic being normally distributed.\n",
    "\n",
    "## Key Terms\n",
    "\n",
    "- **Bootstrap Sample**: A sample taken with replacement from an observed data set.\n",
    "- **Resampling**: The process of taking repeated samples from observed data, including both bootstrap and permutation procedures.\n",
    "\n",
    "## Conceptual Explanation\n",
    "\n",
    "Imagine replicating the original sample thousands or millions of times to create a hypothetical population that embodies all the knowledge from the original sample. From this population, you can draw samples to estimate the sampling distribution.\n",
    "\n",
    "## Practical Algorithm\n",
    "\n",
    "1. Draw a sample value, record it, and then replace it.\n",
    "2. Repeat step 1 \\( n \\) times.\n",
    "3. Record the mean of the \\( n \\) resampled values.\n",
    "4. Repeat steps 1–3 \\( R \\) times.\n",
    "5. Use the \\( R \\) results to:\n",
    "   - Calculate their standard deviation (estimates sample mean standard error).\n",
    "   - Produce a histogram or boxplot.\n",
    "   - Find a confidence interval.\n",
    "\n",
    "The number of iterations \\( R \\) is set somewhat arbitrarily. The more iterations you do, the more accurate the estimate of the standard error or the confidence interval.\n",
    "\n",
    "## Example in R\n",
    "\n",
    "```R\n",
    "library(boot)\n",
    "stat_fun <- function(x, idx) median(x[idx])\n",
    "boot_obj <- boot(loans_income, R=1000, statistic=stat_fun)\n",
    "```\n",
    "\n",
    "The function `stat_fun` computes the median for a given sample specified by the index `idx`. The result is:\n",
    "\n",
    "```\n",
    "Bootstrap Statistics :\n",
    "    original   bias    std. error\n",
    "t1*    62000 -70.5595    209.1515\n",
    "```\n",
    "\n",
    "## Example in Python\n",
    "\n",
    "```python\n",
    "results = []\n",
    "for nrepeat in range(1000):\n",
    "    sample = resample(loans_income)\n",
    "    results.append(sample.median())\n",
    "results = pd.Series(results)\n",
    "print('Bootstrap Statistics:')\n",
    "print(f'original: {loans_income.median()}')\n",
    "print(f'bias: {results.mean() - loans_income.median()}')\n",
    "print(f'std. error: {results.std()}')\n",
    "```\n",
    "\n",
    "## Multivariate Bootstrap\n",
    "\n",
    "The bootstrap can also be applied to multivariate data, where the rows are sampled as units. A model might then be run on the bootstrapped data to estimate the stability (variability) of model parameters or to improve predictive power.\n",
    "\n",
    "## Historical Context\n",
    "\n",
    "The bootstrap was introduced by Bradley Efron in the late 1970s and early 1980s. It gained popularity among researchers who use statistics but are not statisticians, especially for metrics or models where mathematical approximations are not readily available.\n",
    "\n",
    "## Warning\n",
    "\n",
    "The bootstrap does not compensate for a small sample size; it does not create new data, nor does it fill in holes in an existing data set. It merely informs us about how lots of additional samples would behave when drawn from a population like our original sample.\n",
    "\n",
    "## Resampling Versus Bootstrapping\n",
    "\n",
    "- **Resampling**: Includes both bootstrap and permutation procedures.\n",
    "- **Bootstrap**: Always implies sampling with replacement from an observed data set.\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- The bootstrap is a powerful tool for assessing the variability of a sample statistic.\n",
    "- It can be applied in a wide variety of circumstances without extensive study of mathematical approximations to sampling distributions.\n",
    "- When applied to predictive models, aggregating multiple bootstrap sample predictions (bagging) outperforms the use of a single model.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- \"An Introduction to the Bootstrap\" by Bradley Efron and Robert Tibshirani (Chapman & Hall, 1993)\n",
    "- The retrospective on the bootstrap in the May 2003 issue of Statistical Science (vol. 18, no. 2)\n",
    "- \"An Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (Springer, 2013)\n",
    "\n",
    "# Confidence Intervals\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Confidence intervals provide a range within which a sample estimate is expected to fall with a certain level of confidence. They are a way to understand the potential error in a sample estimate.\n",
    "\n",
    "## Key Terms\n",
    "\n",
    "- **Confidence Level**: The percentage of confidence intervals, constructed in the same way from the same population, that are expected to contain the statistic of interest.\n",
    "- **Interval Endpoints**: The top and bottom of the confidence interval.\n",
    "\n",
    "## Conceptual Explanation\n",
    "\n",
    "Confidence intervals present an estimate as a range, grounded in statistical sampling principles. A 90% confidence interval encloses the central 90% of the bootstrap sampling distribution of a sample statistic.\n",
    "\n",
    "## Practical Algorithm\n",
    "\n",
    "1. Draw a random sample of size \\( n \\) with replacement from the data (a resample).\n",
    "2. Record the statistic of interest for the resample.\n",
    "3. Repeat steps 1–2 many (R) times.\n",
    "4. For an x% confidence interval, trim [(100-x) / 2]% of the R resample results from either end of the distribution.\n",
    "5. The trim points are the endpoints of an x% bootstrap confidence interval.\n",
    "\n",
    "## Example in Python\n",
    "\n",
    "```python\n",
    "results = []\n",
    "for nrepeat in range(1000):\n",
    "    sample = resample(loans_income)\n",
    "    results.append(sample.median())\n",
    "results = pd.Series(results)\n",
    "confidence_interval = results.quantile([0.025, 0.975])\n",
    "print(f'95% Confidence Interval: {confidence_interval}')\n",
    "```\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- Confidence intervals are the typical way to present estimates as an interval range.\n",
    "- The more data you have, the less variable a sample estimate will be.\n",
    "- The lower the level of confidence you can tolerate, the narrower the confidence interval will be.\n",
    "- The bootstrap is an effective way to construct confidence intervals.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- \"Introductory Statistics and Analytics: A Resampling Perspective\" by Peter Bruce (Wiley, 2014)\n",
    "- \"Statistics: Unlocking the Power of Data, 2nd ed.\" by Robin Lock and four other Lock family members (Wiley, 2016)\n",
    "- \"Modern Engineering Statistics\" by Thomas Ryan (Wiley, 2007)\n",
    "\n",
    "# Normal Distribution\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The normal distribution is a bell-shaped curve that is iconic in traditional statistics. It is essential for the development of mathematical formulas that approximate the distributions of sample statistics.\n",
    "\n",
    "## Key Terms\n",
    "\n",
    "- **Error**: The difference between a data point and a predicted or average value.\n",
    "- **Standardize**: Subtract the mean and divide by the standard deviation.\n",
    "- **z-score**: The result of standardizing an individual data point.\n",
    "- **Standard Normal**: A normal distribution with mean = 0 and standard deviation = 1.\n",
    "- **QQ-Plot**: A plot to visualize how close a sample distribution is to a specified distribution, e.g., the normal distribution.\n",
    "\n",
    "## Conceptual Explanation\n",
    "\n",
    "In a normal distribution, 68% of the data lies within one standard deviation of the mean, and 95% lies within two standard deviations.\n",
    "\n",
    "## Warning\n",
    "\n",
    "Most raw data is not normally distributed. The utility of the normal distribution derives from the fact that many statistics are normally distributed in their sampling distribution.\n",
    "\n",
    "## Standard Normal and QQ-Plots\n",
    "\n",
    "To compare data to a standard normal distribution, you subtract the mean and then divide by the standard deviation. This is also called normalization or standardization. The transformed value is termed a z-score.\n",
    "\n",
    "## Example in Python\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "norm_sample = stats.norm.rvs(size=100)\n",
    "stats.probplot(norm_sample, plot=ax)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "- The normal distribution was essential to the historical development of statistics.\n",
    "- While raw data is typically not normally distributed, errors often are, as are averages and totals in large samples.\n",
    "- To convert data to z-scores, you subtract the mean of the data and divide by the standard deviation; you can then compare the data to a normal distribution.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- \"An Introduction to the Bootstrap\" by Bradley Efron and Robert Tibshirani (Chapman & Hall, 1993)\n",
    "- \"An Introduction to Statistical Learning\" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (Springer, 2013)\n",
    "- \"Modern Engineering Statistics\" by Thomas Ryan (Wiley, 2007)\n",
    "\n",
    "\n",
    "\n",
    "summary 2\n",
    "\n",
    "\n",
    "## The Bootstrap\n",
    "\n",
    "**Concept:** The bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or model parameters.  It involves drawing multiple samples *with replacement* from the original dataset and recalculating the statistic for each resample. This creates a simulated population reflecting the characteristics of the original sample, allowing estimation of variability.  It's particularly useful when the data doesn't follow a normal distribution or when mathematical approximations for the sampling distribution are unavailable.\n",
    "\n",
    "\n",
    "**Python Implementation (Illustrative):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "loans_income = pd.Series([60000, 65000, 70000, 55000, 62000, 75000, 68000, 58000, 63000, 61000])\n",
    "\n",
    "results = []\n",
    "for nrepeat in range(1000):  # Number of bootstrap resamples\n",
    "    sample = resample(loans_income) #Resample the data with replacement\n",
    "    results.append(sample.median()) #Calculate the median for each resample\n",
    "\n",
    "results = pd.Series(results)\n",
    "\n",
    "print('Bootstrap Statistics:')\n",
    "print(f'original: {loans_income.median()}')\n",
    "print(f'bias: {results.mean() - loans_income.median()}')\n",
    "print(f'std. error: {results.std()}')\n",
    "\n",
    "```\n",
    "\n",
    "This code snippet demonstrates a bootstrap to estimate the median income's standard error and bias.  It generates 1000 bootstrap samples, calculates the median of each, and then computes the mean and standard deviation of those medians to estimate bias and standard error.\n",
    "\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "*   The bootstrap doesn't require assumptions about data distribution (unlike many traditional statistical methods).\n",
    "*   It's computationally intensive, especially with a large number of resamples (R).\n",
    "*   A larger R value leads to more accurate estimates.\n",
    "*   The bootstrap doesn't create new data; it uses existing data to understand its inherent variability.  It is particularly useful when the sample size is small.\n",
    "\n",
    "\n",
    "**Warning:**  The bootstrap doesn't compensate for small sample sizes.  It only reflects the variability inherent within the sample.\n",
    "\n",
    "\n",
    "## Confidence Intervals\n",
    "\n",
    "**Concept:** Confidence intervals provide a range of values likely containing the true population parameter (e.g., mean, median) with a specified level of confidence (e.g., 95%).\n",
    "\n",
    "\n",
    "**Bootstrap Confidence Interval Algorithm (Illustrative):**\n",
    "\n",
    "The algorithm is conceptually similar to the bootstrap itself but is focused on generating the confidence interval:\n",
    "\n",
    "1.  Draw numerous bootstrap samples with replacement.\n",
    "2.  Calculate the statistic (e.g., mean) for each sample.\n",
    "3.  Order the resulting statistics.\n",
    "4.  Trim a percentage of values from the lower and upper ends (the percentage depends on the desired confidence level).  For example, for a 90% confidence interval, trim 5% from each end.\n",
    "5.  The remaining values define the confidence interval range.\n",
    "\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "*   Higher confidence levels lead to wider intervals (more certainty).\n",
    "*   Smaller sample sizes lead to wider intervals (more uncertainty).\n",
    "*   Data scientists often use confidence intervals to communicate uncertainty around estimates.\n",
    "\n",
    "\n",
    "## Normal Distribution\n",
    "\n",
    "**Concept:** The normal distribution is a bell-shaped probability distribution. Many statistical methods rely on the assumption of normality (though this is increasingly less common with the rise of resampling methods).\n",
    "\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "*   **Standardization (z-score):** Transforming data by subtracting the mean and dividing by the standard deviation.  This puts the data on a standard normal distribution scale (mean=0, standard deviation=1).\n",
    "*   **QQ-Plot:** A graphical method for assessing whether sample data follows a specific distribution (often the normal distribution).  A diagonal line indicates a good fit.\n",
    "\n",
    "\n",
    "**Python for QQ-Plot:**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Generate a sample from a normal distribution\n",
    "norm_sample = stats.norm.rvs(size=100) \n",
    "\n",
    "# Create QQ-Plot\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "stats.probplot(norm_sample, plot=ax)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "*   The assumption of normality is less critical thanks to methods like the bootstrap.\n",
    "*   Standardization (z-scores) doesn't make data normally distributed; it transforms it to a standard scale for comparison.\n",
    "\n",
    "\n",
    "**Note:** The provided text also mentions further readings for each section.  These are valuable resources for a deeper understanding of each concept.\n",
    "\n",
    "========================================================================\n",
    "\n",
    "# Structured Summary of Technical Book Excerpt\n",
    "\n",
    "## Long-Tailed Distributions\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Tail**: The long narrow portion of a frequency distribution, where relatively extreme values occur at low frequency.\n",
    "- **Skew**: Where one tail of a distribution is longer than the other.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "Despite the importance of the normal distribution historically in statistics, data is generally not normally distributed. Data can be highly skewed or discrete, and both symmetric and asymmetric distributions may have long tails. The tails of a distribution correspond to the extreme values (small and large). Long tails are widely recognized in practical work, and Nassim Taleb's black swan theory predicts that anomalous events, such as a stock market crash, are much more likely to occur than would be predicted by the normal distribution.\n",
    "\n",
    "### Example: Stock Returns\n",
    "\n",
    "Stock returns often exhibit long-tailed distributions. Figure 2-12 shows the QQ-Plot for the daily stock returns for Netflix (NFLX). The corresponding Python code is:\n",
    "\n",
    "```python\n",
    "nflx = sp500_px.NFLX\n",
    "nflx = np.diff(np.log(nflx[nflx>0]))\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "stats.probplot(nflx, plot=ax)\n",
    "```\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Most data is not normally distributed.\n",
    "- Assuming a normal distribution can lead to underestimation of extreme events (\"black swans\").\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **The Black Swan, 2nd ed.**, by Nassim Nicholas Taleb (Random House, 2010)\n",
    "- **Handbook of Statistical Distributions with Applications, 2nd ed.**, by K. Krishnamoorthy (Chapman & Hall/CRC Press, 2016)\n",
    "\n",
    "## Student’s t-Distribution\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **n**: Sample size.\n",
    "- **Degrees of freedom**: A parameter that allows the t-distribution to adjust to different sample sizes, statistics, and numbers of groups.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "The t-distribution is a normally shaped distribution with thicker tails. It is used extensively in depicting distributions of sample statistics. The t-distribution is often called Student’s t because it was published in 1908 in Biometrika by W. S. Gosset under the name \"Student.\"\n",
    "\n",
    "### Example: Gosset's Experiment\n",
    "\n",
    "Gosset wanted to answer the question “What is the sampling distribution of the mean of a sample, drawn from a larger population?” He started with a resampling experiment and derived a function now known as Student’s t.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- The t-distribution is actually a family of distributions resembling the normal distribution but with thicker tails.\n",
    "- The t-distribution is widely used as a reference basis for the distribution of sample means, differences between two sample means, regression parameters, and more.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- The original W.S. Gosset paper as published in Biometrika in 1908 is available as a PDF.\n",
    "- A standard treatment of the t-distribution can be found in David Lane’s online resource.\n",
    "\n",
    "## Binomial Distribution\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Trial**: An event with a discrete outcome (e.g., a coin flip).\n",
    "- **Success**: The outcome of interest for a trial.\n",
    "- **Binomial**: Having two outcomes.\n",
    "- **Binomial trial**: A trial with two outcomes.\n",
    "- **Binomial distribution**: Distribution of number of successes in x trials.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "The binomial distribution is the frequency distribution of the number of successes (x) in a given number of trials (n) with specified probability (p) of success in each trial.\n",
    "\n",
    "### Example: Binomial Probabilities\n",
    "\n",
    "The R function `dbinom` calculates binomial probabilities. For example:\n",
    "\n",
    "```python\n",
    "stats.binom.pmf(2, n=5, p=0.1)\n",
    "stats.binom.cdf(2, n=5, p=0.1)\n",
    "```\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Binomial outcomes are important to model, since they represent, among other things, fundamental decisions (buy or don’t buy, click or don’t click, survive or die, etc.).\n",
    "- A binomial trial is an experiment with two possible outcomes: one with probability p and the other with probability 1 – p.\n",
    "- With large n, and provided p is not too close to 0 or 1, the binomial distribution can be approximated by the normal distribution.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Read about the “quincunx”, a pinball-like simulation device for illustrating the binomial distribution.\n",
    "- The binomial distribution is a staple of introductory statistics, and all introductory statistics texts will have a chapter or two on it.\n",
    "\n",
    "## Chi-Square Distribution\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Chi-Square Statistic**: A measure of the extent to which a set of observed values \"fits\" a specified distribution.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "The chi-square distribution is typically concerned with counts of subjects or items falling into categories. The chi-square statistic measures the extent of departure from what you would expect in a null model.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- The chi-square distribution is typically concerned with counts of subjects or items falling into categories.\n",
    "- The chi-square statistic measures the extent of departure from what you would expect in a null model.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- The chi-square distribution owes its place in modern statistics to the great statistician Karl Pearson and the birth of hypothesis testing.\n",
    "- For a more detailed exposition, see the section in this book on the chi-square test.\n",
    "\n",
    "## F-Distribution\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **F-Statistic**: The ratio of the variability among the group means to the variability within each group.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "The F-distribution is used with experiments and linear models involving measured data. The F-statistic compares variation due to factors of interest to overall variation.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- The F-distribution is used with experiments and linear models involving measured data.\n",
    "- The F-statistic compares variation due to factors of interest to overall variation.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- George Cobb’s Introduction to Design and Analysis of Experiments (Wiley, 2008) contains an excellent exposition of the decomposition of variance components, which helps in understanding ANOVA and the F-statistic.\n",
    "\n",
    "## Poisson and Related Distributions\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Lambda**: The rate (per unit of time or space) at which events occur.\n",
    "- **Poisson Distribution**: The frequency distribution of the number of events in sampled units of time or space.\n",
    "- **Exponential Distribution**: The frequency distribution of the time or distance from one event to the next event.\n",
    "- **Weibull Distribution**: A generalized version of the exponential distribution in which the event rate is allowed to shift over time.\n",
    "\n",
    "### Concepts\n",
    "\n",
    "#### Poisson Distributions\n",
    "\n",
    "The Poisson distribution tells us the distribution of events per unit of time or space when we sample many such units.\n",
    "\n",
    "#### Example: Generating Random Numbers\n",
    "\n",
    "The scipy function `stats.poisson.rvs` generates random numbers from a Poisson distribution:\n",
    "\n",
    "```python\n",
    "stats.poisson.rvs(2, size=100)\n",
    "```\n",
    "\n",
    "#### Exponential Distribution\n",
    "\n",
    "The exponential distribution models the distribution of the time between events.\n",
    "\n",
    "#### Example: Generating Random Numbers\n",
    "\n",
    "The scipy function `stats.expon.rvs` generates random numbers from an exponential distribution:\n",
    "\n",
    "```python\n",
    "stats.expon.rvs(0.2, size=100)\n",
    "```\n",
    "\n",
    "#### Weibull Distribution\n",
    "\n",
    "The Weibull distribution is used when the event rate changes over time.\n",
    "\n",
    "#### Example: Generating Random Numbers\n",
    "\n",
    "The scipy function `stats.weibull_min.rvs` generates random numbers from a Weibull distribution:\n",
    "\n",
    "```python\n",
    "stats.weibull_min.rvs(1.5, scale=5000, size=100)\n",
    "```\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- For events that occur at a constant rate, the number of events per unit of time or space can be modeled as a Poisson distribution.\n",
    "- You can also model the time or distance between one event and the next as an exponential distribution.\n",
    "- A changing event rate over time (e.g., an increasing probability of device failure) can be modeled with the Weibull distribution.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Modern Engineering Statistics by Thomas Ryan (Wiley, 2007) has a chapter devoted to the probability distributions used in engineering applications.\n",
    "- Read an engineering-based perspective on the use of the Weibull distribution.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In the era of big data, the principles of random sampling remain important when accurate estimates are needed. Random selection of data can reduce bias and yield a higher quality data set than would result from just using the conveniently available data. Knowledge of various sampling and data-generating distributions allows us to quantify potential errors in an estimate that might be due to random variation. At the same time, the bootstrap (sampling with replacement from an observed data set) is an attractive “one size fits all” method to determine possible error in sample estimates.\n",
    "\n",
    "### Tip\n",
    "\n",
    "The bell curve is iconic but perhaps overrated. George W. Cobb, the Mount Holyoke statistician, argued in a November 2015 editorial in the American Statistician that the “standard introductory course, which puts the normal distribution at its center, had outlived the usefulness of its centrality.”\n",
    "\n",
    "\n",
    "\n",
    "summary 2\n",
    "## Long-Tailed Distributions\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **Long-tailed distributions:**  Data distributions where extreme values occur more frequently than predicted by a normal distribution. This means there's a higher chance of observing unusual events (e.g., a stock market crash).\n",
    "*   **Skew:**  One tail of the distribution is longer than the other, indicating asymmetry.\n",
    "*   **Black Swan Theory:** Predicts the likelihood of unexpected, high-impact events is much higher than the normal distribution suggests.\n",
    "\n",
    "\n",
    "**Example: Netflix Stock Returns**\n",
    "\n",
    "The following Python code generates a QQ-plot to visualize whether Netflix stock returns follow a normal distribution.  A QQ-plot compares the quantiles of your data to the quantiles of a theoretical normal distribution. Points significantly deviating from the diagonal line indicate non-normality.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# Assuming 'sp500_px' is a pandas DataFrame containing stock prices\n",
    "nflx = sp500_px.NFLX  # Extract Netflix stock prices\n",
    "nflx = np.diff(np.log(nflx[nflx>0])) # Calculate log returns (adjusting for positive values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "stats.probplot(nflx, plot=ax)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code first extracts Netflix stock prices and then calculates the log returns (a common way to represent stock price changes) before plotting the data on a QQ-plot.  If the points substantially deviate from the diagonal line (which would be added using `abline` in R), it signifies that Netflix stock returns are not normally distributed.\n",
    "\n",
    "\n",
    "**Note:** Fitting distributions to data can be subjective, requiring both statistical and domain knowledge.\n",
    "\n",
    "\n",
    "## Student's t-Distribution\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **t-distribution:**  Similar to the normal distribution but with thicker tails, making it more robust to outliers.  It's used for analyzing sample statistics, especially when the sample size is small.\n",
    "*   **Degrees of freedom:**  A parameter that adjusts the t-distribution based on sample size.  Larger samples lead to t-distributions closer to the normal distribution.\n",
    "\n",
    "\n",
    "**Confidence Intervals:**\n",
    "\n",
    "A 90% confidence interval around a sample mean (x̄) is calculated as:\n",
    "\n",
    "`x̄ ± t_(n-1)(0.05) * s/√n`\n",
    "\n",
    "where:\n",
    "\n",
    "*   `t_(n-1)(0.05)` is the critical t-value with (n-1) degrees of freedom and a significance level of 0.05 (corresponding to a 90% confidence level).\n",
    "*   `s` is the sample standard deviation.\n",
    "*   `n` is the sample size.\n",
    "\n",
    "\n",
    "**Note:** While used in classical statistics, the t-distribution's importance is reduced in data science due to the prevalence of bootstrapping for uncertainty quantification.\n",
    "\n",
    "\n",
    "## Binomial Distribution\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **Binomial Distribution:** Models the probability of getting a certain number of \"successes\" in a fixed number of independent trials, each with two possible outcomes (success/failure).  Examples include coin flips, website clicks converting into sales, etc.\n",
    "*   **Bernoulli trial:**  A single trial with two possible outcomes.\n",
    "\n",
    "\n",
    "**Python Code:**\n",
    "\n",
    "The `scipy.stats` module provides functions for working with binomial distributions:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Probability of exactly 2 successes in 5 trials with probability of success = 0.1\n",
    "prob_exact = stats.binom.pmf(2, n=5, p=0.1) \n",
    "\n",
    "# Probability of 2 or fewer successes in 5 trials with probability of success = 0.1\n",
    "prob_cumulative = stats.binom.cdf(2, n=5, p=0.1)\n",
    "\n",
    "print(f\"Probability of exactly 2 successes: {prob_exact}\")\n",
    "print(f\"Probability of 2 or fewer successes: {prob_cumulative}\")\n",
    "```\n",
    "\n",
    "`pmf` calculates the probability mass function (probability of exactly k successes), and `cdf` calculates the cumulative distribution function (probability of k or fewer successes).\n",
    "\n",
    "**Approximation:**  For large `n` and `p` not too close to 0 or 1, the binomial distribution can be approximated by a normal distribution.\n",
    "\n",
    "\n",
    "## Chi-Square Distribution\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **Chi-square distribution:** Measures the difference between observed and expected frequencies in categorical data.  It's used for hypothesis testing (e.g., testing independence between variables).\n",
    "*   **Null hypothesis:** The assumption that there's no significant difference or relationship between variables.\n",
    "\n",
    "\n",
    "A high chi-square value indicates a significant departure from the expected values, suggesting the null hypothesis might be false.  Degrees of freedom are important in determining the appropriate chi-square distribution to compare results against.\n",
    "\n",
    "\n",
    "## F-Distribution\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **F-distribution:** Used to compare the variances of two or more groups.  Commonly used in ANOVA (Analysis of Variance).\n",
    "*   **ANOVA:**  A statistical test that determines if there are significant differences between the means of three or more groups.\n",
    "\n",
    "\n",
    "The F-statistic is the ratio of the variance between groups to the variance within groups. A large F-statistic suggests significant differences between group means. Python's statistical packages (like `statsmodels`) automatically calculate F-statistics as part of ANOVA and regression analysis.\n",
    "\n",
    "\n",
    "\n",
    "## Poisson and Related Distributions\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "*   **Poisson Distribution:** Models the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known average rate and independently of the time since the last event.\n",
    "*   **Exponential Distribution:** Models the time between events in a Poisson process.\n",
    "*   **Weibull Distribution:**  A generalization of the exponential distribution, allowing for a changing event rate over time (useful for modeling things like equipment failures).\n",
    "\n",
    "\n",
    "**Python Code (Poisson):**\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Generate 100 random numbers from a Poisson distribution with lambda = 2\n",
    "poisson_numbers = stats.poisson.rvs(2, size=100)\n",
    "print(poisson_numbers)\n",
    "```\n",
    "\n",
    "**Python Code (Exponential):**\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Generate 100 random numbers from an exponential distribution with rate = 0.2\n",
    "exponential_numbers = stats.expon.rvs(scale=1/0.2, size=100) #Scale parameter is 1/rate.\n",
    "print(exponential_numbers)\n",
    "\n",
    "```\n",
    "\n",
    "**Python Code (Weibull):**\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Generate 100 random numbers from a Weibull distribution with shape=1.5 and scale=5000\n",
    "weibull_numbers = stats.weibull_min.rvs(1.5, scale=5000, size=100)\n",
    "print(weibull_numbers)\n",
    "\n",
    "```\n",
    "\n",
    "Estimating the failure rate for rare events often involves simulations or goodness-of-fit tests.  The Weibull distribution handles situations where the event rate isn't constant.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This summary covers the key concepts and Python code snippets for various statistical distributions frequently encountered in data analysis.  Remember that appropriate distribution selection depends on the nature of your data and the research question.  Bootstrapping offers a robust, data-driven alternative to many classical statistical inference methods.\n",
    "\n",
    "\n",
    "\n",
    "=====================================================================================================\n",
    "\n",
    "# Chapter 3. Statistical Experiments and Significance Testing\n",
    "\n",
    "## Design of Experiments\n",
    "\n",
    "### Overview\n",
    "Design of experiments is crucial in statistics, helping to confirm or reject hypotheses. It's particularly important in data science for continual experiments in user interface and product marketing. This chapter covers traditional experimental design, common challenges, and key concepts in statistical inference.\n",
    "\n",
    "### Statistical Inference Pipeline\n",
    "The classical statistical inference pipeline involves:\n",
    "1. **Hypothesis**: Start with a hypothesis (e.g., \"drug A is better than the existing standard drug\").\n",
    "2. **Experiment Design**: Design an experiment to test the hypothesis.\n",
    "3. **Data Collection and Analysis**: Collect and analyze data.\n",
    "4. **Conclusion**: Draw a conclusion.\n",
    "\n",
    "The goal is to apply experiment results to a larger process or population.\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "### Definition\n",
    "A/B testing compares two groups to determine which treatment, product, or procedure is superior. One group is the control (standard or no treatment), and the other is the treatment group.\n",
    "\n",
    "### Key Terms\n",
    "- **Treatment**: Something exposed to subjects (e.g., drug, price, web headline).\n",
    "- **Treatment Group**: Group exposed to a specific treatment.\n",
    "- **Control Group**: Group exposed to no or standard treatment.\n",
    "- **Randomization**: Randomly assigning subjects to treatments.\n",
    "- **Subjects**: Items exposed to treatments (e.g., web visitors, patients).\n",
    "- **Test Statistic**: Metric used to measure the effect of the treatment.\n",
    "\n",
    "### Examples\n",
    "- Testing soil treatments for seed germination.\n",
    "- Testing therapies for cancer suppression.\n",
    "- Testing prices for net profit.\n",
    "- Testing web headlines for clicks.\n",
    "- Testing web ads for conversions.\n",
    "\n",
    "### Randomization\n",
    "Randomization ensures that any difference between treatment groups is due to the treatment or random assignment.\n",
    "\n",
    "### Test Statistic\n",
    "The test statistic is crucial for comparing groups. For binary variables (e.g., click/no-click), results are summed up in a 2x2 table.\n",
    "\n",
    "#### Example: 2x2 Table\n",
    "| Outcome       | Price A | Price B |\n",
    "|---------------|---------|---------|\n",
    "| Conversion    | 200     | 182     |\n",
    "| No conversion| 23,539  | 22,406  |\n",
    "\n",
    "For continuous variables (e.g., purchase amount), results might be displayed differently:\n",
    "- **Revenue/page view with price A**: mean = 3.87, SD = 51.10\n",
    "- **Revenue/page view with price B**: mean = 4.11, SD = 62.98\n",
    "\n",
    "### Warning\n",
    "Default statistical software output may not always be useful. For example, standard deviations suggesting negative revenue are not feasible. Mean absolute deviation from the mean is more reasonable.\n",
    "\n",
    "### Control Group Importance\n",
    "A control group ensures that all other conditions are equal, isolating the effect of the treatment.\n",
    "\n",
    "### Blinding in Studies\n",
    "- **Blind Study**: Subjects are unaware of the treatment.\n",
    "- **Double-Blind Study**: Both subjects and investigators are unaware of the treatment.\n",
    "\n",
    "### A/B Testing in Data Science\n",
    "A/B testing in data science typically involves web contexts, such as web page design, product price, or headline wording. Randomization and a single predetermined metric are crucial.\n",
    "\n",
    "### Multi-Arm Bandit Algorithm\n",
    "For questions like \"Which, out of multiple possible prices, is best?\", the multi-arm bandit algorithm is used instead of traditional A/B tests.\n",
    "\n",
    "### Getting Permission\n",
    "In scientific and medical research, permission from subjects and institutional review boards is necessary. In business, this is less common but can be controversial, as seen in Facebook's 2014 experiment.\n",
    "\n",
    "### Key Ideas\n",
    "- Subjects are assigned to groups treated exactly alike, except for the treatment.\n",
    "- Ideally, subjects are assigned randomly to the groups.\n",
    "\n",
    "### Further Reading\n",
    "- **Introductory Statistics and Analytics: A Resampling Perspective** by Peter Bruce.\n",
    "- **Google Analytics help section on experiments**.\n",
    "\n",
    "## Hypothesis Tests\n",
    "\n",
    "### Definition\n",
    "Hypothesis tests help determine if random chance might be responsible for an observed effect.\n",
    "\n",
    "### Key Terms\n",
    "- **Null Hypothesis**: Chance is to blame.\n",
    "- **Alternative Hypothesis**: Counterpoint to the null.\n",
    "- **One-Way Test**: Counts chance results in one direction.\n",
    "- **Two-Way Test**: Counts chance results in two directions.\n",
    "\n",
    "### Purpose\n",
    "Hypothesis tests protect researchers from being fooled by random chance. They involve a null hypothesis (chance is to blame) and an alternative hypothesis (what you hope to prove).\n",
    "\n",
    "### Misinterpreting Randomness\n",
    "Humans tend to underestimate randomness. For example, in a series of 50 coin flips, real results will have longer runs of Hs or Ts than invented ones.\n",
    "\n",
    "### Null Hypothesis\n",
    "The null hypothesis assumes treatments are equivalent, and any difference is due to chance. A resampling permutation procedure can test this hypothesis.\n",
    "\n",
    "### Alternative Hypothesis\n",
    "Examples:\n",
    "- Null = \"no difference between the means of group A and group B\"; alternative = \"A is different from B\".\n",
    "- Null = \"A ≤ B\"; alternative = \"A > B\".\n",
    "- Null = \"B is not X% greater than A\"; alternative = \"B is X% greater than A\".\n",
    "\n",
    "### One-Way vs. Two-Way Hypothesis Tests\n",
    "- **One-Way Test**: Protects from being fooled by chance in one direction.\n",
    "- **Two-Way Test**: Protects from being fooled by chance in either direction.\n",
    "\n",
    "### Key Ideas\n",
    "- A null hypothesis assumes nothing special has happened, and any effect is due to random chance.\n",
    "- The hypothesis test assumes the null hypothesis is true and tests whether the observed effect is a reasonable outcome of that model.\n",
    "\n",
    "### Further Reading\n",
    "- **The Drunkard’s Walk** by Leonard Mlodinow.\n",
    "- **Statistics** by David Freedman, Robert Pisani, and Roger Purves.\n",
    "- **Introductory Statistics and Analytics: A Resampling Perspective** by Peter Bruce.\n",
    "\n",
    "## Resampling\n",
    "\n",
    "### Definition\n",
    "Resampling involves repeatedly sampling values from observed data to assess random variability in a statistic.\n",
    "\n",
    "### Key Terms\n",
    "- **Permutation Test**: Combining samples and randomly reallocating observations.\n",
    "- **Resampling**: Drawing additional samples from an observed data set.\n",
    "- **With or Without Replacement**: Whether an item is returned to the sample before the next draw.\n",
    "\n",
    "### Permutation Test\n",
    "1. Combine results from different groups into a single data set.\n",
    "2. Shuffle the combined data and randomly draw resamples.\n",
    "3. Calculate the test statistic for the resamples.\n",
    "4. Repeat to yield a permutation distribution of the test statistic.\n",
    "5. Compare the observed difference to the permuted differences.\n",
    "\n",
    "### Example: Web Stickiness\n",
    "A company tests which web presentation leads to better sales using a proxy variable (session time).\n",
    "\n",
    "#### Tip\n",
    "A proxy variable stands in for the true variable of interest. It's useful to have data on the true variable to assess the strength of its association with the proxy.\n",
    "\n",
    "#### Code Snippets\n",
    "```python\n",
    "# Boxplot using pandas\n",
    "ax = session_times.boxplot(by='Page', column='Time')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Time (in seconds)')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Mean calculation\n",
    "mean_a = session_times[session_times.Page == 'Page A'].Time.mean()\n",
    "mean_b = session_times[session_times.Page == 'Page B'].Time.mean()\n",
    "mean_b - mean_a\n",
    "\n",
    "# Permutation function\n",
    "def perm_fun(x, nA, nB):\n",
    "    n = nA + nB\n",
    "    idx_B = set(random.sample(range(n), nB))\n",
    "    idx_A = set(range(n)) - idx_B\n",
    "    return x.loc[idx_B].mean() - x.loc[idx_A].mean()\n",
    "\n",
    "# Permutation test\n",
    "perm_diffs = [perm_fun(session_times.Time, nA, nB) for _ in range(1000)]\n",
    "\n",
    "# Histogram\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.hist(perm_diffs, bins=11, rwidth=0.9)\n",
    "ax.axvline(x = mean_b - mean_a, color='black', lw=2)\n",
    "ax.text(50, 190, 'Observed\\ndifference', bbox={'facecolor':'white'})\n",
    "ax.set_xlabel('Session time differences (in seconds)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# Calculate percentage\n",
    "np.mean(perm_diffs > mean_b - mean_a)\n",
    "```\n",
    "\n",
    "### Exhaustive and Bootstrap Permutation Tests\n",
    "- **Exhaustive Permutation Test**: Figures out all possible ways data could be divided.\n",
    "- **Bootstrap Permutation Test**: Draws are made with replacement.\n",
    "\n",
    "### Key Ideas\n",
    "- In a permutation test, multiple samples are combined and shuffled.\n",
    "- The shuffled values are divided into resamples, and the statistic of interest is calculated.\n",
    "- Comparing the observed value to the resampled distribution helps judge whether an observed difference might occur by chance.\n",
    "\n",
    "### Further Reading\n",
    "- **Randomization Tests** by Eugene Edgington and Patrick Onghena.\n",
    "- **Introductory Statistics and Analytics: A Resampling Perspective** by Peter Bruce.\n",
    "\n",
    "## Conclusion\n",
    "This chapter covers the design of experiments, A/B testing, hypothesis tests, and resampling techniques, providing a comprehensive overview of statistical experiments and significance testing in data science.\n",
    "\n",
    "\n",
    "summary 2\n",
    "\n",
    "# Chapter 3: Statistical Experiments and Significance Testing\n",
    "\n",
    "This chapter explores statistical experiments, focusing on A/B testing and hypothesis testing within the context of data science.  It emphasizes practical application over strict adherence to classical statistical inference, highlighting the importance of understanding the underlying concepts rather than rote application of formulas.\n",
    "\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "A/B testing compares two versions (A and B) of a treatment (e.g., website design, pricing strategy) to determine which performs better.  One version typically serves as a control. The goal is to identify the superior treatment based on a chosen metric.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Treatment:** The thing being tested (e.g., a new website design).\n",
    "* **Treatment group:** The group exposed to a specific treatment.\n",
    "* **Control group:** The group exposed to the standard or no treatment.\n",
    "* **Randomization:** Randomly assigning subjects to treatment groups.\n",
    "* **Subjects:** The items being tested (e.g., website visitors).\n",
    "* **Test statistic:** The metric used to compare treatments (e.g., click-through rate, conversion rate).\n",
    "\n",
    "\n",
    "**Example:** Testing two web headlines to see which generates more clicks.\n",
    "\n",
    "\n",
    "**Why a Control Group?** A control group ensures that any observed difference is due to the treatment, not other factors.  Comparing only to past data ignores potential confounding variables.\n",
    "\n",
    "\n",
    "**Blinding:** In some experiments (not usually in data science A/B tests), blinding participants to the treatment they receive prevents bias from influencing their responses.\n",
    "\n",
    "\n",
    "**Beyond A/B (Multi-Arm Bandits):** While A/B testing is common, data scientists often need to compare more than two options.  Multi-arm bandit algorithms are better suited for this.\n",
    "\n",
    "\n",
    "**Ethical Considerations:**  Obtaining informed consent is crucial in research involving human subjects, but less strictly enforced in business contexts. However, ethical considerations remain paramount (e.g., Facebook's 2014 emotional tone experiment).\n",
    "\n",
    "\n",
    "## Hypothesis Tests\n",
    "\n",
    "Hypothesis tests determine whether an observed effect is likely due to chance or a real difference between treatments.\n",
    "\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Null hypothesis:**  Assumes there's no real difference between treatments; any observed difference is due to chance.\n",
    "* **Alternative hypothesis:** The opposite of the null hypothesis; it's what you hope to prove.\n",
    "* **One-way test:** Considers chance results only in one direction (e.g., treatment B is better than A).\n",
    "* **Two-way test:** Considers chance results in both directions (e.g., treatment B is different from A).\n",
    "\n",
    "\n",
    "**Misinterpreting Randomness:** People tend to underestimate randomness, seeing patterns where none exist. Hypothesis testing helps mitigate this bias.\n",
    "\n",
    "\n",
    "**The Null Hypothesis:**  The starting assumption that there's no significant difference; the goal is to disprove this.\n",
    "\n",
    "\n",
    "**Alternative Hypothesis:** The statement you are trying to support, for example \"Treatment B is better than Treatment A\".\n",
    "\n",
    "\n",
    "**One-Way vs. Two-Way Tests:** One-way tests are directional (e.g., A > B), while two-way tests are non-directional (A ≠ B).  The choice depends on the research question.  In Data Science, the distinction is less crucial.\n",
    "\n",
    "\n",
    "## Resampling\n",
    "\n",
    "Resampling involves repeatedly sampling from observed data to assess variability.  It's a powerful tool for hypothesis testing, offering a more flexible approach compared to traditional methods.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Permutation test (Randomization test):**  Combines samples, shuffles the data, and randomly reassigns it to groups.  This simulates the null hypothesis.\n",
    "* **Resampling:** Drawing multiple samples from the observed data.\n",
    "* **With or without replacement:** Whether data points are returned to the pool after sampling.\n",
    "\n",
    "\n",
    "**Permutation Test (Python Example):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace with your own)\n",
    "data = {'Page': ['Page A'] * 21 + ['Page B'] * 15,\n",
    "        'Time': [100, 120, 110, ..., 150, 160, 140]} #replace ... with your actual data\n",
    "session_times = pd.DataFrame(data)\n",
    "\n",
    "nA = 21\n",
    "nB = 15\n",
    "\n",
    "def perm_fun(x, nA, nB):\n",
    "    n = nA + nB\n",
    "    idx_B = set(random.sample(range(n), nB))\n",
    "    idx_A = set(range(n)) - idx_B\n",
    "    return x.loc[idx_B].mean() - x.loc[idx_A].mean()\n",
    "\n",
    "perm_diffs = [perm_fun(session_times.Time, nA, nB) for _ in range(1000)]\n",
    "\n",
    "mean_a = session_times[session_times.Page == 'Page A'].Time.mean()\n",
    "mean_b = session_times[session_times.Page == 'Page B'].Time.mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.hist(perm_diffs, bins=11, rwidth=0.9)\n",
    "ax.axvline(x = mean_b - mean_a, color='black', lw=2)\n",
    "ax.text(50, 190, 'Observed\\ndifference', bbox={'facecolor':'white'})\n",
    "ax.set_xlabel('Session time differences (in seconds)')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "p_value = np.mean(perm_diffs > mean_b - mean_a)\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "```\n",
    "\n",
    "This code performs a permutation test to compare the means of two groups.  It shuffles the data, recalculates the difference in means many times, and then compares the observed difference to this distribution to assess statistical significance.  A low p-value (typically below 0.05) suggests the observed difference is unlikely due to chance.\n",
    "\n",
    "**Exhaustive and Bootstrap Permutation Tests:** These are variations on the basic permutation test.  Exhaustive tests consider all possible permutations (computationally expensive for large datasets). Bootstrap tests sample *with* replacement, adding another layer of randomness.\n",
    "\n",
    "\n",
    "**Permutation Tests in Data Science:** Permutation tests are valuable for their simplicity, flexibility, and avoidance of stringent assumptions (like normally distributed data). They provide an intuitive understanding of statistical significance.\n",
    "\n",
    "\n",
    "**Further Reading:** The provided text includes several books and resources for further learning.  The Google Analytics help section on experiments is also a good resource for practical A/B testing.\n",
    "\n",
    "=========================================================================================================================\n",
    "\n",
    "## Statistical Significance and p-Values\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Statistical significance measures whether an experiment's results are more extreme than what chance might produce. If the results are beyond chance variation, they are statistically significant.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **p-value**: The probability of obtaining results as unusual or extreme as observed, given a null hypothesis.\n",
    "- **Alpha**: The probability threshold for \"unusualness\" that chance results must surpass for actual outcomes to be deemed statistically significant.\n",
    "- **Type 1 error**: Mistakenly concluding an effect is real (when it is due to chance).\n",
    "- **Type 2 error**: Mistakenly concluding an effect is due to chance (when it is real).\n",
    "\n",
    "### Example: Ecommerce Experiment\n",
    "\n",
    "#### Table 3-2: 2×2 Table for Ecommerce Experiment Results\n",
    "\n",
    "| Outcome       | Price A | Price B  |\n",
    "|---------------|---------|----------|\n",
    "| Conversion    | 200     | 182      |\n",
    "| No conversion | 23,539  | 22,406   |\n",
    "\n",
    "Price A converts almost 5% better than Price B. Despite having over 45,000 data points, the conversion rates are low (less than 1%), making the actual meaningful values only in the 100s.\n",
    "\n",
    "#### Permutation Procedure\n",
    "\n",
    "1. **Put cards labeled 1 and 0 in a box**: Represents the shared conversion rate of 382 ones and 45,945 zeros = 0.8246%.\n",
    "2. **Shuffle and draw out a resample of size 23,739 (same n as Price A)**, and record how many 1s.\n",
    "3. **Record the number of 1s in the remaining 22,588 (same n as Price B)**.\n",
    "4. **Record the difference in proportion of 1s**.\n",
    "5. **Repeat steps 2–4**.\n",
    "6. **How often was the difference >= 0.0368?**\n",
    "\n",
    "#### Python Code for Permutation Test\n",
    "\n",
    "```python\n",
    "obs_pct_diff = 100 * (200 / 23739 - 182 / 22588)\n",
    "print(f'Observed difference: {obs_pct_diff:.4f}%')\n",
    "conversion = [0] * 45945\n",
    "conversion.extend([1] * 382)\n",
    "conversion = pd.Series(conversion)\n",
    "\n",
    "perm_diffs = [100 * perm_fun(conversion, 23739, 22588) for _ in range(1000)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.hist(perm_diffs, bins=11, rwidth=0.9)\n",
    "ax.axvline(x=obs_pct_diff, color='black', lw=2)\n",
    "ax.text(0.06, 200, 'Observed\\ndifference', bbox={'facecolor':'white'})\n",
    "ax.set_xlabel('Conversion rate (percent)')\n",
    "ax.set_ylabel('Frequency')\n",
    "```\n",
    "\n",
    "#### Histogram of Differences in Conversion Rates\n",
    "\n",
    "The observed difference of 0.0368% is within the range of chance variation.\n",
    "\n",
    "### p-Value\n",
    "\n",
    "The p-value is the frequency with which the chance model produces a result more extreme than the observed result.\n",
    "\n",
    "#### Python Code for p-Value Calculation\n",
    "\n",
    "```python\n",
    "np.mean([diff > obs_pct_diff for diff in perm_diffs])\n",
    "```\n",
    "\n",
    "The p-value is 0.308, meaning that we would expect a result as extreme as this, or more extreme, by random chance over 30% of the time.\n",
    "\n",
    "#### Approximating p-Value Using Binomial Distribution\n",
    "\n",
    "```python\n",
    "survivors = np.array([[200, 23739 - 200], [182, 22588 - 182]])\n",
    "chi2, p_value, df, _ = stats.chi2_contingency(survivors)\n",
    "\n",
    "print(f'p-value for single sided test: {p_value / 2:.4f}')\n",
    "```\n",
    "\n",
    "The normal approximation yields a p-value of 0.3498, close to the permutation test p-value.\n",
    "\n",
    "### Alpha\n",
    "\n",
    "Alpha is the threshold for \"unusualness\" in a null hypothesis chance model. Typical alpha levels are 5% and 1%.\n",
    "\n",
    "### p-Value Controversy\n",
    "\n",
    "The p-value has been a subject of controversy. The American Statistical Association issued a cautionary statement regarding its use:\n",
    "\n",
    "1. P-values can indicate how incompatible the data are with a specified statistical model.\n",
    "2. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n",
    "3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n",
    "4. Proper inference requires full reporting and transparency.\n",
    "5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n",
    "6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n",
    "\n",
    "### Practical Significance\n",
    "\n",
    "Even if a result is statistically significant, it does not mean it has practical significance. Large samples can make small, non-meaningful effects statistically significant.\n",
    "\n",
    "### Type 1 and Type 2 Errors\n",
    "\n",
    "- **Type 1 error**: Mistakenly concluding an effect is real, when it is due to chance.\n",
    "- **Type 2 error**: Mistakenly concluding that an effect is not real (i.e., due to chance), when it actually is real.\n",
    "\n",
    "### Data Science and p-Values\n",
    "\n",
    "For data scientists, a p-value is a useful metric to determine if a model result is within the range of normal chance variability. It should not be considered controlling but merely another point of information.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Significance tests determine whether an observed effect is within the range of chance variation for a null hypothesis model.\n",
    "- The p-value is the probability that results as extreme as the observed results might occur, given a null hypothesis model.\n",
    "- The alpha value is the threshold of \"unusualness\" in a null hypothesis chance model.\n",
    "- Significance testing has been more relevant for formal reporting of research than for data science.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Stephen Stigler, “Fisher and the 5% Level,” Chance 21, no. 4 (2008): 12.\n",
    "- See also “Hypothesis Tests” and the further reading mentioned there.\n",
    "\n",
    "## t-Tests\n",
    "\n",
    "### Introduction\n",
    "\n",
    "t-Tests are common significance tests used for comparing means. They are based on Student’s t-distribution.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Test statistic**: A metric for the difference or effect of interest.\n",
    "- **t-statistic**: A standardized version of common test statistics such as means.\n",
    "- **t-distribution**: A reference distribution to which the observed t-statistic can be compared.\n",
    "\n",
    "### Example: t-Test in Python\n",
    "\n",
    "```python\n",
    "res = stats.ttest_ind(session_times[session_times.Page == 'Page A'].Time,\n",
    "                      session_times[session_times.Page == 'Page B'].Time,\n",
    "                      equal_var=False)\n",
    "print(f'p-value for single sided test: {res.pvalue / 2:.4f}')\n",
    "```\n",
    "\n",
    "The p-value of 0.1408 is fairly close to the permutation test p-values.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Before the advent of computers, resampling tests were not practical, and statisticians used standard reference distributions.\n",
    "- A test statistic could then be standardized and compared to the reference distribution.\n",
    "- One such widely used standardized statistic is the t-statistic.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Any introductory statistics text will have illustrations of the t-statistic and its uses.\n",
    "- For a treatment of both the t-test and resampling procedures in parallel, see Introductory Statistics and Analytics: A Resampling Perspective by Peter Bruce (Wiley, 2014) or Statistics: Unlocking the Power of Data, 2nd ed., by Robin Lock and four other Lock family members (Wiley, 2016).\n",
    "\n",
    "## Multiple Testing\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Multiple testing increases the risk of concluding that something is significant just by chance.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Type 1 error**: Mistakenly concluding that an effect is statistically significant.\n",
    "- **False discovery rate**: The rate of making a Type 1 error across multiple tests.\n",
    "- **Alpha inflation**: The multiple testing phenomenon, in which alpha increases as you conduct more tests.\n",
    "- **Adjustment of p-values**: Accounting for doing multiple tests on the same data.\n",
    "- **Overfitting**: Fitting the noise.\n",
    "\n",
    "### Example: Alpha Inflation\n",
    "\n",
    "The probability that at least one predictor will (falsely) test significant is 0.64.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- Multiplicity in a research study or data mining project increases the risk of concluding that something is significant just by chance.\n",
    "- For situations involving multiple statistical comparisons, there are statistical adjustment procedures.\n",
    "- In a data mining situation, use of a holdout sample with labeled outcome variables can help avoid misleading results.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- For a short exposition of one procedure (Dunnett’s test) to adjust for multiple comparisons, see David Lane’s online statistics text.\n",
    "- Megan Goldman offers a slightly longer treatment of the Bonferroni adjustment procedure.\n",
    "- For an in-depth treatment of more flexible statistical procedures for adjusting p-values, see Resampling-Based Multiple Testing by Peter Westfall and Stanley Young (Wiley, 1993).\n",
    "- For a discussion of data partitioning and the use of holdout samples in predictive modeling, see Chapter 2 of Data Mining for Business Analytics, by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions for R, Python, Excel, and JMP).\n",
    "\n",
    "## Degrees of Freedom\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Degrees of freedom refer to the number of values free to vary in a sample.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **n or sample size**: The number of observations in the data.\n",
    "- **d.f.**: Degrees of freedom.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- The number of degrees of freedom forms part of the calculation to standardize test statistics so they can be compared to reference distributions.\n",
    "- The concept of degrees of freedom lies behind the factoring of categorical variables into n – 1 indicator or dummy variables when doing a regression (to avoid multicollinearity).\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "There are several web tutorials on degrees of freedom.\n",
    "\n",
    "\n",
    "\n",
    "summary 2\n",
    "\n",
    "\n",
    "c:\\Users\\mridu\\Pictures\\FastApi\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "# Statistical Significance and p-Values\n",
    "\n",
    "## Key Terms\n",
    "\n",
    "* **p-value:** The probability of getting results as extreme as the observed results, assuming the null hypothesis (no effect) is true.  A low p-value suggests the null hypothesis is unlikely.\n",
    "* **Alpha (α):** The threshold for statistical significance.  If the p-value is less than alpha, the result is considered statistically significant. Common alpha levels are 0.05 (5%) and 0.01 (1%).\n",
    "* **Type 1 error:**  Rejecting the null hypothesis when it's actually true (false positive).\n",
    "* **Type 2 error:** Failing to reject the null hypothesis when it's actually false (false negative).\n",
    "\n",
    "\n",
    "## Example: A/B Test on Ecommerce Prices\n",
    "\n",
    "An A/B test compared two prices (A and B) resulting in the following conversion rates:\n",
    "\n",
    "| Outcome       | Price A | Price B |\n",
    "|---------------|---------|---------|\n",
    "| Conversion    | 200     | 182     |\n",
    "| No Conversion | 23539   | 22406   |\n",
    "\n",
    "\n",
    "Price A showed a seemingly better conversion rate (0.8425% vs 0.8057%), but is this difference statistically significant?  A permutation test helps determine if this difference could be due to random chance.\n",
    "\n",
    "### Python Code for Permutation Test\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Function (assumed defined elsewhere -  see original text for details)\n",
    "def perm_fun(conversion, n1, n2):\n",
    "  # This function shuffles the conversion data and returns the difference in proportions\n",
    "  pass  # Placeholder; the actual function would be implemented here.\n",
    "\n",
    "obs_pct_diff = 100 * (200 / 23739 - 182 / 22588)\n",
    "print(f'Observed difference: {obs_pct_diff:.4f}%')\n",
    "conversion = [0] * 45945  # 0 represents no conversion\n",
    "conversion.extend([1] * 382)  # 1 represents conversion\n",
    "conversion = pd.Series(conversion)\n",
    "\n",
    "perm_diffs = [100 * perm_fun(conversion, 23739, 22588)\n",
    "              for _ in range(1000)] # Perform 1000 permutations\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.hist(perm_diffs, bins=11, rwidth=0.9)\n",
    "ax.axvline(x=obs_pct_diff, color='black', lw=2)\n",
    "ax.text(0.06, 200, 'Observed\\ndifference', bbox={'facecolor':'white'})\n",
    "ax.set_xlabel('Conversion rate (percent)')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code simulates many possible outcomes under the null hypothesis (no price difference).  The histogram visualizes these simulated differences, showing where the observed difference falls.\n",
    "\n",
    "### Calculating the p-value\n",
    "\n",
    "The p-value is estimated by calculating the proportion of simulated differences that are as extreme or more extreme than the observed difference:\n",
    "\n",
    "```python\n",
    "p_value = np.mean([diff > obs_pct_diff for diff in perm_diffs])\n",
    "print(f\"Estimated p-value: {p_value}\")\n",
    "```\n",
    "\n",
    "A high p-value (e.g., >0.05) suggests the observed difference is not statistically significant and likely due to chance.\n",
    "\n",
    "### Alternative: Using `scipy.stats.chi2_contingency`\n",
    "\n",
    "The `chi2_contingency` function provides a more direct way to calculate a p-value for this 2x2 contingency table:\n",
    "\n",
    "```python\n",
    "survivors = np.array([[200, 23739 - 200], [182, 22588 - 182]])\n",
    "chi2, p_value, df, _ = stats.chi2_contingency(survivors)\n",
    "print(f'p-value for single sided test: {p_value / 2:.4f}')\n",
    "```\n",
    "\n",
    "Note: The p-value is halved because it's a one-sided test (we're only interested if Price A is better).\n",
    "\n",
    "## Alpha and the p-value Controversy\n",
    "\n",
    "Alpha sets the threshold for significance.  However, the p-value is often misinterpreted as the probability that the result is due to chance.  The American Statistical Association highlights the importance of not relying solely on p-values for decision-making.\n",
    "\n",
    "## Practical Significance\n",
    "\n",
    "Statistical significance doesn't always equate to practical significance. A statistically significant result might represent a small, unimportant effect.\n",
    "\n",
    "## Type 1 and Type 2 Errors\n",
    "\n",
    "* **Type 1 error:**  Falsely concluding an effect is real.\n",
    "* **Type 2 error:** Falsely concluding an effect is not real.\n",
    "\n",
    "Significance tests aim to minimize Type 1 errors.\n",
    "\n",
    "## Data Science and p-Values\n",
    "\n",
    "In data science, p-values are helpful to understand if an interesting result could be due to chance.  They shouldn't be the sole basis for decisions.\n",
    "\n",
    "\n",
    "## t-Tests\n",
    "\n",
    "The t-test is a common significance test for comparing means of two groups of numerical data.\n",
    "\n",
    "### Python Code for t-test\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "# Assuming 'session_times' is a Pandas DataFrame with columns 'Time' and 'Page'\n",
    "res = stats.ttest_ind(session_times[session_times.Page == 'Page A'].Time,\n",
    "                      session_times[session_times.Page == 'Page B'].Time,\n",
    "                      equal_var=False) # Welch's t-test (does not assume equal variances)\n",
    "print(f'p-value for single sided test: {res.pvalue / 2:.4f}')\n",
    "```\n",
    "\n",
    "This code performs an independent samples t-test.  `equal_var=False` uses Welch's t-test, which is more robust if the variances of the two groups are not equal.  The p-value is divided by 2 for a one-sided test.\n",
    "\n",
    "\n",
    "## Multiple Testing\n",
    "\n",
    "Performing many tests increases the chance of finding a statistically significant result by chance (Type 1 error).  This is called alpha inflation.\n",
    "\n",
    "## False Discovery Rate\n",
    "\n",
    "The false discovery rate (FDR) is the rate of Type 1 errors across multiple tests.  It's particularly relevant in situations with many comparisons, like genomic studies.\n",
    "\n",
    "## Degrees of Freedom\n",
    "\n",
    "Degrees of freedom (df) represent the number of values in a calculation that are free to vary.  It's important for standardizing test statistics and in regression when dealing with categorical predictors (to avoid multicollinearity).  For large datasets, the impact of degrees of freedom on calculations is often negligible.\n",
    "\n",
    "\n",
    "========================================================================================================================\n",
    "\n",
    "## Summary of ANOVA and Chi-Square Test\n",
    "\n",
    "### ANOVA\n",
    "\n",
    "#### Introduction\n",
    "ANOVA (Analysis of Variance) is a statistical procedure used to test for significant differences among multiple groups. It extends the A/B test to multiple groups, assessing whether the overall variation among groups is within the range of chance variation.\n",
    "\n",
    "#### Key Terms\n",
    "- **Pairwise comparison**: A hypothesis test between two groups among multiple groups.\n",
    "- **Omnibus test**: A single hypothesis test of the overall variance among multiple group means.\n",
    "- **Decomposition of variance**: Separation of components contributing to an individual value.\n",
    "- **F-statistic**: Measures the extent to which differences among group means exceed what might be expected in a chance model.\n",
    "- **SS (Sum of Squares)**: Refers to deviations from some average value.\n",
    "\n",
    "#### Example: Web Page Stickiness\n",
    "Table 3-3 shows the stickiness (in seconds) of four web pages.\n",
    "\n",
    "| Page 1 | Page 2 | Page 3 | Page 4 |\n",
    "|--------|--------|--------|--------|\n",
    "| 164    | 178    | 175    | 155    |\n",
    "| 172    | 191    | 193    | 166    |\n",
    "| 177    | 182    | 171    | 164    |\n",
    "| 156    | 185    | 163    | 170    |\n",
    "| 195    | 177    | 176    | 168    |\n",
    "| **Average** | 172 | 185 | 176 | 162 |\n",
    "| **Grand average** | 173.75 |\n",
    "\n",
    "#### Pairwise Comparisons\n",
    "With four means, there are six possible comparisons between groups:\n",
    "1. Page 1 compared to Page 2\n",
    "2. Page 1 compared to Page 3\n",
    "3. Page 1 compared to Page 4\n",
    "4. Page 2 compared to Page 3\n",
    "5. Page 2 compared to Page 4\n",
    "6. Page 3 compared to Page 4\n",
    "\n",
    "#### Permutation Test in R\n",
    "```r\n",
    "> library(lmPerm)\n",
    "> summary(aovp(Time ~ Page, data=four_sessions))\n",
    "[1] \"Settings:  unique SS \"\n",
    "Component 1 :\n",
    "            Df R Sum Sq R Mean Sq Iter Pr(Prob)\n",
    "Page         3    831.4    277.13 3104  0.09278 .\n",
    "Residuals   16   1618.4    101.15\n",
    "---\n",
    "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "```\n",
    "\n",
    "#### Permutation Test in Python\n",
    "```python\n",
    "observed_variance = four_sessions.groupby('Page').mean().var()[0]\n",
    "print('Observed means:', four_sessions.groupby('Page').mean().values.ravel())\n",
    "print('Variance:', observed_variance)\n",
    "\n",
    "def perm_test(df):\n",
    "    df = df.copy()\n",
    "    df['Time'] = np.random.permutation(df['Time'].values)\n",
    "    return df.groupby('Page').mean().var()[0]\n",
    "\n",
    "perm_variance = [perm_test(four_sessions) for _ in range(3000)]\n",
    "print('Pr(Prob)', np.mean([var > observed_variance for var in perm_variance]))\n",
    "```\n",
    "\n",
    "#### F-Statistic\n",
    "The F-statistic is based on the ratio of the variance across group means to the variance due to residual error.\n",
    "\n",
    "#### ANOVA Table in R\n",
    "```r\n",
    "> summary(aov(Time ~ Page, data=four_sessions))\n",
    "            Df Sum Sq Mean Sq F value Pr(>F)\n",
    "Page         3  831.4   277.1    2.74 0.0776 .\n",
    "Residuals   16 1618.4   101.2\n",
    "---\n",
    "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
    "```\n",
    "\n",
    "#### ANOVA Table in Python\n",
    "```python\n",
    "model = smf.ols('Time ~ Page', data=four_sessions).fit()\n",
    "aov_table = sm.stats.anova_lm(model)\n",
    "aov_table\n",
    "```\n",
    "\n",
    "#### Decomposition of Variance\n",
    "1. Start with grand average (173.75 for web page stickiness data).\n",
    "2. Add treatment effect, which might be negative (independent variable = web page).\n",
    "3. Add residual error, which might be negative.\n",
    "\n",
    "#### Two-Way ANOVA\n",
    "Involves a second factor (e.g., weekend vs. weekday) and identifies the interaction effect.\n",
    "\n",
    "#### Key Ideas\n",
    "- ANOVA is used to analyze the results of an experiment with multiple groups.\n",
    "- It identifies variance components associated with group treatments, interaction effects, and errors.\n",
    "\n",
    "#### Further Reading\n",
    "- *Introductory Statistics and Analytics: A Resampling Perspective* by Peter Bruce (Wiley, 2014)\n",
    "- *Introduction to Design and Analysis of Experiments* by George Cobb (Wiley, 2008)\n",
    "\n",
    "### Chi-Square Test\n",
    "\n",
    "#### Introduction\n",
    "The chi-square test is used with count data to test how well it fits some expected distribution. It is commonly used with r × c contingency tables to assess independence among variables.\n",
    "\n",
    "#### Key Terms\n",
    "- **Chi-square statistic**: Measures the extent to which observed data departs from expectation.\n",
    "- **Expectation or expected**: How data is expected to turn out under some assumption, typically the null hypothesis.\n",
    "\n",
    "#### Example: Web Testing Results\n",
    "Table 3-4 shows the results for three different headlines.\n",
    "\n",
    "| Headline A | Headline B | Headline C |\n",
    "|------------|------------|------------|\n",
    "| Click      | 14         | 8          | 12        |\n",
    "| No-click   | 986        | 992        | 988       |\n",
    "\n",
    "#### Resampling Approach\n",
    "1. Constitute a box with 34 ones (clicks) and 2,966 zeros (no clicks).\n",
    "2. Shuffle, take three separate samples of 1,000, and count the clicks in each.\n",
    "3. Find the squared differences between the shuffled counts and the expected counts and sum them.\n",
    "4. Repeat steps 2 and 3, say, 1,000 times.\n",
    "5. How often does the resampled sum of squared deviations exceed the observed? That’s the p-value.\n",
    "\n",
    "#### Chi-Square Test in Python\n",
    "```python\n",
    "box = [1] * 34\n",
    "box.extend([0] * 2966)\n",
    "random.shuffle(box)\n",
    "\n",
    "def chi2(observed, expected):\n",
    "    pearson_residuals = []\n",
    "    for row, expect in zip(observed, expected):\n",
    "        pearson_residuals.append([(observe - expect) ** 2 / expect\n",
    "                                  for observe in row])\n",
    "    # return sum of squares\n",
    "    return np.sum(pearson_residuals)\n",
    "\n",
    "expected_clicks = 34 / 3\n",
    "expected_noclicks = 1000 - expected_clicks\n",
    "expected = [34 / 3, 1000 - 34 / 3]\n",
    "chi2observed = chi2(clicks.values, expected)\n",
    "\n",
    "def perm_fun(box):\n",
    "    sample_clicks = [sum(random.sample(box, 1000)),\n",
    "                     sum(random.sample(box, 1000)),\n",
    "                     sum(random.sample(box, 1000))]\n",
    "    sample_noclicks = [1000 - n for n in sample_clicks]\n",
    "    return chi2([sample_clicks, sample_noclicks], expected)\n",
    "\n",
    "perm_chi2 = [perm_fun(box) for _ in range(2000)]\n",
    "\n",
    "resampled_p_value = sum(perm_chi2 > chi2observed) / len(perm_chi2)\n",
    "print(f'Observed chi2: {chi2observed:.4f}')\n",
    "print(f'Resampled p-value: {resampled_p_value:.4f}')\n",
    "```\n",
    "\n",
    "#### Statistical Theory\n",
    "The chi-square distribution is typically skewed, with a long tail to the right. The degrees of freedom for a contingency table are related to the number of rows (r) and columns (c) as follows:\n",
    "\\[ \\text{degrees of freedom} = (r - 1) \\times (c - 1) \\]\n",
    "\n",
    "#### Chi-Square Test in Python\n",
    "```python\n",
    "chisq, pvalue, df, expected = stats.chi2_contingency(clicks)\n",
    "print(f'Observed chi2: {chi2observed:.4f}')\n",
    "print(f'p-value: {pvalue:.4f}')\n",
    "```\n",
    "\n",
    "#### Fisher’s Exact Test\n",
    "Used when counts are extremely low. R code for Fisher’s exact test:\n",
    "```r\n",
    "> fisher.test(clicks)\n",
    "\tFisher's Exact Test for Count Data\n",
    "\n",
    "data:  clicks\n",
    "p-value = 0.4824\n",
    "alternative hypothesis: two.sided\n",
    "```\n",
    "\n",
    "#### Detecting Scientific Fraud\n",
    "An example involving the distribution of digits in laboratory data, where the chi-square test was used to detect fabrication.\n",
    "\n",
    "#### Relevance for Data Science\n",
    "- Used to determine appropriate sample sizes for web experiments.\n",
    "- Used in spatial statistics and mapping.\n",
    "- Used in automated feature selection in machine learning.\n",
    "\n",
    "#### Key Ideas\n",
    "- Tests whether observed data counts are consistent with an assumption of independence.\n",
    "- The chi-square distribution is the reference distribution to which the observed chi-square statistic must be compared.\n",
    "\n",
    "#### Further Reading\n",
    "- R. A. Fisher’s “Lady Tasting Tea” example.\n",
    "- Stat Trek offers a good tutorial on the chi-square test.\n",
    "\n",
    "\n",
    "\n",
    "summaary 2\n",
    "\n",
    "# ANOVA (Analysis of Variance)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ANOVA is a statistical test used to compare the means of three or more groups.  It's an extension of the t-test, which only compares two groups.  Instead of multiple pairwise comparisons (which increases the chance of false positives), ANOVA tests if there's a significant overall difference between the group means.\n",
    "\n",
    "## Example: Web Page Stickiness\n",
    "\n",
    "This example analyzes the \"stickiness\" (time spent) of four web pages (A, B, C, D). Each page was shown to five visitors, and the time spent was recorded. The goal is to determine if there's a significant difference in stickiness between the pages.\n",
    "\n",
    "**Table 3-3 (Stickiness in seconds):**  (This table was provided in the text but is omitted here for brevity.  It contains the stickiness data for each page and visitor)\n",
    "\n",
    "\n",
    "**Python Code (Permutation Test):**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data (replace with your actual data from Table 3-3)\n",
    "data = {'Page': ['A']*5 + ['B']*5 + ['C']*5 + ['D']*5,\n",
    "        'Time': [164, 172, 177, 156, 195, 178, 191, 182, 185, 177, 175, 193, 171, 163, 176, 155, 166, 164, 170, 168]}\n",
    "four_sessions = pd.DataFrame(data)\n",
    "\n",
    "observed_variance = four_sessions.groupby('Page').mean().var()[0]\n",
    "print('Observed means:', four_sessions.groupby('Page').mean().values.ravel())\n",
    "print('Variance:', observed_variance)\n",
    "\n",
    "def perm_test(df):\n",
    "    df = df.copy()\n",
    "    df['Time'] = np.random.permutation(df['Time'].values)\n",
    "    return df.groupby('Page').mean().var()[0]\n",
    "\n",
    "perm_variance = [perm_test(four_sessions) for _ in range(3000)]\n",
    "print('Pr(Prob)', np.mean([var > observed_variance for var in perm_variance]))\n",
    "\n",
    "```\n",
    "\n",
    "This code performs a permutation test to determine the p-value.  It shuffles the time data randomly and recalculates the variance between group means many times. The proportion of times the shuffled variance exceeds the observed variance gives the p-value. A low p-value (typically below 0.05) suggests a significant difference between the groups.\n",
    "\n",
    "\n",
    "**Python Code (ANOVA using statsmodels):**\n",
    "\n",
    "```python\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "model = smf.ols('Time ~ Page', data=four_sessions).fit()\n",
    "aov_table = sm.stats.anova_lm(model)\n",
    "print(aov_table)\n",
    "```\n",
    "\n",
    "This code uses the `statsmodels` library to perform a traditional ANOVA. The output provides an ANOVA table with Sum of Squares, Mean Squares, F-statistic, and p-value.\n",
    "\n",
    "\n",
    "## F-Statistic\n",
    "\n",
    "The F-statistic is the ratio of the variance *between* groups to the variance *within* groups. A high F-statistic indicates that the variance between groups is much larger than the variance within groups, suggesting a significant difference between the groups.\n",
    "\n",
    "\n",
    "## Decomposition of Variance\n",
    "\n",
    "Each data point can be broken down into:\n",
    "\n",
    "1. **Grand Average:** The overall average of all data points.\n",
    "2. **Treatment Effect:** The difference between the group mean and the grand average.\n",
    "3. **Residual Error:** The difference between the individual data point and its group mean.\n",
    "\n",
    "\n",
    "## Two-Way ANOVA\n",
    "\n",
    "Two-way ANOVA extends the analysis to include two or more factors (e.g., web page and day of the week).  It also allows for the assessment of interaction effects between the factors.\n",
    "\n",
    "\n",
    "## Chi-Square Test\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The chi-square test is used to analyze categorical data (counts) to see if there's a significant association between two or more categorical variables.  It compares observed counts to expected counts under the assumption of independence between the variables.\n",
    "\n",
    "\n",
    "## Example: Headline Click-Through Rates\n",
    "\n",
    "This example examines click-through rates for three different headlines (A, B, C).  The goal is to determine if there's a significant difference in click-through rates between the headlines.\n",
    "\n",
    "**Table 3-4 (Headline Clicks):** (This table was provided in the text but is omitted here for brevity. It contains the click and no-click counts for each headline).\n",
    "\n",
    "**Python Code (Permutation Test):**\n",
    "\n",
    "```python\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data (replace with your actual data from Table 3-4)\n",
    "clicks = np.array([[14, 986], [8, 992], [12, 988]])\n",
    "\n",
    "box = [1] * 34\n",
    "box.extend([0] * 2966)\n",
    "random.shuffle(box)\n",
    "\n",
    "def chi2(observed, expected):\n",
    "    pearson_residuals = []\n",
    "    for row, expect in zip(observed, expected):\n",
    "        pearson_residuals.append([(observe - expect) ** 2 / expect\n",
    "                                  for observe in row])\n",
    "    # return sum of squares\n",
    "    return np.sum(pearson_residuals)\n",
    "\n",
    "expected_clicks = 34 / 3\n",
    "expected_noclicks = 1000 - expected_clicks\n",
    "expected = [34 / 3, 1000 - 34 / 3]\n",
    "chi2observed = chi2(clicks, expected)\n",
    "\n",
    "def perm_fun(box):\n",
    "    sample_clicks = [sum(random.sample(box, 1000)),\n",
    "                     sum(random.sample(box, 1000)),\n",
    "                     sum(random.sample(box, 1000))]\n",
    "    sample_noclicks = [1000 - n for n in sample_clicks]\n",
    "    return chi2([sample_clicks, sample_noclicks], expected)\n",
    "\n",
    "perm_chi2 = [perm_fun(box) for _ in range(2000)]\n",
    "\n",
    "resampled_p_value = sum(perm_chi2 > chi2observed) / len(perm_chi2)\n",
    "print(f'Observed chi2: {chi2observed:.4f}')\n",
    "print(f'Resampled p-value: {resampled_p_value:.4f}')\n",
    "\n",
    "```\n",
    "\n",
    "This code performs a permutation test for the chi-square statistic.  It shuffles the click/no-click data and calculates the chi-square statistic many times.  The p-value indicates the probability of observing the actual chi-square statistic or a more extreme value under the assumption of independence.\n",
    "\n",
    "\n",
    "\n",
    "**Python Code (Chi-Square Test using scipy):**\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "chisq, pvalue, df, expected = stats.chi2_contingency(clicks)\n",
    "print(f'Observed chi2: {chisq:.4f}')\n",
    "print(f'p-value: {pvalue:.4f}')\n",
    "\n",
    "```\n",
    "\n",
    "This code uses the `scipy.stats` library to perform a traditional chi-square test.  The p-value is obtained using the chi-square distribution as an approximation.\n",
    "\n",
    "\n",
    "## Fisher's Exact Test\n",
    "\n",
    "Fisher's exact test is an alternative to the chi-square test, particularly useful when the expected counts are low.  It calculates the exact p-value by considering all possible arrangements of the data.  Note that an easy-to-use Python implementation for Fisher's exact test wasn't explicitly provided in the original text.\n",
    "\n",
    "\n",
    "##  Detecting Scientific Fraud (Example)\n",
    "\n",
    "The example of Thereza Imanishi-Kari's case illustrates how the chi-square test can be used to detect anomalies in data, potentially indicating data fabrication or manipulation.  The analysis involved examining the distribution of digits in the data and comparing it to the expected uniform distribution.  Deviations from the expected distribution could suggest irregularities.\n",
    "\n",
    "\n",
    "## Relevance for Data Science\n",
    "\n",
    "In data science, the chi-square test (or Fisher's exact test) isn't primarily used to establish statistical significance for publication but rather as a tool for feature selection, sample size determination, and identifying potential data anomalies.  Multi-armed bandit algorithms are often preferred for optimizing treatments in online experiments.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "\n",
    "## Multi-Arm Bandit Algorithm\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Multi-Arm Bandit (MAB) algorithm is a method for optimizing decision-making in experiments, particularly in web testing. It allows for more rapid decision-making compared to traditional statistical approaches.\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Multi-arm bandit**: An analogy for a multitreatment experiment, where each arm represents a different treatment with varying payoffs.\n",
    "- **Arm**: A treatment in an experiment (e.g., \"headline A in a web test\").\n",
    "- **Win**: The desired outcome in an experiment (e.g., \"customer clicks on the link\").\n",
    "\n",
    "### Traditional A/B Testing vs. Multi-Arm Bandits\n",
    "\n",
    "Traditional A/B testing involves collecting data to answer a specific question, such as which treatment is better. However, this approach has limitations:\n",
    "- **Inconclusive Results**: The results may not be statistically significant.\n",
    "- **Delayed Action**: You may want to act on results before the experiment concludes.\n",
    "- **Flexibility**: Traditional methods are inflexible and do not allow for changes based on new data.\n",
    "\n",
    "Bandit algorithms address these issues by allowing multiple treatments to be tested simultaneously and adjusting the sampling process based on ongoing results.\n",
    "\n",
    "### Bandit Algorithms\n",
    "\n",
    "#### Epsilon-Greedy Algorithm\n",
    "\n",
    "This is a simple algorithm for an A/B test:\n",
    "\n",
    "1. Generate a uniformly distributed random number between 0 and 1.\n",
    "2. If the number lies between 0 and epsilon (a small number between 0 and 1):\n",
    "   - Flip a fair coin (50/50 probability).\n",
    "   - If heads, show offer A.\n",
    "   - If tails, show offer B.\n",
    "3. If the number is ≥ epsilon, show the offer with the highest response rate to date.\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def epsilon_greedy(epsilon, offer_A_rate, offer_B_rate):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(['A', 'B'])\n",
    "    else:\n",
    "        return 'A' if offer_A_rate > offer_B_rate else 'B'\n",
    "```\n",
    "\n",
    "- **Epsilon**: A parameter that governs the algorithm. If epsilon is 1, it results in a standard A/B experiment. If epsilon is 0, it results in a purely greedy algorithm.\n",
    "\n",
    "#### Thompson Sampling\n",
    "\n",
    "Thompson Sampling is a more sophisticated algorithm that uses a Bayesian approach to maximize the probability of choosing the best arm. It assumes a prior distribution of rewards and updates this distribution with each draw.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- **Traditional A/B Tests**: Random sampling process, leading to excessive exposure to inferior treatments.\n",
    "- **Multi-Arm Bandits**: Alter the sampling process to incorporate information learned during the experiment, reducing the frequency of inferior treatments.\n",
    "- **Efficient Treatment**: Facilitates efficient treatment of more than two treatments.\n",
    "- **Algorithms**: Different algorithms for shifting sampling probability away from inferior treatments to the presumed superior one.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **Bandit Algorithms for Website Optimization** by John Myles White (O’Reilly, 2012).\n",
    "- **\"Analysis of Thompson Sampling for the Multi-armed Bandit Problem\"** by Shipra Agrawal and Navin Goyal.\n",
    "\n",
    "## Power and Sample Size\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Effect Size**: The minimum size of the effect you hope to detect.\n",
    "- **Power**: The probability of detecting a given effect size with a given sample size.\n",
    "- **Significance Level**: The statistical significance level at which the test will be conducted.\n",
    "\n",
    "### Determining Sample Size\n",
    "\n",
    "To decide how long a web test should run, consider the frequency of the desired goal. There is no general guidance; it depends on the goal's attainment frequency.\n",
    "\n",
    "### Steps for Statistical Calculations\n",
    "\n",
    "1. **Hypothetical Data**: Start with data representing your best guess about the results.\n",
    "2. **Second Sample**: Create a second sample by adding the desired effect size to the first sample.\n",
    "3. **Bootstrap Sample**: Draw a bootstrap sample of size n from each box.\n",
    "4. **Hypothesis Test**: Conduct a hypothesis test on the two bootstrap samples and record the significance.\n",
    "5. **Estimated Power**: Repeat the above steps many times to determine the estimated power.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you are testing a new ad against an existing ad with click-through rates of 1.1% and seeking a 10% boost to 1.21%.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Effect size calculation\n",
    "effect_size = proportion_effectsize(0.0121, 0.011)\n",
    "\n",
    "# Power analysis\n",
    "analysis = TTestIndPower()\n",
    "result = analysis.solve_power(effect_size=effect_size, alpha=0.05, power=0.8, alternative='larger')\n",
    "print('Sample Size: %.3f' % result)\n",
    "```\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- **Sample Size**: Depends on effect size, power, and significance level.\n",
    "- **Power Calculation**: Involves specifying the effect size, power, and significance level.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **Sample Size Determination and Power** by Thomas Ryan (Wiley, 2013).\n",
    "- Steve Simon's narrative-style post on the subject.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Experimental design principles, such as randomization, allow for valid conclusions about treatment effectiveness. Formal statistical inference is less critical from a data science perspective, but understanding random variation's role is essential. Intuitive resampling procedures like permutation and bootstrap help gauge the extent of chance variation in data analysis.\n",
    "\n",
    "### Technical Concepts\n",
    "\n",
    "- **Multiplication Rule**: The probability of n independent events all happening is the product of the individual probabilities. For example, the probability of two coins both landing heads is 0.5 × 0.5 = 0.25.\n",
    "\n",
    "This structured summary captures all the essential information from the provided text, including code snippets, examples, and key points.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "summaary 2\n",
    "\n",
    "# Multi-Arm Bandit Algorithm\n",
    "\n",
    "## Key Terms\n",
    "\n",
    "* **Multi-arm bandit:** A metaphor for a multi-treatment experiment, like a slot machine with multiple arms (treatments), each with different payoff probabilities.\n",
    "* **Arm:** A single treatment in the experiment (e.g., a headline variant on a website).\n",
    "* **Win:** A successful outcome of a treatment (e.g., a user clicking a link).\n",
    "\n",
    "\n",
    "## Traditional A/B Testing Limitations\n",
    "\n",
    "Traditional A/B testing has limitations:\n",
    "\n",
    "* **Inconclusive results:**  Experiments may not have enough data to definitively show a difference between treatments.\n",
    "* **Delayed action:** Results are only acted upon after the experiment concludes.\n",
    "* **Inflexibility:**  The approach doesn't easily adapt to changing data during the experiment.\n",
    "\n",
    "\n",
    "## Multi-Arm Bandits: A Superior Approach\n",
    "\n",
    "Bandit algorithms offer a more flexible and efficient way to run experiments, especially in web testing.  They allow testing multiple treatments concurrently and making quicker, data-driven decisions.\n",
    "\n",
    "The goal is to identify the best-performing arm (treatment) as quickly as possible, maximizing overall reward (e.g., clicks, conversions).  Unlike traditional A/B tests, bandit algorithms dynamically adjust the allocation of \"pulls\" (exposures) to different arms based on observed performance. Initially, all arms may be tested equally. Over time, better-performing arms receive a larger proportion of the pulls.\n",
    "\n",
    "## Epsilon-Greedy Algorithm\n",
    "\n",
    "This is a simple bandit algorithm:\n",
    "\n",
    "1. A random number between 0 and 1 is generated.\n",
    "2. If the number is less than `epsilon` (a small, pre-defined value), a coin flip decides which arm to choose (exploration).\n",
    "3. Otherwise, the arm with the highest observed success rate is chosen (exploitation).\n",
    "\n",
    "`epsilon` controls the balance between exploration (trying all arms) and exploitation (focusing on the seemingly best arm).  A higher `epsilon` means more exploration, while a lower `epsilon` leads to more exploitation.\n",
    "\n",
    "\n",
    "**No Python code was provided in the original text related to the epsilon-greedy algorithm.**\n",
    "\n",
    "\n",
    "## Thompson Sampling\n",
    "\n",
    "This is a more sophisticated algorithm employing a Bayesian approach. It starts with an initial belief about the probability of success for each arm (a prior distribution, often a Beta distribution).  After each pull, this belief is updated based on the outcome. The arm is selected probabilistically, where the probability of selecting an arm is proportional to its estimated probability of success.  This method balances exploration and exploitation effectively.\n",
    "\n",
    "**No Python code was provided in the original text related to Thompson sampling.**\n",
    "\n",
    "\n",
    "## Key Ideas of Multi-Arm Bandits\n",
    "\n",
    "* Reduce exposure to inferior treatments.\n",
    "* Adapt sampling based on learned information.\n",
    "* Efficiently handle multiple (more than two) treatments.\n",
    "\n",
    "\n",
    "## Power and Sample Size\n",
    "\n",
    "Determining the necessary sample size for an A/B test is crucial to avoid inconclusive results. This involves considering:\n",
    "\n",
    "* **Effect size:** The minimum difference between treatments you want to detect.\n",
    "* **Power:** The probability of detecting a real effect of a given size.\n",
    "* **Significance level (alpha):** The probability of falsely concluding there's a difference when there isn't.\n",
    "\n",
    "\n",
    "## Calculating Power and Sample Size: An Intuitive Approach\n",
    "\n",
    "This approach avoids complex statistical formulas:\n",
    "\n",
    "1. Create hypothetical data representing your best guess of the results for each treatment.\n",
    "2. Create a second dataset, modified to include the desired effect size.\n",
    "3. Draw bootstrap samples from each dataset.\n",
    "4. Perform a hypothesis test on the bootstrap samples and record the significance.\n",
    "5. Repeat this many times to estimate power.\n",
    "\n",
    "\n",
    "## Python Code for Power Calculation (Using `pwr` and `statsmodels`)\n",
    "\n",
    "The original text provides R code.  Since you requested Python code, equivalent Python code is shown below that achieves the same function using the `statsmodels` package.  Note that the `pwr` package is an R package, so a direct Python equivalent does not exist.\n",
    "\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Example: Calculating sample size for a 10% improvement in click-through rate\n",
    "# from 1.1% to 1.21% with 80% power and 5% significance level.\n",
    "\n",
    "effect_size = sm.stats.proportion_effectsize(0.0121, 0.011) # Calculate effect size\n",
    "analysis = sm.stats.TTestIndPower() # Initialize power analysis object\n",
    "\n",
    "# Solve for sample size\n",
    "result = analysis.solve_power(effect_size=effect_size,\n",
    "                              alpha=0.05, power=0.8, alternative='larger')\n",
    "\n",
    "print('Sample Size: %.3f' % result) \n",
    "```\n",
    "\n",
    "This code calculates the sample size needed to detect a 10% difference in click-through rates with 80% power and a 5% significance level using a one-sided test.\n",
    "\n",
    "\n",
    "## Key Ideas on Power and Sample Size\n",
    "\n",
    "* Sample size determination requires considering the planned statistical test.\n",
    "* Specify the minimum detectable effect size, desired power, and significance level.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "The document explains Multi-Arm Bandit algorithms as a superior alternative to traditional A/B testing for web optimization, highlighting their flexibility and efficiency in identifying the best-performing treatment.  It also discusses the importance of power and sample size calculations in experimental design, providing an intuitive method alongside Python code for sample size estimation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
