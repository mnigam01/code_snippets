{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llamaindex vs langchain check stars on github\n",
    "crew ai vs langgraph\n",
    "\n",
    "\n",
    "\n",
    "#### Drawbacks of Using LLMs Directly\n",
    "\n",
    "The instructor highlighted the limitations of Large Language Models (LLMs) when building generative AI applications:\n",
    "\n",
    "1. **Limited Context Window:** LLMs have input limits (token size).\n",
    "2. **Outdated Knowledge Base:** LLMs are trained on a specific dataset and may not have access to the latest information. For example, ChatGPT's knowledge base is generally considered to be up to October 2023.\n",
    "3. **Data Privacy Concerns:** Using LLMs directly can pose data privacy risks.\n",
    "4. **Cost:** Using LLMs, especially raw models, can be expensive, especially with large inputs and outputs.\n",
    "5. **Lack of Third-Party Tool Connections:** LLMs generally lack direct integration with external tools and data sources.\n",
    "\n",
    "#### Why LangChain?\n",
    "\n",
    "LangChain addresses the limitations of using LLMs directly by providing a universal framework for building generative AI applications. It acts as an orchestration framework, integrating various tools and technologies commonly used in generative AI, including:\n",
    "\n",
    "- Large Language Models (LLMs) from various providers (OpenAI, Llama, Mistral, etc.)\n",
    "- Vector databases\n",
    "- LangSmith (for monitoring)\n",
    "- LangServe (for deployment)\n",
    "- LangGraph (for agents and multi-agent systems)\n",
    "\n",
    "The instructor contrasted LangChain with LlamaIndex, stating both are good but have different capabilities. The choice to focus on LangChain was due to the series' focus and LangChain's comprehensive ecosystem. The instructor also mentioned `kuru.ai` as an alternative that builds on LangChain, offering a higher-level abstraction with less customization but easier to understand syntax. LangChain, in comparison, provides greater customization capabilities.\n",
    "\n",
    "\n",
    "\n",
    "#### LangChain Evaluation and Evolution\n",
    "\n",
    "The evolution of LangChain was discussed, covering these key points:\n",
    "\n",
    "- **From Completion Models to Chat Models:** LangChain initially primarily supported completion models (like older GPT models) but has evolved to integrate chat models, which allow for conversational interactions and system prompts. The instructor demonstrated the difference between the two using the OpenAI playground.\n",
    "  \n",
    "- **From Legacy LangChain to LangChain Expression Language (LCEL):** LangChain's syntax has evolved from a function-based approach to using a pipeline-based approach with the `|>` operator. This shift simplifies chain creation.\n",
    "  \n",
    "- **LangChain Ecosystem:** The development of a robust ecosystem, including LangSmith, LangServe, and LangGraph, enhanced LangChain’s capabilities.\n",
    "  \n",
    "- **Documentation:** The instructor noted that the LangChain documentation has historically been considered difficult to navigate.\n",
    "\n",
    "\n",
    "\n",
    "#### Learning Roadmap\n",
    "\n",
    "The proposed learning path for LangChain was outlined as follows:\n",
    "\n",
    "1. **Introduction and Overview:** (Covered in this session)\n",
    "2. **Interacting with LLMs using LangChain:** Using different LLMs (OpenAI, open-source).\n",
    "3. **Working with Data and RAG:** Loading custom data, connecting LLMs with data sources.\n",
    "4. **Chains and Runnables:** Building chains of execution using LangChain’s expression language.\n",
    "5. **Memory:** Implementing memory to maintain conversational context.\n",
    "6. **Ecosystem Exploration:** Using LangSmith, LangServe, and LangGraph.\n",
    "7. **Application Development:** Creating applications at three levels of complexity:\n",
    "   - **Level 1:** Basic Python-based applications without a UI.\n",
    "   - **Level 2:** Applications with a proof-of-concept (POC) UI using frameworks like Streamlit, Flask, or FastAPI.\n",
    "   - **Level 3:** Professional, full-stack applications.\n",
    "  \n",
    "\n",
    "# LangChain: Interacting with Large Language Models (LLMs) - Session Summary\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This session focuses on interacting with Large Language Models (LLMs) using LangChain, emphasizing practical application and local environment setup. The agenda includes connecting with OpenAI and alternative LLMs, interacting with LLMs in different languages, and an introduction to prompt templates and chains.\n",
    "\n",
    "## Rationale for Using OpenAI\n",
    "\n",
    "OpenAI is preferred for production environments due to its robustness, accuracy, and cost-effectiveness when combined with efficient prompt engineering. While open-source models like Meta Llama and Mistral are suitable for Proof of Concepts (POCs), they may not be as reliable for production use.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "The session advocates for local development to avoid dependencies on cloud environments like Google Colab. The setup involves:\n",
    "\n",
    "1. **Creating a Conda Environment:**\n",
    "   ```bash\n",
    "   conda create -n llm_app python=3.11 -y\n",
    "   conda activate llm_app\n",
    "   ```\n",
    "\n",
    "difference between conda env and virtualenv \n",
    "don't try to download the latest version try to download a just previous stable version.\n",
    "\n",
    "2. **Installing Required Packages:**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "   - `requirements.txt` includes:\n",
    "     ```\n",
    "     langchain==0.2.10\n",
    "     langchain-openai==0.1.17\n",
    "     python-dotenv\n",
    "     langchain-grok==0.1.6\n",
    "     ```\n",
    "\n",
    "## Connecting to OpenAI Models\n",
    "\n",
    "### Loading API Keys\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def load_env():\n",
    "    load_dotenv()\n",
    "    return os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_api_key = load_env()\n",
    "```\n",
    "\n",
    "completion model is only for completing things they don't chat\n",
    "chat model is for conversation\n",
    "\n",
    "### Using OpenAI Completion Model\n",
    "```python\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm_model = OpenAI()\n",
    "response = llm_model.invoke(\"Tell me one fact about the Kennedy family.\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "# for streaming\n",
    "for chunk in llmModel.stream(\n",
    "\"Tell me one fun fact about the Kennedy family in detail\"\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "### Using OpenAI Chat Model\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a historian expert in the Kennedy family.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me one curious thing about JFK.\"},\n",
    "]\n",
    "or \n",
    "messages = [\n",
    "    (\"system\", \"You are a historian expert in the Kennedy family.\"),\n",
    "    (\"user\", \"Tell me one curious thing about JFK.\"),\n",
    "]\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)\n",
    "response.schema()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Alternative LLMs via Grok\n",
    "\n",
    "### Loading and Using Grok LLMs\n",
    "```python\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "grok_api_key = load_env()\n",
    "os.environ[\"GROQ_API_KEY\"] = grok_api_key\n",
    "llama_model = ChatGroq(model=\"llama-3-7b-8192\")\n",
    "mistral_model = ChatGroq(model=\"mistral-7b-v0.1\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a historian expert in the Kennedy family.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many members of the family died tragically?\"},\n",
    "]\n",
    "llama_response = llama_model.invoke(messages)\n",
    "print(llama_response.content)\n",
    "```\n",
    "\n",
    "## Tips and Best Practices\n",
    "\n",
    "- **Stable Versions:** Use specific versions of packages to avoid breaking changes.\n",
    "- **Local Setup:** Focus on local development for real-world projects.\n",
    "- **Environment Variables:** Manage sensitive information like API keys using `.env` files.\n",
    "\n",
    "## Conclusion and Resources\n",
    "\n",
    "The session concludes with a summary, emphasizing the importance of using chat models over completion models in modern applications. \n",
    "\n",
    "\n",
    "## LangChain & LLM Ops Series: Day 3 - Prompt Templates, Chains, and Output Parsers\n",
    "\n",
    "This document summarizes Day 3 of a LangChain and LLM Ops series, focusing on prompt templates, chains, and output parsers.\n",
    "\n",
    "\n",
    "### II. Today's Agenda (Day 3)\n",
    "\n",
    "1. **Prompt Templates**\n",
    "2. **Chains**\n",
    "3. **Output Parsers**\n",
    "\n",
    "### III. Prompt Templates\n",
    "\n",
    "**A. What is a Prompt?**\n",
    "\n",
    "A prompt is a way to communicate with LLMs, essentially a programming language for them.\n",
    "\n",
    "**B. What is a Prompt Template?**\n",
    "\n",
    "A prompt template allows dynamic input during runtime, enabling the insertion of variables instead of hardcoding prompts.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Instead of multiple hardcoded prompts, a template can be used:\n",
    "\n",
    "\n",
    "\n",
    "prompttemplate is for completion model.\n",
    "chatprompttemplate is for chat model\n",
    "\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"topic\"],\n",
    "    template=\"Tell me a {adjective} story about {topic}.\",\n",
    ")\n",
    "\n",
    "llm_prompt = prompt_template.format(adjective=\"curious\", topic=\"the Kennedy family\")\n",
    "response = llm(llm_prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        (\"user\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"assistant\", \"Sure\"),\n",
    "        (\"user\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "message = chat_template.format_messages(profession=\"software engineer\", topic=\"python\", user_input=\"What is the output of print(1+1)?\")\n",
    "\n",
    "```\n",
    "\n",
    "###   Prompt things\n",
    "\n",
    "\n",
    "Certainly! Let's break down the differences among these prompt templates and understand when to use each one with examples.\n",
    "\n",
    "### 1. **BasePromptTemplate**\n",
    "   - **Description**: This is the base class for all prompt templates. It defines the common interface and methods that all prompt templates should implement.\n",
    "   - **Usage**: You typically don't use this directly but rather as a base class for other prompt templates.\n",
    "\n",
    "### 2. **StringPromptTemplate**\n",
    "   - **Description**: This is a simple prompt template that outputs a single string. It is used when you want to generate a prompt as a single string.\n",
    "   - **Usage**: \n",
    "     ```python\n",
    "     from langchain import StringPromptTemplate\n",
    "\n",
    "     template = StringPromptTemplate(template=\"Translate the following text to {language}: {text}\")\n",
    "     prompt = template.format(language=\"French\", text=\"Hello, how are you?\")\n",
    "     print(prompt)  # Output: \"Translate the following text to French: Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 3. **PromptTemplate**\n",
    "   - **Description**: This is a more general prompt template that can handle multiple input variables and format them into a single string.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import PromptTemplate\n",
    "\n",
    "     template = PromptTemplate(template=\"Translate the following text to {language}: {text}\", input_variables=[\"language\", \"text\"])\n",
    "     prompt = template.format(language=\"Spanish\", text=\"Hello, how are you?\")\n",
    "     print(prompt)  # Output: \"Translate the following text to Spanish: Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 4. **ChatPromptTemplate**\n",
    "   - **Description**: This is used for generating prompts for chat-based models. It can handle multiple messages (e.g., system, user, assistant messages) and format them into a structured chat prompt.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "     system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n",
    "     human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "\n",
    "     chat_prompt = ChatPromptTemplate.from_messages([system_template, human_template])\n",
    "     prompt = chat_prompt.format_prompt(text=\"Translate the following text to French: Hello, how are you?\").to_messages()\n",
    "     print(prompt)\n",
    "     # Output: [SystemMessage(content='You are a helpful assistant.'), HumanMessage(content='Translate the following text to French: Hello, how are you?')]\n",
    "     ```\n",
    "\n",
    "### 5. **SystemMessagePromptTemplate**\n",
    "   - **Description**: This is used to create a system message in a chat prompt. System messages typically set the context or role of the assistant.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import SystemMessagePromptTemplate\n",
    "\n",
    "     system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n",
    "     print(system_template.format())  # Output: \"You are a helpful assistant.\"\n",
    "     ```\n",
    "\n",
    "### 6. **HumanMessagePromptTemplate**\n",
    "   - **Description**: This is used to create a human message in a chat prompt. Human messages are typically the input from the user.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import HumanMessagePromptTemplate\n",
    "\n",
    "     human_template = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "     print(human_template.format(text=\"Hello, how are you?\"))  # Output: \"Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 7. **AIMessagePromptTemplate**\n",
    "   - **Description**: This is used to create an AI message in a chat prompt. AI messages are typically the responses from the assistant.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import AIMessagePromptTemplate\n",
    "\n",
    "     ai_template = AIMessagePromptTemplate.from_template(\"{response}\")\n",
    "     print(ai_template.format(response=\"I'm doing well, thank you!\"))  # Output: \"I'm doing well, thank you!\"\n",
    "     ```\n",
    "\n",
    "### 8. **FewShotPromptTemplate**\n",
    "   - **Description**: This is used to create a prompt that includes few-shot examples. Few-shot examples are used to guide the model by showing it how to respond to similar inputs.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "     examples = [\n",
    "         {\"input\": \"Hello\", \"output\": \"Hi there!\"},\n",
    "         {\"input\": \"How are you?\", \"output\": \"I'm doing well, thank you!\"}\n",
    "     ]\n",
    "\n",
    "     example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n",
    "     few_shot_prompt = FewShotPromptTemplate(\n",
    "         examples=examples,\n",
    "         example_prompt=example_prompt,\n",
    "         prefix=\"Translate the following text to French:\",\n",
    "         suffix=\"Input: {text}\\nOutput:\",\n",
    "         input_variables=[\"text\"]\n",
    "     )\n",
    "\n",
    "     prompt = few_shot_prompt.format(text=\"Hello, how are you?\")\n",
    "     print(prompt)\n",
    "     # Output: \"Translate the following text to French:\\nInput: Hello\\nOutput: Hi there!\\nInput: How are you?\\nOutput: I'm doing well, thank you!\\nInput: Hello, how are you?\\nOutput:\"\n",
    "     ```\n",
    "\n",
    "### 9. **PipelinePromptTemplate**\n",
    "   - **Description**: This is used to create a pipeline of prompts where the output of one prompt is used as the input for the next.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import PipelinePromptTemplate, PromptTemplate\n",
    "\n",
    "     full_template = PromptTemplate(template=\"Final answer: {answer}\", input_variables=[\"answer\"])\n",
    "     partial_template = PromptTemplate(template=\"Translate the following text to {language}: {text}\", input_variables=[\"language\", \"text\"])\n",
    "\n",
    "     pipeline_prompt = PipelinePromptTemplate(\n",
    "         final_prompt=full_template,\n",
    "         pipeline_prompts=[(\"answer\", partial_template)]\n",
    "     )\n",
    "\n",
    "     prompt = pipeline_prompt.format(language=\"French\", text=\"Hello, how are you?\")\n",
    "     print(prompt)  # Output: \"Final answer: Translate the following text to French: Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 10. **MessagesPlaceholder**\n",
    "   - **Description**: This is used to dynamically insert a list of messages into a chat prompt. It allows you to dynamically generate or modify the messages in the prompt.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "\n",
    "     chat_prompt = ChatPromptTemplate.from_messages([\n",
    "         MessagesPlaceholder(variable_name=\"history\"),\n",
    "         HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "     ])\n",
    "\n",
    "     messages = [HumanMessage(content=\"Hello\"), AIMessage(content=\"Hi there!\")]\n",
    "     prompt = chat_prompt.format_prompt(history=messages, input=\"How are you?\").to_messages()\n",
    "     print(prompt)\n",
    "     # Output: [HumanMessage(content='Hello'), AIMessage(content='Hi there!'), HumanMessage(content='How are you?')]\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "- **BasePromptTemplate**: Base class, not used directly.\n",
    "- **StringPromptTemplate**: Simple string-based prompts.\n",
    "- **PromptTemplate**: General-purpose prompt with multiple input variables.\n",
    "- **ChatPromptTemplate**: Structured chat prompts with multiple message types.\n",
    "- **SystemMessagePromptTemplate**: System messages in chat prompts.\n",
    "- **HumanMessagePromptTemplate**: Human messages in chat prompts.\n",
    "- **AIMessagePromptTemplate**: AI messages in chat prompts.\n",
    "- **FewShotPromptTemplate**: Prompts with few-shot examples.\n",
    "- **PipelinePromptTemplate**: Pipelined prompts.\n",
    "- **MessagesPlaceholder**: Dynamically insert messages into chat prompts.\n",
    "\n",
    "Each of these templates is designed for a specific use case, and choosing the right one depends on the structure and complexity of the prompt you need to generate.\n",
    "\n",
    "\n",
    "Certainly! The methods like `from_message`, `from_template`, and others are convenience methods provided by LangChain to simplify the creation of prompt templates. Let's break down each of these methods and understand how they work.\n",
    "\n",
    "### 1. **from_template**\n",
    "   - **Description**: This method is used to create a prompt template from a string template. It automatically infers the input variables from the template string.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import PromptTemplate\n",
    "\n",
    "     template = PromptTemplate.from_template(\"Translate the following text to {language}: {text}\")\n",
    "     prompt = template.format(language=\"French\", text=\"Hello, how are you?\")\n",
    "     print(prompt)  # Output: \"Translate the following text to French: Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 2. **from_message**\n",
    "   - **Description**: This method is used to create a message template from a message object. It is typically used in chat-based models to create message templates from existing message objects.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import HumanMessagePromptTemplate, HumanMessage\n",
    "\n",
    "     message = HumanMessage(content=\"Hello, how are you?\")\n",
    "     message_template = HumanMessagePromptTemplate.from_message(message)\n",
    "     print(message_template.format())  # Output: \"Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 3. **from_role**\n",
    "   - **Description**: This method is used to create a message template from a role. It is typically used in chat-based models to create message templates based on the role of the message (e.g., system, user, assistant).\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import HumanMessagePromptTemplate\n",
    "\n",
    "     message_template = HumanMessagePromptTemplate.from_role(role=\"user\")\n",
    "     print(message_template.format(content=\"Hello, how are you?\"))  # Output: \"Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 4. **from_examples**\n",
    "   - **Description**: This method is used to create a few-shot prompt template from a list of examples. It is useful for guiding the model with examples of how to respond to similar inputs.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "     examples = [\n",
    "         {\"input\": \"Hello\", \"output\": \"Hi there!\"},\n",
    "         {\"input\": \"How are you?\", \"output\": \"I'm doing well, thank you!\"}\n",
    "     ]\n",
    "\n",
    "     example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n",
    "     few_shot_prompt = FewShotPromptTemplate.from_examples(\n",
    "         examples=examples,\n",
    "         example_prompt=example_prompt,\n",
    "         prefix=\"Translate the following text to French:\",\n",
    "         suffix=\"Input: {text}\\nOutput:\",\n",
    "         input_variables=[\"text\"]\n",
    "     )\n",
    "\n",
    "     prompt = few_shot_prompt.format(text=\"Hello, how are you?\")\n",
    "     print(prompt)\n",
    "     # Output: \"Translate the following text to French:\\nInput: Hello\\nOutput: Hi there!\\nInput: How are you?\\nOutput: I'm doing well, thank you!\\nInput: Hello, how are you?\\nOutput:\"\n",
    "     ```\n",
    "\n",
    "### 5. **from_file**\n",
    "   - **Description**: This method is used to create a prompt template from a file. It reads the template from a file and infers the input variables.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import PromptTemplate\n",
    "\n",
    "     template = PromptTemplate.from_file(\"path/to/template.txt\")\n",
    "     prompt = template.format(language=\"French\", text=\"Hello, how are you?\")\n",
    "     print(prompt)  # Output: \"Translate the following text to French: Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### 6. **from_prompt**\n",
    "   - **Description**: This method is used to create a prompt template from another prompt template. It is useful when you want to reuse or extend an existing prompt template.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from langchain import PromptTemplate\n",
    "\n",
    "     base_template = PromptTemplate(template=\"Translate the following text to {language}: {text}\", input_variables=[\"language\", \"text\"])\n",
    "     extended_template = PromptTemplate.from_prompt(base_template, additional_input_variables=[\"context\"])\n",
    "     prompt = extended_template.format(language=\"French\", text=\"Hello, how are you?\", context=\"Formal conversation\")\n",
    "     print(prompt)  # Output: \"Translate the following text to French: Hello, how are you?\"\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "- **from_template**: Create a prompt template from a string template.\n",
    "- **from_message**: Create a message template from a message object.\n",
    "- **from_role**: Create a message template from a role.\n",
    "- **from_examples**: Create a few-shot prompt template from a list of examples.\n",
    "- **from_file**: Create a prompt template from a file.\n",
    "- **from_prompt**: Create a prompt template from another prompt template.\n",
    "\n",
    "These methods provide a convenient way to create and manage prompt templates, making it easier to work with different types of prompts and messages in LangChain.\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"The capital of France is Paris.\",\n",
    "        \"source\": \"Wikipedia\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the capital of Germany?\",\n",
    "        \"answer\": \"The capital of Germany is Berlin.\",\n",
    "        \"source\": \"Wikipedia\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"user\",\"{input}\"),\n",
    "        (\"assistant\",\"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    \n",
    ")\n",
    "chain  = final_prompt | llm \n",
    "\n",
    "chain.invoke({\"input\":\"How are you\"})\n",
    "\n",
    "\n",
    "\n",
    "### IV. Chains\n",
    "\n",
    "**A. Understanding Chains**\n",
    "\n",
    "Chains represent sequences of executable actions, including prompts, models, and output parsers.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from langchian.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers import SimpleJsonOutputParser\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"Return a JSON object with an 'answer' key that answers the following question: {question}.\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "llm = OpenAI()\n",
    "output_parser = SimpleJsonOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is the biggest country?\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### V. Output Parsers\n",
    "\n",
    "**A. Understanding Output Parsers**\n",
    "\n",
    "Output parsers format the LLM's response into a specific structure.\n",
    "\n",
    "**Example: JSON Output Parser**\n",
    "\n",
    "\n",
    "\n",
    "**B. Custom Output Parser using Pydantic**\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "    \n",
    "    \n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n {format_instructions}\\n{user_query}\\n\",\n",
    "    input_variables=[\"user_query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"user_query\": \"What is the meaning of life?\"})\n",
    "```\n",
    "\n",
    "\n",
    "The `partial_variables` parameter in the `PromptTemplate` class allows you to specify variables that are pre-filled with values when the template is created. These variables are not expected to be provided as input when the template is used. Instead, they are included in the template with predefined values.\n",
    "\n",
    "In your example, `partial_variables` is used to pre-fill the `format_instructions` variable with the value returned by `parser.get_format_instructions()`. This means that when you create the `PromptTemplate`, the `format_instructions` variable will already have a value, and you won't need to provide it as input when you use the template.\n",
    "\n",
    "Here's a breakdown of the code and an explanation of how `partial_variables` works:\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Assume parser is an object with a method get_format_instructions()\n",
    "class Parser:\n",
    "    def get_format_instructions(self):\n",
    "        return \"Please provide the answer in a clear and concise manner.\"\n",
    "\n",
    "parser = Parser()\n",
    "\n",
    "# Create the prompt template with partial variables\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n {format_instructions}\\n{user_query}\\n\",\n",
    "    input_variables=[\"user_query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What is the capital of Italy?\"\n",
    "formatted_prompt = prompt.format(user_query=user_query)\n",
    "print(formatted_prompt)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Parser Class**:\n",
    "   - The `Parser` class has a method `get_format_instructions()` that returns a string with format instructions.\n",
    "\n",
    "2. **PromptTemplate**:\n",
    "   - The `PromptTemplate` class is used to create a prompt template.\n",
    "   - The `template` parameter defines the structure of the prompt, including placeholders for variables.\n",
    "   - The `input_variables` parameter specifies the variables that will be provided as input when the template is used. In this case, `user_query` is the input variable.\n",
    "   - The `partial_variables` parameter specifies the variables that are pre-filled with values. In this case, `format_instructions` is pre-filled with the value returned by `parser.get_format_instructions()`.\n",
    "\n",
    "3. **Example Usage**:\n",
    "   - The `format` method of the `PromptTemplate` instance is used to generate the formatted prompt.\n",
    "   - The `user_query` variable is provided as input, and the `format_instructions` variable is already pre-filled.\n",
    "\n",
    "### Output\n",
    "\n",
    "The output will be:\n",
    "\n",
    "```\n",
    "Answer the user query.\n",
    "Please provide the answer in a clear and concise manner.\n",
    "What is the capital of Italy?\n",
    "```\n",
    "\n",
    "### Benefits of Using `partial_variables`\n",
    "\n",
    "1. **Pre-filled Values**:\n",
    "   - You can pre-fill certain variables with values that are constant or predefined, reducing the need to provide them as input every time you use the template.\n",
    "\n",
    "2. **Simplified Input**:\n",
    "   - By pre-filling some variables, you simplify the input required to use the template, making it easier to use and reducing the risk of errors.\n",
    "\n",
    "3. **Consistency**:\n",
    "   - Pre-filled variables ensure consistency in the prompt, as the same values are used every time the template is used.\n",
    "\n",
    "By using `partial_variables`, you can create more flexible and reusable prompt templates that are easier to work with.\n",
    "\n",
    "\n",
    "\n",
    "# Day Four: Langchain and LLM Ops Series - Working with Data and RAG Basic Demo\n",
    "\n",
    "This document summarizes a technical session on working with data and Retrieval Augmented Generation (RAG) using Langchain.\n",
    "\n",
    "## I. Introduction\n",
    "\n",
    "The session is part of a series covering Langchain and LLM Ops. The focus is on the basics of RAG, specifically how to work with custom data and overcome limitations of directly feeding large datasets to LLMs. The session covers data loading, an introduction to RAG, its components, and a basic RAG demo with Langchain's expression language.\n",
    "\n",
    "## II. Agenda\n",
    "\n",
    "1. **Data Loading:** Techniques for loading custom data in various formats.\n",
    "2. **Introduction to RAG:** Explanation of RAG and its components.\n",
    "3. **RAG Components:** Detailed explanation of splitter, embeddings, vector store, retriever, and top-k.\n",
    "4. **RAG Demo:** Building a RAG system using Langchain's expression language and custom data.\n",
    "\n",
    "## III. Limitations of Direct LLM Input\n",
    "\n",
    "Directly using pre-trained LLMs without custom data has limitations. For example, querying about specific information not known to the model results in unsatisfactory responses.\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0301\")\n",
    "\n",
    "prompt = \"\"\"You are an helpful assistant.\n",
    "Tell me about DS with Bubby\"\"\"\n",
    "\n",
    "response = llm(prompt)\n",
    "print(response)  # Output indicates lack of specific information.\n",
    "```\n",
    "\n",
    "## IV. Data Loading with Langchain\n",
    "\n",
    "Langchain provides tools to load data in various formats. Install necessary packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain-community\n",
    "```\n",
    "\n",
    "### A. TXT Data Loading\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('data/b_good.txt')\n",
    "loaded_data = loader.load()\n",
    "print(loaded_data)  # List of Document objects\n",
    "```\n",
    "\n",
    "### B. CSV Data Loading\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader('data/straight_tree_list.csv')\n",
    "loaded_data = loader.load()\n",
    "print(loaded_data)  # List of Document objects per row\n",
    "```\n",
    "\n",
    "### C. HTML Data Loading\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import UnstructuredURLLoader UnstructedHTMLLoader\n",
    "\n",
    "# Load HTML data (path or URL to HTML file)\n",
    "```\n",
    "\n",
    "### D. PDF Data Loading\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('data/mypdf.pdf')\n",
    "loaded_data = loader.load()\n",
    "print(loaded_data)  # List of Document objects per page\n",
    "```\n",
    "\n",
    "### E. Wikipedia Data Loading\n",
    "\n",
    "```python\n",
    "from langchain.utilities import WikipediaAPILoader\n",
    "\n",
    "loader = WikipediaAPILoader(query=\"Tesla\", load_max_docs=1)\n",
    "loaded_data =loader.load()[0].page_content\n",
    "print(loaded_data)  # List of Document objects\n",
    "```\n",
    "\n",
    "## V. Drawbacks of Directly Loading Large Datasets\n",
    "\n",
    "1. **Input Limit:** LLMs have context window limits (e.g., 16385 tokens for `gpt-3.5-turbo`).\n",
    "2. **Cost:** Processing large datasets increases costs due to per-token charges.\n",
    "\n",
    "## VI. Introduction to RAG (retrieval augmented generation)\n",
    "\n",
    "RAG overcomes these limitations by:\n",
    "\n",
    "1. **Splitting:** Dividing documents into smaller chunks.\n",
    "2. **Embedding:** Converting text chunks into numerical vectors.\n",
    "3. **Vector Database:** Storing embeddings for efficient retrieval.\n",
    "4. **Retrieval:** Fetching relevant embeddings based on user queries.\n",
    "5. **LLM Processing:** Using retrieved context with queries to generate responses.\n",
    "\n",
    "## VII. Next Steps and Conclusion\n",
    "\n",
    "The next session will delve into RAG components and demonstrate building a RAG application. The speaker shares resources like GitHub repositories and suggests skills for generative AI engineering. The session concludes with a Q&A and a thank you to viewers.\n",
    "\n",
    "--- \n",
    "\n",
    "This summary consolidates key points from both sessions, providing a comprehensive overview of working with data and RAG using Langchain.\n",
    "\n",
    "## Summary of Technical Session on RAG Components\n",
    "\n",
    "### I. Introduction\n",
    "\n",
    "The session focuses on Retrieval-Augmented Generation (RAG) components, emphasizing the importance of understanding these components before implementing a RAG-based system. The previous session discussed data loaders and the necessity of splitting large datasets for large language models (LLMs). The current session covers the core components of RAG: data splitting, embedding generation, vector database storage, and retrievers. The next session will cover the full implementation of RAG using LangChain.\n",
    "\n",
    "### II. Data Splitting (Chunking)\n",
    "\n",
    "**Purpose:**  \n",
    "Dividing large datasets into smaller chunks suitable for LLM input to manage input limits of LLMs.\n",
    "\n",
    "**Techniques:**\n",
    "\n",
    "1. **Character Text Splitter:**\n",
    "   - **Parameters:** `separator`, `chunk_size`, `chunk_overlap`, `length_function`, `is_separator_regex`.\n",
    "   - **Example Code:**\n",
    "     ```python\n",
    "     from langchain.text_splitter import CharacterTextSplitter\n",
    "     from langchain_community.document_loader import TxtLoader\n",
    "\n",
    "     # Load data\n",
    "     data = TxtLoader(\"in.txt\").load()\n",
    "\n",
    "     # Initialize CharacterTextSplitter\n",
    "     text_splitter = CharacterTextSplitter(\n",
    "         separator='\\n',\n",
    "         chunk_size=1000,\n",
    "         chunk_overlap=200,\n",
    "         length_function=len,\n",
    "     )\n",
    "\n",
    "     # Split the data\n",
    "     chunks = text_splitter.create_documents([data[0].page_content])\n",
    "\n",
    "     # Access individual chunks (example)\n",
    "     print(chunks[0].page_content)  # First chunk\n",
    "     print(len(chunks)) # Number of chunks\n",
    "     ```\n",
    "\n",
    "recursive text spliter first separated by x, then by y and recursivelt keep on splitting.you can check what symbols it splits on.\n",
    "\n",
    "2. **Recursive Character Text Splitter:**\n",
    "   - **Example Code:**\n",
    "     ```python\n",
    "     from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "     # ... (load data as before) ...\n",
    "     recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "     chunks = recursive_splitter.create_documents([data])\n",
    "     #... (access chunks as before) ...\n",
    "     ```\n",
    "\n",
    "### III. Embedding Generation\n",
    "\n",
    "**Purpose:**  \n",
    "Transforming text chunks into numerical vector representations (embeddings) for efficient similarity searching in vector databases.\n",
    "\n",
    "**Example Code:**\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Generate embeddings\n",
    "text_chunks = [\"hi there\", \"hello\", \"what's your name\", \"james bond\"]\n",
    "embeddings_list = embeddings.embed_documents(text_chunks)\n",
    "\n",
    "# Access the embeddings (example)\n",
    "print(embeddings_list[0]) #First embedding.\n",
    "print(len(embeddings_list[0])) # Dimension of embeddings.\n",
    "```\n",
    "\n",
    "### IV. Vector Database Storage\n",
    "\n",
    "**Purpose:**  \n",
    "Storing and retrieving embeddings efficiently.\n",
    "\n",
    "**Vector Databases Mentioned:** ChromaDB, FAISS, Pinecone, Weaviate, Qdrant.\n",
    "\n",
    "**Example using ChromaDB:**\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector database\n",
    "loaded_document = TextLoader('./data/state_of_the_union.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks_of_text = text_splitter.split_documents(loaded_document)\n",
    "db = Chroma.from_documents(chunks_of_text, OpenAIEmbeddings())\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"What did the president say about the John Lewis Voting Rights Act?\"\n",
    "results = db.similarity_search(query)\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "```\n",
    "\n",
    "**Example using FAISS:**\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "# ... (load data, split, embed as before) ...\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "# ... (Perform similarity search and print results as before) ...\n",
    "```\n",
    "\n",
    "### V. Retrievers\n",
    "\n",
    "**Purpose:**  \n",
    "Providing a more powerful method for accessing information than simple similarity search, returning metadata (source document and location) along with the retrieved text.\n",
    "\n",
    "**Example using FAISS Retriever:**\n",
    "```python\n",
    "# ... (load data, split, embed, and create FAISS vectorstore as before) ...\n",
    "\n",
    "# Create retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "retriever.invoke(query)\n",
    "\n",
    "# Retrieve information\n",
    "query = \"What did he say about kitten?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print results with metadata\n",
    "print(results)\n",
    "```\n",
    "\n",
    "**Parameter `k` (top_k):**  \n",
    "Controls the number of retrieved documents. The default is 4. This can be adjusted in the retriever creation (e.g., `retriever = db.as_retriever(search_kwargs={\"k\": 2})`).\n",
    "\n",
    "### VI. Tips\n",
    "\n",
    "- **Practice:** Download and execute the provided code to understand the concepts better.\n",
    "- **Resources:** Utilize the provided playlists and documentation for further learning.\n",
    "- **Model Selection:** For professional applications, prefer using powerful models like OpenAI's.\n",
    "\n",
    "# Summary: Implementing Basic RAG with LangChain Expression Language\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This session focuses on implementing a basic Retrieval-Augmented Generation (RAG) system using LangChain's Expression Language (LCEL). Building on previous sessions that covered LangChain introductions, connecting to LLMs, prompt templates, and RAG components, this tutorial provides a comprehensive guide to creating a RAG system.\n",
    "\n",
    "## Key Concepts and Components\n",
    "\n",
    "### RAG Components\n",
    "\n",
    "1. **Data Splitting (Chunking)**:\n",
    "   - Custom data, such as PDFs, TXT, or XML, is split into manageable chunks.\n",
    "\n",
    "2. **Embedding Generation**:\n",
    "   - Vector embeddings are generated using models like OpenAI's embeddings.\n",
    "\n",
    "3. **Vector Storage**:\n",
    "   - Embeddings are stored in a vector database, such as FAISS, forming a knowledge base.\n",
    "\n",
    "4. **Retriever**:\n",
    "   - A retriever fetches the most relevant chunks based on user queries.\n",
    "\n",
    "5. **Large Language Model (LLM)**:\n",
    "   - The LLM processes the retrieved context and query to generate accurate responses.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - Custom data is loaded into the system.\n",
    "\n",
    "2. **Chunking**:\n",
    "   - Data is split into smaller chunks using techniques like `CharacterTextSplitter`.\n",
    "\n",
    "3. **Embedding Generation**:\n",
    "   - Vector embeddings are generated for each chunk.\n",
    "\n",
    "4. **Vector Storage**:\n",
    "   - Embeddings are stored in a vector database.\n",
    "\n",
    "5. **Retriever**:\n",
    "   - Retrieves relevant information based on user queries.\n",
    "\n",
    "6. **LLM Connection**:\n",
    "   - The LLM generates responses based on retrieved context and user queries.\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "### Step-by-Step Implementation\n",
    "\n",
    "1. **Loading the API Key**\n",
    "\n",
    "   ```python\n",
    "   import os\n",
    "   from dotenv import load_dotenv\n",
    "\n",
    "   load_dotenv()\n",
    "   openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "   ```\n",
    "\n",
    "2. **Importing Libraries**\n",
    "\n",
    "   ```python\n",
    "   from langchain.vectorstores import FAISS\n",
    "   from langchain.embeddings import OpenAIEmbeddings\n",
    "   from langchain.text_splitter import CharacterTextSplitter\n",
    "   from langchain.document_loaders import TextLoader\n",
    "   from langchain.chains import LLMChain\n",
    "   from langchain_openai import ChatOpenAI\n",
    "   from langchain.prompts import ChatPromptTemplate\n",
    "   from langchain_core.output_parsers import StrOutputParser\n",
    "   from langchain_core.runnables import RunnablePassthrough\n",
    "   ```\n",
    "\n",
    "3. **Loading Data**\n",
    "\n",
    "   ```python\n",
    "   loader = TextLoader('state_of_the_union.txt')\n",
    "   documents = loader.load()\n",
    "   ```\n",
    "\n",
    "4. **Data Splitting (Chunking)**\n",
    "\n",
    "   ```python\n",
    "   text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "   chunks = text_splitter.split_documents(documents)\n",
    "   print(f\"Number of chunks: {len(chunks)}\")\n",
    "   ```\n",
    "\n",
    "5. **Embedding Generation and Vector Storage**\n",
    "\n",
    "   ```python\n",
    "   embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "   vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "   vectorstore.save_local(\"faiss_index\")\n",
    "   ```\n",
    "\n",
    "6. **Creating the Retriever**\n",
    "\n",
    "   ```python\n",
    "   retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "   ```\n",
    "\n",
    "7. **Connecting to the LLM**\n",
    "\n",
    "   ```python\n",
    "   template = \"\"\"Answer the question based on the context below.\n",
    "   Context: {context}\n",
    "   Question: {question}\n",
    "   \"\"\"\n",
    "   prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "   llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "   def format_documents(docs):\n",
    "       return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "   chain = (\n",
    "   {\"context\" : retriever | format_docs, \"question\": RunnablePassthrough}\n",
    "   | prompt | model | StrOutputParser()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "8. **Querying the Chain**\n",
    "\n",
    "   ```python\n",
    "   response = chain.invoke(\"What did he say about Ketanji Brown Jackson?\")\n",
    "   print(response)\n",
    "   ```\n",
    "\n",
    "## Tips and Best Practices\n",
    "\n",
    "- **Parameter Tuning**: Adjust the `top_k` parameter in the retriever to control the number of retrieved results.\n",
    "- **Context Management**: Ensure the context provided to the LLM is accurate and relevant.\n",
    "- **Output Parsing**: Use output parsers to format the LLM's response as needed.\n",
    "\n",
    "# LangChain Expression Language (LCEL) - Day 7 Summary\n",
    "\n",
    "\n",
    "## Introduction to LCEL\n",
    "\n",
    "LCEL is presented as a newer, more efficient approach to creating chains in LangChain compared to the legacy method. The instructor emphasizes that while legacy chains are still supported, LCEL is favored for its enhanced functionality and integration with newer LangChain tools. The core concept is building chains as sequences of \"runnables,\" which are executable objects. The primary difference between the legacy and LCEL approaches lies in how chains are defined: legacy chains use predefined functions from the `langchain.chains` module, while LCEL uses a pipe (`|`) symbol to sequence runnables.\n",
    "\n",
    "## High-Level Architecture of LangChain Applications\n",
    "\n",
    "The lecture illustrates the typical architecture of a LangChain application:\n",
    "\n",
    "1. **Data Input:** The process begins with input data.\n",
    "2. **Action:** Actions are performed on the data, including prompt creation and chain execution.\n",
    "3. **Runnables:** These actions involve the use of runnables (executable objects like prompts, models, and parsers).\n",
    "4. **LLM Interaction:** The data is passed to the Large Language Model (LLM).\n",
    "5. **Output:** The LLM generates an output.\n",
    "\n",
    "LCEL plays a crucial role in the \"Action\" stage.\n",
    "\n",
    "## Legacy LangChain vs. LCEL\n",
    "\n",
    "The instructor contrasts the legacy LangChain approach with the new LCEL method. Legacy chains utilize predefined chain functions (e.g., `RetrievalQA`, `LLMMathChain`, `LLMCheckerChain`), requiring explicit function calls and parameter passing. LCEL, in contrast, uses a more concise syntax with the pipe symbol (`|`) to chain runnables together. The order of runnables in LCEL is strictly left-to-right, unlike the legacy approach where order may not always be critical. The instructor notes that legacy methods are still relevant in some cases and that LCEL is under active development.\n",
    "\n",
    "## Example: Basic LCEL Chain\n",
    "\n",
    "The following Python code demonstrates a basic LCEL chain using OpenAI's `gpt-3.5-turbo` model:\n",
    "\n",
    "```python\n",
    "# Import necessary modules\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = \"\"\"Tell me a curious fact about {politician}\"\"\"\n",
    "\n",
    "# Define the chain using LCEL syntax\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(prompt_template)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain with an input\n",
    "politician = \"JFK\"\n",
    "output = chain.invoke({\"politician\": politician})\n",
    "print(output) \n",
    "\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "for s in chain.stream({\"politician\": politician}):\n",
    "    print(s, end=\"\", flush=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "chain.batch([{\"politician\": politician}, {\"politician\": politician}])\n",
    "```\n",
    "\n",
    "if you import something from chains it is legacy code\n",
    "\n",
    "all things are runnable means they can be executed\n",
    "\n",
    "This code creates a chain that:\n",
    "\n",
    "1. Constructs a prompt using `ChatPromptTemplate`.\n",
    "2. Passes the prompt to the LLM (`llm`).\n",
    "3. Uses `StrOutputParser` to extract the string content from the LLM's AI message format response.\n",
    "\n",
    "**Key Point:** The pipe symbol (`|`) connects the runnables, enforcing left-to-right execution. The `StrOutputParser` is crucial for converting the LLM's response into a usable string.\n",
    "\n",
    "## Runnable Execution Order and Alternatives\n",
    "\n",
    "The lecture emphasizes the left-to-right execution order of runnables in LCEL chains. An image visually demonstrates the flow: Input -> Prompt -> LLM -> Output Parser -> Output.\n",
    "\n",
    "The lecture also presents alternatives to the `invoke` method for executing chains:\n",
    "\n",
    "- **Streaming:** Using `chain.stream()` allows for streaming the output from the LLM, word by word, instead of receiving the entire output at once. Example code is provided demonstrating this approach.\n",
    "\n",
    "- **Batching:** Using the chain with a list of inputs enables processing multiple inputs in batches. Example code is provided, demonstrating how to pass a list of soccer players to the chain and receive responses for each.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language Session Summary\n",
    "\n",
    "\n",
    "### Agenda\n",
    "- **Focus:** Built-in Runnables in LangChain Expression Language (LCEL).\n",
    "- **Subtopics:**\n",
    "  - Runnable Pass-Through\n",
    "  - Runnable Lambda\n",
    "  - Runnable Parallel\n",
    "  - Item Getter\n",
    "\n",
    "### Built-in Runnables\n",
    "\n",
    "#### Runnable Pass-Through\n",
    "- **Description:** Outputs the input unchanged.\n",
    "- **Purpose:** Useful for passing user input directly.\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain_core.runnables import RunnablePassThrough\n",
    "\n",
    "  chain = RunnablePassThrough()\n",
    "  output = chain.invoke(\"Puppy\")\n",
    "  print(output)  # Output: Puppy\n",
    "  ```\n",
    "\n",
    "#### Runnable Lambda   lambda means function\n",
    "- **Description:** Wraps custom functions for use in chains.\n",
    "- **Purpose:** Integrates custom logic into the chain.\n",
    "- **Example:**\n",
    "  ```python\n",
    "  def russian_last_name(name: str) -> str:\n",
    "      return name + \"ovs\"\n",
    "\n",
    "  chain = runnables.RunnableLambda(russian_last_name)\n",
    "  output = chain.invoke(\"Puppy\")\n",
    "  print(output)  # Output: Puppyovs\n",
    "  ```\n",
    "\n",
    "Difference between runnable passthrough, runnable parallel, runnable lambda\n",
    "\n",
    "#### Runnable Parallel\n",
    "- **Description:** Executes multiple runnables concurrently.\n",
    "- **Purpose:** Improves efficiency in complex chains.\n",
    "- **Example:**\n",
    "  ```python\n",
    "  runnable_parallel = \n",
    "      runnables.RunnablePassThrough() |\n",
    "      runnables.RunnableLambda(russian_last_name)\n",
    " \n",
    "\n",
    "  output = runnable_parallel.invoke(\"Puppy\")\n",
    "  print(output)  # Output: {'operation_0': 'Puppy', 'operation_1': 'Puppyovs'}\n",
    "  ```\n",
    "\n",
    "### Advanced Use of Runnable Parallel\n",
    "- **Example with Vector Database:**\n",
    "  - Demonstrates parallel execution with a vector database and retriever.\n",
    "  - Combines context retrieval and question handling efficiently.\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"operation_a\": RunnablePassthrough(),\n",
    "    \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "    \"operation_c\": RunnablePassthrough(),\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "\n",
    "chain.invoke({\n",
    "    \"name1\" : \"Jordon\", \n",
    "    \"name\":\"Abhram\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithhappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "retrieval_chain.invoke(\"What is dswithbappy?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Item Getter\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in English.\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is dswithhappy?\", \"language\": \"Pirate English\"})\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "- **Description:** Extracts specific items from parallel operations.\n",
    "- **Purpose:** Handles multiple inputs in parallel.\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain_core import runnables, operator\n",
    "\n",
    "  chain = runnables.RunnableParallel({\n",
    "      \"operator_a\" : RunnablePassThrough(), \n",
    "      \"operator_b\" : RunnableLambda(russian_last_name)\n",
    "  })\n",
    "\n",
    "  context = retriever.retrieve(data)\n",
    "  question = \"What is DS with Puppy?\"\n",
    "  language = \"pirate\"\n",
    "  prompt = prompt_template.format(context=context, question=question, language=language)\n",
    "  response = chat_model.generate_response(prompt)\n",
    "  output = output_parser.StrOutputParser().parse(response)\n",
    "  print(output)  # Output: DS with Puppy be a platform focusin' on content like data science, AI, ML, DL, CV, NLP, Python programmin'.\n",
    "  ```\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python programming, etc. in Eng\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is dswithbappy?\", \"language\": \"Pirate English\"})\n",
    "\n",
    "\n",
    "### Code Snippets\n",
    "- **Runnable Pass-Through Example:**\n",
    "  ```python\n",
    "  from langchain import runnables\n",
    "\n",
    "  chain = runnables.RunnablePassThrough()\n",
    "  output = chain.invoke(\"Puppy\")\n",
    "  print(output)  # Output: Puppy\n",
    "  ```\n",
    "\n",
    "- **Runnable Lambda Example:**\n",
    "  ```python\n",
    "  from langchain import runnables\n",
    "\n",
    "  def russian_last_name(name: str) -> str:\n",
    "      return name + \"ovs\"\n",
    "\n",
    "  chain = runnables.RunnableLambda(russian_last_name)\n",
    "  output = chain.invoke(\"Puppy\")\n",
    "  print(output)  # Output: Puppyovs\n",
    "  ```\n",
    "\n",
    "- **Runnable Parallel Example:**\n",
    "  ```python\n",
    "  from langchain import runnables\n",
    "\n",
    "  runnable_parallel = runnables.RunnableParallel([\n",
    "      runnables.RunnablePassThrough(),\n",
    "      runnables.RunnableLambda(russian_last_name)\n",
    "  ])\n",
    "\n",
    "  output = runnable_parallel.invoke(\"Puppy\")\n",
    "  print(output)  # Output: {'operation_0': 'Puppy', 'operation_1': 'Puppyovs'}\n",
    "  ```\n",
    "\n",
    "- **Advanced Use of Runnable Parallel with Vector Database:**\n",
    "  - Combines context retrieval and question handling in parallel.\n",
    "\n",
    "- **Item Getter Example:**\n",
    "  - Handles multiple inputs in parallel using `itemgetter`.\n",
    "\n",
    "**Merged Summary of LangChain Session**\n",
    "\n",
    "---\n",
    "Use of .bind() to add arguments to a Runnable in a LCEL Chain • For example, we can add an argument to stop the model response when it reaches the word \"Ronaldo\".\n",
    "\n",
    "### Built-in Functions in Runnables\n",
    "\n",
    "- **`.bind` Function:**\n",
    "  - Example: Using `.bind` to stop the model's response at a specific point.\n",
    "  ```python\n",
    "  chain = (\n",
    "      prompt_model | \n",
    "      model.bind(stop=[\"Ronaldo\"]) \n",
    "      | output_parser\n",
    "  )\n",
    "  response = chain.invoke({\"soccer_player\": \"Ronaldo\"})\n",
    "  print(response)  # Output: Stopping when \"Ronaldo\" is encountered.\n",
    "  ```\n",
    "\n",
    "### Combining Chains\n",
    "\n",
    "- **Simple Chain Combination:**\n",
    "  - Example: Creating a chain to assess a politician's impact.\n",
    "  ```python\n",
    "  from operator import itemgetter\n",
    "\n",
    "\n",
    "chain1 = (\n",
    "    ChatPromptTemplate.from_template(\"What is the country {politician} is from?\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    {\"country\" : chain1, \"language\":itemgetter(\"language\")}\n",
    "    | ChatPromptTemplate.from_template(\"What is the continent of {country}? respond in {language}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "response = chain2.invoke({\"politician\": \"Emmanuel Macron\", \"language\":\"hindi\"})\n",
    "print(response)  # Output: L'Europe\n",
    "  ```\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the country {politician} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what continent is the country {country} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"politician\": \"Mitterrand\", \"language\": \"French\"})\n",
    "\n",
    "\n",
    "See webbaseloader and langchain hub for prompts\n",
    "### RAG Application Demo\n",
    "\n",
    "- **Process:**\n",
    "  - Loading data from URLs using `WebBaseLoader`.\n",
    "  - Chunking and vector storage with ChromaDB.\n",
    "  - Creating the RAG chain with ParallelRunnable for efficiency.\n",
    "  ```python\n",
    "  rag_chain =  {\n",
    "      \"retriever\": retriever,\n",
    "      \"question\": question,\n",
    "      \"format_documents\": lambda x: x,\n",
    "  } | prompt | LLMChatOpenAI(...) | str_output_parser\n",
    "  response = rag_chain.invoke(\"What is task decomposition?\")\n",
    "  ```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context as a reference to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Context: Solidity​\n",
    "Here's an example using the Solidity text splitter:\n",
    "SOL_CODE = \"\"\"pragma solidity ^0.8.20;contract HelloWorld {   function add(uint a, uint b) pure public returns(uint) {       return a + b;   }}\"\"\"sol_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.SOL, chunk_size=128, chunk_overlap=0)sol_docs = sol_splitter.create_documents([SOL_CODE])sol_docs\n",
    "[Document(page_content='pragma solidity ^0.8.20;'), Document(page_content='contract HelloWorld {\\n   function add(uint a, uint b) pure public returns(uint) {\\n       return a + b;\\n   }\\n}')]\n",
    "C#​\n",
    "Here's an example using the C# text splitter:\n",
    "\n",
    "for over a dozen programming languages.\"headers_to_split_on = [    (\"#\", \"Header 1\"),    (\"##\", \"Header 2\"),]# MD splitsmarkdown_splitter = MarkdownHeaderTextSplitter(    headers_to_split_on=headers_to_split_on, strip_headers=False)md_header_splits = markdown_splitter.split_text(markdown_document)# Char-level splitsfrom langchain_text_splitters import RecursiveCharacterTextSplitterchunk_size = 250chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(md_header_splits)splitsAPI Reference:RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter.split_text(state_of_the_union)[:2]\n",
    "['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and', 'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.']\n",
    "Let's go through the parameters set above for RecursiveCharacterTextSplitter:\n",
    "\n",
    "C#​\n",
    "Here's an example using the C# text splitter:\n",
    "C_CODE = \"\"\"using System;class Program{    static void Main()    {        int age = 30; // Change the age value as needed        // Categorize the age without any console output        if (age < 18)        {            // Age is under 18        }        else if (age >= 18 && age < 65)        {            // Age is an adult        }        else        {            // Age is a senior citizen        }    }}\"\"\"c_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.CSHARP, chunk_size=128, chunk_overlap=0)c_docs = c_splitter.create_documents([C_CODE])c_docs\n",
    "\n",
    "How-to guidesHow to recursively split text by charactersOn this pageHow to recursively split text by characters\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: by list of characters.\n",
    "How the chunk size is measured: by number of characters.\n",
    "\n",
    "recommended text splitter for generic text use cases.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,  # chunk size (characters)    chunk_overlap=200,  # chunk overlap (characters)    add_start_index=True,  # track index in original document)all_splits = text_splitter.split_documents(docs)print(f\"Split blog post into {len(all_splits)} sub-documents.\")API Reference:RecursiveCharacterTextSplitter\n",
    "Split blog post into 66 sub-documents.\n",
    "Go deeper​\n",
    "TextSplitter: Object that splits a list of Documents into smaller\n",
    "chunks. Subclass of DocumentTransformers.\n",
    "\n",
    "How-to guidesHow to split codeOn this pageHow to split code\n",
    "RecursiveCharacterTextSplitter includes pre-built lists of separators that are useful for splitting text in a specific programming language.\n",
    "Supported languages are stored in the langchain_text_splitters.Language enum. They include:\n",
    "\"cpp\",\"go\",\"java\",\"kotlin\",\"js\",\"ts\",\"php\",\"proto\",\"python\",\"rst\",\"ruby\",\"rust\",\"scala\",\"swift\",\"markdown\",\"latex\",\"html\",\"sol\",\"csharp\",\"cobol\",\"c\",\"lua\",\"perl\",\"haskell\"\n",
    "To view the list of separators for a given language, pass a value from this enum into\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language`\n",
    "To instantiate a splitter that is tailored for a specific language, pass a value from the enum into\n",
    "RecursiveCharacterTextSplitter.from_language\n",
    "Below we demonstrate examples for the various languages.\n",
    "%pip install -qU langchain-text-splitters\n",
    "from langchain_text_splitters import (    Language,    RecursiveCharacterTextSplitter,)API Reference:Language | RecursiveCharacterTextSplitter\n",
    "\n",
    "This can be done using the .split_documents method of the second splitter:\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitterchunk_size = 500chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splits[80:85]API Reference:RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    model_name=\"gpt-4\",    chunk_size=100,    chunk_overlap=0,)API Reference:RecursiveCharacterTextSplitter\n",
    "We can also load a TokenTextSplitter splitter, which works with tiktoken directly and will ensure each split is smaller than chunk size.\n",
    "from langchain_text_splitters import TokenTextSplittertext_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)texts = text_splitter.split_text(state_of_the_union)print(texts[0])API Reference:TokenTextSplitter\n",
    "Madam Speaker, Madam Vice President, our\n",
    "\n",
    "= html_splitter.split_text(html_string)chunk_size = 500chunk_overlap = 30text_splitter = RecursiveCharacterTextSplitter(    chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Splitsplits = text_splitter.split_documents(html_header_splits)splitsAPI Reference:RecursiveCharacterTextSplitter\n",
    "\n",
    "latex_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0)latex_docs = latex_splitter.create_documents([latex_text])latex_docs\n",
    "\n",
    "The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n",
    "If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n",
    "This process continues down to the word level if necessary.\n",
    "\n",
    "Here is example usage:\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:RecursiveCharacterTextSplitter\n",
    "Further reading\n",
    "See the how-to guide for recursive text splitting.\n",
    "\n",
    "Document-structured based​\n",
    "Some documents have an inherent structure, such as HTML, Markdown, or JSON files.\n",
    "In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.\n",
    "Key benefits of structure-based splitting:\n",
    "\n",
    "Below we show example usage.\n",
    "To obtain the string content directly, use .split_text.\n",
    "To create LangChain Document objects (e.g., for use in downstream tasks), use .create_documents.\n",
    "%pip install -qU langchain-text-splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter# Load example documentwith open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()text_splitter = RecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,    chunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts = text_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])API Reference:RecursiveCharacterTextSplitter\n",
    "page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and'page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'\n",
    "text_splitter.split_text(state_of_the_union)[:2]\n",
    "\n",
    "Example implementation using LangChain's CharacterTextSplitter with token-based splitting:\n",
    "from langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:CharacterTextSplitter\n",
    "Further reading\n",
    "See the how-to guide for token-based splitting.\n",
    "See the how-to guide for character-based splitting.\n",
    "\n",
    "Text-structured based​\n",
    "Text is naturally organized into hierarchical units such as paragraphs, sentences, and words.\n",
    "We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\n",
    "LangChain's RecursiveCharacterTextSplitter implements this concept:\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitterhtml_string = \"\"\"    <!DOCTYPE html>    <html>    <body>        <div>            <h1>Foo</h1>            <p>Some intro text about Foo.</p>            <div>                <h2>Bar main section</h2>                <p>Some intro text about Bar.</p>                <h3>Bar subsection 1</h3>                <p>Some text about the first subtopic of Bar.</p>                <h3>Bar subsection 2</h3>                <p>Some text about the second subtopic of Bar.</p>            </div>            <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>            <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on = [    (\"h1\", \"Header 1\"),    (\"h2\", \"Header 2\"),    (\"h3\", \"Header 3\"),    (\"h4\", \"Header 4\"),]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)chunk_size =\n",
    "\n",
    "JS​\n",
    "Here's an example using the JS text splitter:\n",
    "JS_CODE = \"\"\"function helloWorld() {  console.log(\"Hello, World!\");}// Call the functionhelloWorld();\"\"\"js_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.JS, chunk_size=60, chunk_overlap=0)js_docs = js_splitter.create_documents([JS_CODE])js_docs\n",
    "[Document(page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'), Document(page_content='// Call the function\\nhelloWorld();')]\n",
    "TS​\n",
    "Here's an example using the TS text splitter:\n",
    "TS_CODE = \"\"\"function helloWorld(): void {  console.log(\"Hello, World!\");}// Call the functionhelloWorld();\"\"\"ts_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.TS, chunk_size=60, chunk_overlap=0)ts_docs = ts_splitter.create_documents([TS_CODE])ts_docs\n",
    "[Document(page_content='function helloWorld(): void {'), Document(page_content='console.log(\"Hello, World!\");\\n}'), Document(page_content='// Call the function\\nhelloWorld();')]\n",
    "Markdown​\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])\n",
    "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution.\n",
    "To implement a hard constraint on the chunk size, we can use RecursiveCharacterTextSplitter.from_tiktoken_encoder, where each split will be recursively split if it has a larger size:\n",
    "\n",
    "mitigate the possibility of separating a statement from important\n",
    "context related to it. We use the\n",
    "RecursiveCharacterTextSplitter,\n",
    "which will recursively split the document using common separators like\n",
    "new lines until each chunk is the appropriate size. This is the\n",
    "recommended text splitter for generic text use cases.\n",
    "We set add_start_index=True so that the character index where each\n",
    "split Document starts within the initial Document is preserved as\n",
    "metadata attribute “start_index”.\n",
    "See this guide for more detail about working with PDFs, including how to extract text from specific sections and images.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)API Reference:RecursiveCharacterTextSplitter\n",
    "514\n",
    "Embeddings​\n",
    "\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplittersplitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)text = \"Lorem \"count_start_and_stop_tokens = 2text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokensprint(text_token_count)API Reference:SentenceTransformersTokenTextSplitter\n",
    "2\n",
    "token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1# `text_to_split` does not fit in a single chunktext_to_split = text * token_multiplierprint(f\"tokens in text to split: {splitter.count_tokens(text=text_to_split)}\")\n",
    "tokens in text to split: 514\n",
    "text_chunks = splitter.split_text(text=text_to_split)print(text_chunks[1])\n",
    "lorem\n",
    "NLTK​\n",
    "noteThe Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.\n",
    "\n",
    "How-to guidesHow to split by characterHow to split by character\n",
    "This is the simplest method. This splits based on a given character sequence, which defaults to \"\\n\\n\". Chunk length is measured by number of characters.\n",
    "\n",
    "How the text is split: by single character separator.\n",
    "How the chunk size is measured: by number of characters.\n",
    "\n",
    "[Document(page_content='main :: IO ()'), Document(page_content='main = do\\n    putStrLn \"Hello, World!\"\\n-- Some'), Document(page_content='sample functions\\nadd :: Int -> Int -> Int\\nadd x y'), Document(page_content='= x + y')]\n",
    "PHP​\n",
    "Here's an example using the PHP text splitter:\n",
    "PHP_CODE = \"\"\"<?phpnamespace foo;class Hello {    public function __construct() { }}function hello() {    echo \"Hello World!\";}interface Human {    public function breath();}trait Foo { }enum Color{    case Red;    case Blue;}\"\"\"php_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.PHP, chunk_size=50, chunk_overlap=0)php_docs = php_splitter.create_documents([PHP_CODE])php_docs\n",
    "\n",
    "Rather than just splitting on \"\\n\\n\", we can use NLTK to split based on NLTK tokenizers.\n",
    "\n",
    "CharacterTextSplitter, RecursiveCharacterTextSplitter, and TokenTextSplitter can be used with tiktoken directly.\n",
    "%pip install --upgrade --quiet langchain-text-splitters tiktoken\n",
    "from langchain_text_splitters import CharacterTextSplitter# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()API Reference:CharacterTextSplitter\n",
    "To split with a CharacterTextSplitter and then merge chunks with tiktoken, use its .from_tiktoken_encoder() method. Note that splits from this method can be larger than the chunk size measured by the tiktoken tokenizer.\n",
    "The .from_tiktoken_encoder() method takes either encoding_name as an argument (e.g. cl100k_base), or the model_name (e.g. gpt-4). All additional arguments like chunk_size, chunk_overlap, and separators are used to instantiate CharacterTextSplitter:\n",
    "\n",
    "Markdown​\n",
    "Here's an example using the Markdown text splitter:\n",
    "markdown_text = \"\"\"# 🦜️🔗 LangChain⚡ Building applications with LLMs through composability ⚡## What is LangChain?# Hopefully this code block isn't splitLangChain is a framework for...As an open-source project in a rapidly developing field, we are extremely open to contributions.\"\"\"\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0)md_docs = md_splitter.create_documents([markdown_text])md_docs\n",
    "\n",
    "# pip install nltk\n",
    "# This is a long document we can split up.with open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()\n",
    "from langchain_text_splitters import NLTKTextSplittertext_splitter = NLTKTextSplitter(chunk_size=1000)API Reference:NLTKTextSplitter\n",
    "texts = text_splitter.split_text(state_of_the_union)print(texts[0])\n",
    "\n",
    "HTML​\n",
    "Here's an example using an HTML text splitter:\n",
    "html_text = \"\"\"<!DOCTYPE html><html>    <head>        <title>🦜️🔗 LangChain</title>        <style>            body {                font-family: Arial, sans-serif;            }            h1 {                color: darkblue;            }        </style>    </head>    <body>        <div>            <h1>🦜️🔗 LangChain</h1>            <p>⚡ Building applications with LLMs through composability ⚡</p>        </div>        <div>            As an open-source project in a rapidly developing field, we are extremely open to contributions.        </div>    </body></html>\"\"\"\n",
    "html_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.HTML, chunk_size=60, chunk_overlap=0)html_docs = html_splitter.create_documents([html_text])html_docs\n",
    "\n",
    "# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\n",
    "['lion']['tiger']['wolf']['gorilla']['raccoon']\n",
    "Invoking it gives a full array of values:\n",
    "list_chain.invoke({\"animal\": \"bear\"})\n",
    "\n",
    "Use .split_text to obtain the string content directly:\n",
    "text_splitter.split_text(state_of_the_union)[0]\n",
    "\n",
    "1) How to split HTML strings:​\n",
    "from langchain_text_splitters import HTMLSectionSplitterhtml_string = \"\"\"    <!DOCTYPE html>    <html>    <body>        <div>            <h1>Foo</h1>            <p>Some intro text about Foo.</p>            <div>                <h2>Bar main section</h2>                <p>Some intro text about Bar.</p>                <h3>Bar subsection 1</h3>                <p>Some text about the first subtopic of Bar.</p>                <h3>Bar subsection 2</h3>                <p>Some text about the second subtopic of Bar.</p>            </div>            <div>                <h2>Baz</h2>                <p>Some text about Baz</p>            </div>            <br>            <p>Some concluding text about Foo</p>        </div>    </body>    </html>\"\"\"headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]html_splitter = HTMLSectionSplitter(headers_to_split_on)html_header_splits = html_splitter.split_text(html_string)html_header_splitsAPI Reference:HTMLSectionSplitter\n",
    "\n",
    "To obtain the string content directly, use .split_text.\n",
    "To create LangChain Document objects (e.g., for use in downstream tasks), use .create_documents.\n",
    "%pip install -qU langchain-text-splitters\n",
    "from langchain_text_splitters import CharacterTextSplitter# Load an example documentwith open(\"state_of_the_union.txt\") as f:    state_of_the_union = f.read()text_splitter = CharacterTextSplitter(    separator=\"\\n\\n\",    chunk_size=1000,    chunk_overlap=200,    length_function=len,    is_separator_regex=False,)texts = text_splitter.create_documents([state_of_the_union])print(texts[0])API Reference:CharacterTextSplitter:\n",
    "\n",
    "Question: write code for recursive text splitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/how_to/message_history/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Ecosystem and Q&A Session Summary\n",
    "\n",
    "### Introduction\n",
    "\n",
    "- **Session Overview**: The session focuses on the LangChain ecosystem, including Lang Smith, Lang Serve, and Lang Graph. The instructor emphasizes the importance of understanding the ecosystem and encourages participants to ask questions.\n",
    "\n",
    "### LangChain Ecosystem\n",
    "\n",
    "- **LangChain Overview**: LangChain is a comprehensive tool for managing and optimizing large language models (LLMs). It has evolved significantly, introducing new tools like Lang Smith, Lang Serve, and Lang Graph.\n",
    "\n",
    "- **Lang Smith**:\n",
    "  - **Purpose**: An LLM ops tool for monitoring and evaluating LLM applications.\n",
    "  - **Functionality**: Helps monitor the performance of LLM applications, including relevance, accuracy, and hallucination detection.\n",
    "\n",
    "- **Lang Serve**:\n",
    "  - **Purpose**: A tool for deploying LLM applications.\n",
    "  - **Functionality**: Uses FastAPI to create APIs for LLM applications, making deployment easier.\n",
    "\n",
    "- **Lang Graph**:\n",
    "  - **Purpose**: A tool for creating agents and multi-agent systems.\n",
    "  - **Functionality**: Connects different third-party tools to fetch real-time information, enhancing the capabilities of LLMs.\n",
    "\n",
    "### Tools and Ecosystem\n",
    "\n",
    "- **Integration Tools**:\n",
    "  - Lang Smith, Lang Serve, and Lang Graph are part of the LangChain ecosystem, aimed at making LLM applications more robust and versatile.\n",
    "  - **Crew AI**: A high-level framework built on top of LangChain, similar to how Keras is built on TensorFlow.\n",
    "\n",
    "- **Documentation and Learning**:\n",
    "  - **Importance of Documentation**: The instructor emphasizes the importance of referring to the latest documentation for updates and new features.\n",
    "  - **Stable Versions**: Uses stable versions of packages to avoid installation issues.\n",
    "\n",
    "\n",
    "## LANGSERVE\n",
    "```python\n",
    "\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    ")\n",
    "chain = prompt_template | llm | parser\n",
    "app = FastAPI(\n",
    "    title=\"simpleTranslator\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "go to route  /chain/playground\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "chain = RemoteRunnable(\"http://localhost:8000/chain/c/N4XyA\")\n",
    "print(chain.invoke({\"language\": \"Spanish\", \"text\": \"Generative AI is a bigger opport\"}))\n",
    "\n",
    "```\n",
    "\n",
    "check langserve docs\n",
    "\n",
    "## LangChain and LangGraph: Building AI Agents for Real-Time Search\n",
    "\n",
    "This document details a walkthrough of building AI agents using LangChain and LangGraph, focusing on real-time search capabilities. The tutorial uses Tabily, a search API, and demonstrates agent creation, memory management, and stream processing. A course announcement for advanced LLM application development is also included.\n",
    "\n",
    "### I. Setup and API Keys\n",
    "\n",
    "1. **Environment:** The tutorial begins by creating a Jupyter Notebook in VS Code named \"line_graph_demo.ipynb\".\n",
    "\n",
    "2. **OpenAI API Key:** The first step involves loading the OpenAI API key. This is crucial for interacting with the language model (LLM). The key is loaded from an `.env` file.\n",
    "\n",
    "   ```bash\n",
    "   # .env file content (Example)\n",
    "   OPENAI_API_KEY=your_actual_api_key\n",
    "   TABILY_API_KEY=your_actual_tabily_api_key\n",
    "   ```\n",
    "\n",
    "3. **LLM Loading:** The LLM (GPT-3.5-turbo) is loaded.\n",
    "\n",
    "   ```python\n",
    "   from langchain import ChatCompletion\n",
    "   llm = ChatCompletion(model=\"gpt-3.5-turbo\")\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Tabily Search:** The tutorial uses LangChain's `TabularSearch` tool to interact with Tabily. The following code snippet demonstrates searching for information and retrieving results.\n",
    "\n",
    "   ```python\n",
    "   from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search.invoke(\"Who are the top stars of the 2024 Eurocup?\")\n",
    "   ```\n",
    "\n",
    "```python\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "agent_executor = create_react_agent(llm, tools)\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"Where is the soccer Eurocup 2024\")]})\n",
    "response[\"messages\"]\n",
    "\n",
    "```\n",
    "   This code snippet runs the query through the `agents_executor` and prints the response, showing how the agent makes use of the Tabily API when necessary. The difference in response time and the presence/absence of URLs indicate the source used (real-time search vs. knowledge base).\n",
    "\n",
    "3. **Stream Processing:** The tutorial mentions the ability to use `executor.stream` for stream processing of the agent's responses, although the code for this is not included.\n",
    "```python\n",
    "\n",
    "\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"When and where will it be the 2024 Eurocup final match?\")]}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "\n",
    "\n",
    "```\n",
    "### IV. Memory Management with LangGraph\n",
    "\n",
    "1. **Memory Implementation:** The use of LangGraph's `MemorySaver` allows for conversational context. A `MemorySaver` object is added to the `create_react_agents` function.\n",
    "\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = {\"type\": \"user\", \"content\": \"hi! I'm bob\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "input_message = {\"type\": \"user\", \"content\": \"what's my name?\"}\n",
    "for chunk in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "```\n",
    "\n",
    "   ```python\n",
    "   from langgraph.checkpoint.memory import MemorySaver\n",
    "   memory = MemorySaver()\n",
    "   agents_executor_with_memory = create_react_agents(llm=llm, tools=[search_tool], checkpointer=memory, config={\"config\": {\"trainable_id\": \"001\"}})\n",
    "   ```\n",
    "\n",
    "   This shows how to incorporate memory into the agent, maintaining conversational context using a specific `trainable_id` for different users. The `trainable_id` helps manage separate conversation histories for each user.\n",
    "\n",
    "2. **Memory Demonstration:** Examples are given to demonstrate how the agent retains conversation history when using the `MemorySaver`. Switching `trainable_id` shows that separate conversations are maintained for different users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith: An LLM Ops Platform - Session Summary\n",
    "\n",
    "create an account in langchian, create a project, open langsmith docs. get api key for\n",
    "langsmith \n",
    "This document summarizes a session introducing LangSmith, an LLM Ops platform within the LangChain ecosystem.\n",
    "\n",
    "\n",
    "check langsmith and langgraph once.\n",
    "see memory in langchain\n",
    "\n",
    "### Session Overview\n",
    "\n",
    "The session (Day 14 of a series) demonstrated LangSmith's capabilities for monitoring, debugging, tracing, and evaluating LLM-powered applications. The instructor emphasized practical application over theoretical explanations. The session began with an overview of LangSmith's place within the LangChain ecosystem, then proceeded to a hands-on demo. The course announcement for a LangChain live batch starting December 17th, 2024, was also included.\n",
    "\n",
    "### What is LangSmith?\n",
    "\n",
    "LangSmith is an LLM Ops platform developed by LangChain. It allows for:\n",
    "\n",
    "- **Monitoring:** Tracking application performance, costs, and errors.\n",
    "- **Debugging:** Identifying and resolving issues within LLM applications.\n",
    "- **Tracing:** Observing the execution flow of applications.\n",
    "- **Evaluation:** Assessing the performance and quality of LLM applications (briefly mentioned, further exploration promised for the future).\n",
    "\n",
    "LangSmith works independently of LangChain, though integration is straightforward. It requires setting environment variables for API key and project name.\n",
    "\n",
    "### Setting up LangSmith\n",
    "\n",
    "1. **Create a LangChain Account:** If you don't have one, sign up on the LangChain website.\n",
    "2. **Generate an API Key:** Navigate to the settings in your LangChain dashboard and generate a personal access token.\n",
    "3. **Set Environment Variables:** Add the API key and project name to your `.env` file. The example shown used the following variables:\n",
    "\n",
    "   ```bash\n",
    "   LANGCHAIN_API_KEY=<your_api_key>\n",
    "   LANGCHAIN_PROJECT=test_app \n",
    "   ```\n",
    "\n",
    "**Tip:** Ensure you replace `<your_api_key>` with your actual API key and choose a descriptive project name. The instructor strongly cautioned against using the provided key.\n",
    "\n",
    "### Practical Demo: Monitoring an LLM Application\n",
    "\n",
    "The demo used a Jupyter Notebook with the following code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys and configure LangChain tracing\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Optional: You can use other models like Grok here\n",
    "model = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# LangChain tracing setup (Crucial for LangSmith integration)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Create a ChatPromptTemplate\n",
    "prompt_template = \"\"\"You are a helpful assistant. Please response to the user request only based on the given context.\n",
    "{context}\n",
    "User: {question}\"\"\"\n",
    "chat_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Create a simple chain\n",
    "chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "# Example question and context\n",
    "question = \"Can you summarize this story?\"\n",
    "context = \"\"\"[Insert story here. Example from Google search]\"\"\"\n",
    "\n",
    "# Invoke the chain\n",
    "summary = chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n",
    "```\n",
    "\n",
    "This code snippet shows a simple LangChain application using ChatOpenAI. The crucial part for LangSmith integration is `os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"`. This enables LangChain's v2 tracing functionality, which sends data to LangSmith.\n",
    "\n",
    "The instructor highlighted the importance of using LangChain's v2 (second generation) for compatibility with LangSmith. Legacy LangChain versions will not work.\n",
    "\n",
    "After running this code, the instructor demonstrated viewing the execution details within the LangSmith platform.\n",
    "\n",
    "### LangSmith Dashboard Features\n",
    "\n",
    "The LangSmith dashboard displays various metrics and logs:\n",
    "\n",
    "- **Project Overview:** Shows high-level statistics like total cost, total tokens used, and error rates.\n",
    "- **Run Details:** Provides detailed information for each execution, including inputs, outputs, logs, and execution time.\n",
    "- **Monitor Section:** Presents graphical visualizations of application performance over time (hours and days).\n",
    "- **Runs Section:** Displays all completed runs, allowing for individual inspection of the runnables (prompt, model, parser, etc.).\n",
    "\n",
    "The ability to view costs and resource utilization was emphasized. The dashboard also displays the execution as a sequence of runnables, allowing detailed analysis of each step. The instructor mentioned the ability to view data in JSON and YAML formats.\n",
    "\n",
    "### Comparison to MLflow\n",
    "\n",
    "The instructor drew parallels between LangSmith and MLflow, both enabling experiment tracking and monitoring.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The session provided a practical introduction to LangSmith, demonstrating its ease of use and the valuable insights it provides for developing and deploying production-ready LLM applications. The instructor encouraged viewers to explore the LangSmith documentation further and announced an upcoming course focused on practical LLM application development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-google-vertexai langchain-groq langchain-mistralai google-generativeai langchain-google-genai langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# setting api keys\n",
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\"\"\"\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\"\"\"\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\n",
    "\n",
    "model.invoke(\"Hello\")\n",
    "\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n",
    "model.invoke([HumanMessage(\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"animal_type\", \"pet_color\"],\n",
    "    template=\"I have a {animal_type} pet, and it is {pet_color} in color. Suggest me five cool names for it.\"\n",
    ")\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke({\n",
    "    \"animal_type\" : \"dog\",\n",
    "    \"pet_color\" : \"pink\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format(**{\n",
    "    \"animal_type\" : \"dog\",\n",
    "    \"pet_color\" : \"pink\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format(animal_type = \"dog\", pet_color = \"pink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.format(text=\"Hello, how are you?\", language=\"Spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"topic\":\"cat\"})\n",
    "chain.run({\"topic\":\"sheep\"})  ## deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic rag working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")    # sentence-transformers/all-mpnet-base-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://python.langchain.com/docs/how_to/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.find_all('article')[0]\n",
    "text = article.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\n",
    "# embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")    # sentence-transformers/all-mpnet-base-v2\n",
    "vector_store = Chroma(collection_name=\"langchain\",  embedding_function=embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# Set of visited links to prevent loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_links = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def scrape_and_store(url, base_url):\n",
    "    \"\"\"\n",
    "    Recursively scrape and store content in FAISS from a given URL.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if url in visited_links or not \"/docs/how_to\" in url or not base_url in url or \"v0.2\" in url or \"v0.1\" in url or \"#\" in url: \n",
    "            return\n",
    "        \n",
    "        print(url)\n",
    "        # Mark this URL as visited\n",
    "        visited_links.add(url)\n",
    "        \n",
    "        # Fetch the page content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the `article` tag and extract text\n",
    "        article = soup.find_all('article')[0]\n",
    "        text = article.get_text()\n",
    "        \n",
    "        # Use LangChain's RecursiveCharacterTextSplitter to split the text\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        docs = splitter.create_documents([text])\n",
    "        \n",
    "        # Add chunks to FAISS vector store\n",
    "        vector_store.add_documents(docs)\n",
    "        \n",
    "        # Find all links within the article tag\n",
    "        links = article.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            # Make sure the link is absolute\n",
    "            if href.startswith('/'):\n",
    "                href = base_url + href\n",
    "            elif not href.startswith('http'):\n",
    "                continue  # Skip malformed or relative links\n",
    "            \n",
    "            # Recursively scrape linked pages\n",
    "            scrape_and_store(href, base_url)\n",
    "            \n",
    "        time.sleep(0.5)  # Add a delay to avoid overloading the server\n",
    "    except Exception as e:\n",
    "        # print(\"Troubled url : \", url)\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def scrape_concurrently(start_url, base_url):\n",
    "    \"\"\"\n",
    "    Use a thread pool to scrape URLs concurrently.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        urls_to_process = [start_url]\n",
    "        while urls_to_process:\n",
    "            # Submit scraping tasks for all URLs\n",
    "            futures = [executor.submit(scrape_and_store, url, base_url) for url in urls_to_process]\n",
    "            urls_to_process = []\n",
    "\n",
    "            # Collect results and add new URLs to the queue\n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    urls_to_process.extend(result)\n",
    "\n",
    "\n",
    "\n",
    "# Start scraping\n",
    "\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = 'https://python.langchain.com'\n",
    "\n",
    "# Starting URL\n",
    "start_url = 'https://python.langchain.com/docs/tutorials/'\n",
    "\n",
    "scrape_concurrently(start_url, base_url)\n",
    "# Start scraping and storing\n",
    "# scrape_and_store(start_url, base_url)\n",
    "\n",
    "# Persist FAISS vector store to disk\n",
    "# vector_store.save_local(\"langchain\")\n",
    "\n",
    "print(\"Scraping and storage complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# setting api keys\n",
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\"\"\"\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\"\"\"\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "question = \"from_messages create prompt from message? explain it in detail with examples explain to me like a 13 year old child\" \n",
    "\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context as a reference to come up with an answer. \n",
    "Context: {context}:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "it may happen that all information is not available in the context. In that case, try to come up with best guesses.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 100})\n",
    "# docs = retriever.invoke(question)\n",
    "# docs_text = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "def func(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "docs_text = RunnablePassthrough() | {\"context\" : retriever | func, \"question\" : RunnablePassthrough()} | prompt | llm | StrOutputParser() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs_text.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Who won the world series in 2020?\"),\n",
    "    (\"assistant\", \"The Los Angeles Dodgers won the World Series in 2020.\"),\n",
    "])\n",
    "\n",
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "chain2 = RunnableLambda(russian_lastname)\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "prompt.invoke({\"input\":\"mridul\", \"age\":\"12\"}).messages[1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "chain1 = (\n",
    "    ChatPromptTemplate.from_template(\"What is the country {politician} is from?\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    {\"country\" : chain1, \"language\":itemgetter(\"language\")}\n",
    "    | ChatPromptTemplate.from_template(\"What is the continent of {country}? respond in {language}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "response = chain2.invoke({\"politician\": \"Emmanuel Macron\", \"language\":\"hindi\"})\n",
    "print(response)  # Output: L'Europe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    (\"system\", \"You are a helpful assistant. Please response to the user request only based on the given context\"),\n",
    "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
    "]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "question = \"Can you summarize this text?\"\n",
    "\n",
    "context = \"A dragon is a mythical beast that is said to live in the mountains of a fictional country. It is said to be the king of the mountain and is feared by the people of the country. The dragon is often depicted as a fierce and powerful creature, with scales and wings. It is also said to have the ability to breathe fire and other powerful magics. In some stories, dragons are depicted as having the body of a lion, while in others, they are depicted as having the body of a serpent. Overall, dragons are a popular and fascinating subject in mythology and folklore. They are often seen as symbols of power, strength, and wisdom. They are also often associated with good fortune and prosperity. In some stories, dragons are said to bring good luck and blessings to those who cross their paths. They are also said to be protectors of the forest and the animals that live in it. Overall, dragons are a powerful and mysterious creature that continues to captivate people's imaginations and imaginations.\"\n",
    "res = chain.invoke({\"question\":question, \"context\":context})\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LANGCHAIN\n",
    "\n",
    "Langchain Framework Overview\n",
    "Langchain is a framework designed to simplify the creation of applications using large language models (LLMs). It allows developers to connect AI models with various data sources to create customized NLP applications. The framework is open-source and currently offered in Python and JavaScript (TypeScript). It supports LLMs like GPT-4 from OpenAI or HuggingFace.\n",
    "Key Concepts\n",
    "Components:\n",
    "LLM Wrappers: Connect to LLMs like GPT-4 or HuggingFace.\n",
    "Prompt Templates: Avoid hardcoding text inputs.\n",
    "Indexes: Extract relevant information for LLMs.\n",
    "Chains: Combine multiple components to solve specific tasks.\n",
    "Agents: Allow LLMs to interact with external environments and APIs.\n",
    "Prerequisites\n",
    "Python: Version 3.8 or higher.\n",
    "Pip: Python package manager.\n",
    "Code Editor: Visual Studio Code (or any other preferred editor).\n",
    "OpenAI Account: For using OpenAI's LLM.\n",
    "Setting Up the Environment\n",
    "Create an OpenAI Account:\n",
    "Go to OpenAI and sign up.\n",
    "Generate an API key from the user account settings.\n",
    "Create a Project Directory:\n",
    "Open a terminal and navigate to your desired directory.\n",
    "Create a new directory: mkdir langchain-llm-app.\n",
    "Set Up a Virtual Environment:\n",
    "Create a virtual environment: python -m venv .venv.\n",
    "Activate the virtual environment: env\\Scripts\\activate.ps1 (Windows) or source .venv/bin/activate (macOS/Linux).\n",
    "Install Required Packages:\n",
    "Use pip to install necessary packages: pip install langchain openai streamlit python-dotenv.\n",
    "\n",
    "Using Agents\n",
    "Import Agent Components:\n",
    "Import the necessary components:\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Wikipedia, LLMMath\n",
    "\n",
    "\n",
    "Define the Agent:\n",
    "Create an agent function:\n",
    "def langchain_agent():\n",
    "    llm = LLM(model=\"text-davinci-003\", temperature=0.5)\n",
    "    tools = [Wikipedia(), LLMMath()]\n",
    "    agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "    result = agent.run(\"What is the average age of a dog? Multiply the age by 3.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "Run the Agent Function:\n",
    "Add the following code to run the agent function:\n",
    "if __name__ == \"__main__\":\n",
    "    print(langchain_agent())\n",
    "\n",
    "\n",
    "Building a YouTube Assistant\n",
    "Create a youtube_assistant Directory:\n",
    "Create a new directory for the YouTube assistant.\n",
    "Set Up the Environment:\n",
    "Create a virtual environment and install necessary packages:\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install langchain openai youtube_transcript faiss-cpu python-dotenv\n",
    "\n",
    "\n",
    "Create the langchain_helper.py File:\n",
    "Import necessary libraries:\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import LLM, PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "Load Environment Variables:\n",
    "Load the environment variables:\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "Define the Function to Create Vector DB:\n",
    "Create a function to load and split the YouTube transcript:\n",
    "def create_vector_db_from_youtube(video_url: str):\n",
    "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    transcript = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(transcript)\n",
    "    db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "    return db\n",
    "\n",
    "\n",
    "Define the Function to Get Response:\n",
    "Create a function to get the response from the query:\n",
    "def get_response_from_query(db, query: str, k: int = 4):\n",
    "    docs = db.similarity_search(query, k=k)\n",
    "    docs_page_content = \" \".join([doc.page_content for doc in docs])\n",
    "    llm = LLM(model=\"text-davinci-003\")\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"docs\"],\n",
    "        template=\"You are a helpful YouTube assistant that can answer questions about videos based on the video's transcript.\\n\\nQuestion: {question}\\n\\nVideo Transcript: {docs}\\n\\nAnswer:\"\n",
    "    )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = llm_chain.run(question=query, docs=docs_page_content)\n",
    "    return response\n",
    "\n",
    "\n",
    "Create the Streamlit Interface:\n",
    "Update main.py to create the interface:\n",
    "import streamlit as st\n",
    "import langchain_helper as lch\n",
    "import textwrap\n",
    "\n",
    "st.title(\"YouTube Assistant\")\n",
    "\n",
    "with st.form(key=\"my_form\"):\n",
    "    video_url = st.text_input(\"What is the YouTube video URL?\", max_chars=50)\n",
    "    query = st.text_area(\"Ask me about the video\", max_chars=50, key=\"curie\")\n",
    "    submit_button = st.form_submit_button(label=\"Submit\")\n",
    "\n",
    "if submit_button:\n",
    "    db = lch.create_vector_db_from_youtube(video_url)\n",
    "    response = lch.get_response_from_query(db, query)\n",
    "    st.subheader(\"Answer\")\n",
    "    st.text(textwrap.fill(response, width=80))\n",
    "\n",
    "\n",
    "Run the Streamlit App:\n",
    "Run the app in the terminal: streamlit run main.py.\n",
    "Cost Considerations\n",
    "OpenAI API Costs:\n",
    "The cost of using OpenAI's API is relatively low, around $0.002 per 1,000 tokens.\n",
    "For the course, the total cost was less than $0.50.\n",
    "Public App Considerations:\n",
    "To avoid being charged for public use, add a field for users to input their OpenAI API key.\n",
    "Conclusion\n",
    "Langchain is a powerful framework for building applications using large language models. By understanding its key components—LLMs, prompt templates, chains, agents, and indexes—you can create innovative applications that leverage the power of LLMs.\n",
    "Tips\n",
    "Streamlit Course: If interested, consider creating a course on Streamlit to build cool Python interfaces.\n",
    "Environment Variables: Store sensitive information like API keys in environment variables for security.\n",
    "This structured format covers all the information provided in the text, ensuring nothing is left out.\n",
    "\n",
    "\n",
    "Langchain Framework for Building LLM Applications: A Beginner's Course Summary\n",
    "This document summarizes a Langchain course for beginners, covering core concepts, setup, and example applications.\n",
    "I. Course Overview and Requirements\n",
    "Course Goal: To simplify the creation of applications using large language models (LLMs) with Langchain. The course focuses on connecting AI models to various data sources for customized NLP applications.\n",
    "Instructor: Rishabh Kumar, an experienced engineer.\n",
    "Langchain: An open-source framework (Python and TypeScript) enabling developers to combine LLMs (e.g., GPT-4 from OpenAI or Hugging Face) with external computation and data sources. It goes beyond simple text input, allowing integration with databases, PDFs (converted to vector databases), and external APIs.\n",
    "\n",
    "\n",
    "Components\n",
    "Llm wrappers\n",
    "Prompt templates\n",
    "Indexes for information retrieval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Chains\n",
    "Agents\n",
    "\n",
    "II. Setup and Environment\n",
    "OpenAI Account and API Key:\n",
    "Create an OpenAI account at openai.com.\n",
    "Generate an API key (found under your user account -> view API keys -> create new API key). Important: Save this key securely; it's only shown once.\n",
    "Project Setup:\n",
    "Create a project directory (e.g., langchain-llm-app) using the command mkdir langchain-llm-app.\n",
    "Navigate to the directory using cd langchain-llm-app.\n",
    "Open the directory in your code editor.\n",
    "Virtual Environment:\n",
    "Create a virtual environment: python -m venv .venv\n",
    "Activate the virtual environment (Windows): .\\.venv\\Scripts\\activate.ps1\n",
    "Package Installation:\n",
    "Install necessary packages using pip:\n",
    "pip install langchain openai streamlit python-dotenv\n",
    "\n",
    "\n",
    ".env File:\n",
    "Create a .env file in your project directory.\n",
    "Add your OpenAI API key as an environment variable: OPENAI_API_KEY=your_api_key\n",
    "\n",
    "\n",
    "V. Langchain Agents: Interacting with the Environment\n",
    "Agents allow LLMs to interact with external APIs and tools. The course demonstrates this with a function that retrieves the average age of a dog from Wikipedia and performs a calculation:\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import Wikipedia, LLMMath\n",
    "\n",
    "llm = OpenAI(temperature=0.5)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=Wikipedia().run,\n",
    "        description=\"useful for looking up information\",\n",
    "    ),\n",
    "    Tool(name=\"Calculator\", func=LLMMath().run, description=\"useful for math\"),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "result = agent.run(\"What is the average age of a dog? Multiply the age by 3.\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0.5)\n",
    "\n",
    "tools = load_tools([“wikipedia”, “llm-math”], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "result = agent.run(\"What is the average age of a dog? Multiply the age by 3.\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "VI. Indexes and Vector Databases: A YouTube Assistant\n",
    "This section builds a YouTube assistant that answers questions based on a YouTube video transcript. It demonstrates using document loaders, text splitters, and vector databases (using FAISS) to handle large text data.\n",
    "1. Vector Database Creation:\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#import faiss\n",
    "From langchain.llms import OpenAI\n",
    "From langchain import PromptTemplate\n",
    "From langchain.chains import LLMChain\n",
    "From langchain.vectorstores import FAISS\n",
    "From dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "def create_vector_db_from_youtube(video_url):\n",
    "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    transcript = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(transcript)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\tDb = FAISS.from_documents(docs, embeddings)\n",
    "    #db = faiss.index_factory(len(docs[0].page_content.split()), \"IVF10,PQ8\") # Example FAISS index\n",
    "    \n",
    "    #db.add(embeddings.embed_documents([doc.page_content for doc in docs]))\n",
    "    return db\n",
    "\n",
    "\n",
    "2. Querying the Database:\n",
    "def get_response_from_query(db, query, k=4):\n",
    "    #docs = db.search(query, k)\n",
    "\tDocs = db.similarity_search(query)\n",
    "    docs_page_content = \" \".join([doc.page_content for doc in docs])\n",
    "    llm = OpenAI(temperature=0, model_name=\"text-davinci-003\") # Note temperature = 0 for factual answers\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"question\", \"docs\"],\n",
    "        template=\"\"\"You are a helpful YouTube assistant that can answer questions about videos based on the video's transcript.\n",
    "        The following is the question and the video transcript. Use only factual information from the transcript to answer the question. If you feel like you don't have enough information simply say I don't know.\n",
    "\n",
    "        Question: {question}\n",
    "        Video Transcript: {docs}\"\"\",\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    response = chain.run({\"question\": query, \"docs\": docs_page_content})\n",
    "    return response.replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "3. Streamlit Interface for YouTube Assistant: Similar to the pet name generator, a Streamlit app is created to allow users to input a YouTube URL and a question. The create_vector_db_from_youtube and get_response_from_query functions are called to generate the answer.\n",
    "=================================================================\n",
    "\n",
    "\n",
    "Summary of LanqChain Crash Course\n",
    "Introduction to LanqChain\n",
    "LanqChain is a framework designed to build applications on top of Large Language Models (LLMs) like GPT-3.5 or GPT-4. This course will cover the basics of LanqChain and demonstrate how to build a restaurant idea generator application using Streamlit.\n",
    "Key Points\n",
    "LLMs vs. Applications: LLMs are models like GPT-3.5 or GPT-4, while applications like ChatGPT use these models via APIs.\n",
    "Limitations of Direct API Use:\n",
    "Cost associated with API calls.\n",
    "Limited knowledge (e.g., ChatGPT's knowledge cutoff is September 2021).\n",
    "No access to internal organizational data.\n",
    "Need for a Framework: LanqChain provides a framework to build applications using LLMs, supporting various models and integrations.\n",
    "Setting Up LanqChain\n",
    "Steps to Install and Set Up LanqChain\n",
    "Create an OpenAI Account:\n",
    "Go to the OpenAI website and create an account.\n",
    "Obtain an API key from the dashboard.\n",
    "Install Required Modules:\n",
    "pip install langchain\n",
    "pip install openai\n",
    "\n",
    "\n",
    "Set Up Environment Variables:\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key\"\n",
    "\n",
    "\n",
    "Code Snippets\n",
    "Importing and Setting Up OpenAI\n",
    "from langchain import OpenAI\n",
    "\n",
    "# Create an OpenAI model instance\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "Creating a Prompt Template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"I want to open a restaurant for {cuisine} food.\"\n",
    ")\n",
    "\n",
    "\n",
    "Using the Prompt Template\n",
    "formatted_prompt = prompt_template.format(cuisine=\"Mexican\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "\n",
    "Creating a Chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(cuisine=\"American\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "Building a Sequential Chain\n",
    "Steps to Create a Sequential Chain\n",
    "Create Individual Chains:\n",
    "One for generating restaurant names.\n",
    "Another for generating menu items.\n",
    "Combine Chains into a Sequential Chain:\n",
    "Code Snippets\n",
    "Creating Individual Chains\n",
    "# Chain for generating restaurant names\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt_template)\n",
    "\n",
    "# Chain for generating menu items\n",
    "menu_chain = LLMChain(llm=llm, prompt=menu_prompt_template)\n",
    "\n",
    "\n",
    "Combining Chains\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Create a sequential chain\n",
    "sequential_chain = SimpleSequentialChain(chains=[name_chain, menu_chain], input_variables=[\"cuisine\"], output_variables=[\"restaurant_name\", \"menu_items\"])\n",
    "\n",
    "# Run the sequential chain\n",
    "response = sequential_chain(cuisine=\"Indian\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "Building a Streamlit Application\n",
    "Steps to Create a Streamlit Application\n",
    "Install Streamlit:\n",
    "pip install streamlit\n",
    "\n",
    "\n",
    "Create the Streamlit App:\n",
    "Create a main.py file.\n",
    "Import Streamlit and create the UI.\n",
    "Code Snippets\n",
    "Basic Streamlit App\n",
    "import streamlit as st\n",
    "\n",
    "# Title of the app\n",
    "st.title(\"Restaurant Name Generator\")\n",
    "\n",
    "# Sidebar for cuisine selection\n",
    "cuisine = st.sidebar.selectbox(\"Pick a cuisine\", [\"Indian\", \"American\", \"Mexican\"])\n",
    "\n",
    "# Function to generate restaurant name and menu items\n",
    "def get_restaurant_name_and_items(cuisine):\n",
    "    # Placeholder for actual implementation\n",
    "    return {\"restaurant_name\": \"Curry Delight\", \"menu_items\": \"Biryani, Naan, Samosa\"}\n",
    "\n",
    "# Generate and display the restaurant name and menu items\n",
    "if cuisine:\n",
    "    response = get_restaurant_name_and_items(cuisine)\n",
    "    st.header(response[\"restaurant_name\"].strip())\n",
    "    st.write(\"Menu Items:\")\n",
    "    for item in response[\"menu_items\"].split(\",\"):\n",
    "        st.write(f\"- {item.strip()}\")\n",
    "\n",
    "\n",
    "Modularizing the Code\n",
    "Create a langchain_helper.py file for the LanqChain logic.\n",
    "Create a secret_key.py file for the OpenAI API key.\n",
    "Example of langchain_helper.py\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from secret_key import OPENAI_API_KEY\n",
    "\n",
    "# Set up OpenAI\n",
    "llm = OpenAI(temperature=0.7, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define prompt templates\n",
    "name_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"I want to open a restaurant for {cuisine} food.\"\n",
    ")\n",
    "\n",
    "menu_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"Suggest some food menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt_template)\n",
    "menu_chain = LLMChain(llm=llm, prompt=menu_prompt_template)\n",
    "\n",
    "# Create a sequential chain\n",
    "sequential_chain = SimpleSequentialChain(chains=[name_chain, menu_chain], input_variables=[\"cuisine\"], output_variables=[\"restaurant_name\", \"menu_items\"])\n",
    "\n",
    "# Function to generate restaurant name and menu items\n",
    "def generate_restaurant_name_and_items(cuisine):\n",
    "    response = sequential_chain(cuisine=cuisine)\n",
    "    return response\n",
    "\n",
    "# Main function for testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate_restaurant_name_and_items(\"Italian\"))\n",
    "\n",
    "\n",
    "Agents in LanqChain\n",
    "Key Points\n",
    "Agents: Use LLM's reasoning capabilities to perform tasks by connecting with external tools.\n",
    "Tools: Wikipedia, Google Search API, LLM Math, etc.\n",
    "Example: Finding flight options, calculating age, etc.\n",
    "Code Snippets\n",
    "Setting Up Agents\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import WikipediaAPIWrapper, GoogleSerperAPIWrapper\n",
    "\n",
    "# Initialize tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=WikipediaAPIWrapper().run,\n",
    "        description=\"A wrapper around Wikipedia.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"LLM Math\",\n",
    "        func=llm.run,\n",
    "        description=\"A wrapper for performing mathematical calculations.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "response = agent.run(\"When was Elon Musk born and what is his age in 2023?\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "Memory in LanqChain\n",
    "Key Points\n",
    "Memory: Allows chains to remember past conversations.\n",
    "Conversational Buffer Memory: Stores conversation history.\n",
    "Conversational Buffer Window Memory: Limits the size of the conversation history.\n",
    "Code Snippets\n",
    "Conversational Buffer Memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a memory object\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Attach memory to a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(cuisine=\"Indian\")\n",
    "print(chain.memory.buffer)\n",
    "\n",
    "\n",
    "Conversational Buffer Window Memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Create a memory object with a window size\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "# Attach memory to a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(cuisine=\"Indian\")\n",
    "print(chain.memory.buffer)\n",
    "\n",
    "\n",
    "Conclusion\n",
    "This crash course covered the basics of LanqChain, including setting up the environment, building a restaurant idea generator application, creating sequential chains, building a Streamlit application, and exploring agents and memory in LanqChain. For more advanced features and future updates, stay tuned.\n",
    "LangChain Crash Course: Building a Restaurant Idea Generator\n",
    "This document summarizes a crash course video on LangChain, a framework for building applications using Large Language Models (LLMs). The video culminates in building a Streamlit application that generates restaurant names and menu items based on a chosen cuisine.\n",
    "1. Introduction to LangChain and its Advantages\n",
    "LangChain addresses limitations of directly using LLMs like OpenAI's GPT models for application development. These limitations include:\n",
    "Cost: OpenAI API calls are costly.\n",
    "Limited Knowledge: LLMs have knowledge cutoffs (e.g., September 2021).\n",
    "Lack of Access to Internal Data: LLMs cannot access private organizational data.\n",
    "LangChain overcomes these by:\n",
    "Providing a framework to integrate various LLMs (OpenAI, Hugging Face, etc.)\n",
    "Enabling integration with external data sources (Google Search, Wikipedia, databases).\n",
    "Offering a plug-and-play architecture for different models.\n",
    "2. Setup and Initial LangChain Usage\n",
    "2.1 OpenAI API Key:\n",
    "Obtain an API key from your OpenAI account. Store it securely (e.g., in a separate Python file, environment variable). The video uses a secret_key.py file. Example:\n",
    "# secret_key.py\n",
    "OPENAI_API_KEY = \"sk-YOUR_API_KEY_HERE\"\n",
    "\n",
    "\n",
    "# main.py\n",
    "import os\n",
    "From secret_key import OPEN_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-YOUR_API_KEY_HERE\" # or OPEN_API_KEY\n",
    "\n",
    "\n",
    "2.2 Installation:\n",
    "Install necessary packages:\n",
    "pip install langchain openai\n",
    "\n",
    "\n",
    "2.3 Basic LLM Interaction:\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.7) # temperature controls creativity (0-1)\n",
    "print(llm(\"I want to open a restaurant for Indian food and need a fancy name.\"))\n",
    "\n",
    "\n",
    "This code snippet shows a basic interaction with the OpenAI LLM. The temperature parameter influences the creativity of the response.\n",
    "3. Prompt Templates and Chains\n",
    "To avoid hardcoding prompts, use PromptTemplate:\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for it.\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"cuisine\"], template=template)\n",
    "print(prompt.format(cuisine=\"Mexican\"))\n",
    "\n",
    "\n",
    "This code creates a prompt template that can be dynamically populated with different cuisines. The format method substitutes the input variable.\n",
    "LangChain's LLMChain simplifies the process of combining an LLM and a prompt:\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run(\"Italian\"))\n",
    "\n",
    "\n",
    "4. Sequential Chains\n",
    "To perform multiple tasks sequentially, use SimpleSequentialChain or SequentialChain. The output of one chain becomes the input for the next.\n",
    "4.1 SimpleSequentialChain Example:\n",
    "This example generates a restaurant name and then menu items:\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"cuisine\"], template=\"Suggest a fancy name for an {cuisine} restaurant.\"))\n",
    "menu_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"restaurant_name\"], template=\"Suggest some food menu items for {restaurant_name} restaurant, separated by commas.\"))\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[name_chain, menu_chain], verbose=True)\n",
    "print(overall_chain.run(\"Indian\"))\n",
    "\n",
    "\n",
    "4.2 SequentialChain for Multiple Outputs:\n",
    "To obtain multiple outputs from a sequential chain, use SequentialChain. This example gets both the restaurant name and menu items:\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"cuisine\"], template=\"Suggest a fancy name for an {cuisine} restaurant.\"), output_key=\"restaurant_name\")\n",
    "menu_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"restaurant_name\"], template=\"Suggest some food menu items for the {restaurant_name} restaurant, separated by commas.\"), output_key=\"menu_items\")\n",
    "\n",
    "chain = SequentialChain(chains=[name_chain, menu_chain], input_variables=[\"cuisine\"], output_variables=[\"restaurant_name\", \"menu_items\"])\n",
    "print(chain({\"cuisine\": \"Arabic\"}))\n",
    "\n",
    "\n",
    "5. Streamlit Application\n",
    "The video demonstrates creating a Streamlit application using the LangChain functionality developed previously.\n",
    "5.1 Streamlit Setup:\n",
    "Install Streamlit:\n",
    "pip install streamlit\n",
    "\n",
    "\n",
    "6. LangChain Agents\n",
    "Agents enhance LLMs by allowing interaction with external tools (e.g., Wikipedia, search engines).\n",
    "6.1 Agent Example with Wikipedia and Math Tools:\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "print(agent.run(\"What was the US GDP in 2022, plus 5?\"))\n",
    "\n",
    "\n",
    "This example uses the serpapi tool (Google Search API) and llm-math to answer a question requiring external data and calculation. Remember to set up your SERP API key.\n",
    "7. LangChain Memory\n",
    "LLMs are typically stateless. LangChain provides mechanisms to add memory, making them remember past conversations.\n",
    "7.1 Conversational Buffer Memory:\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "print(chain.run(\"Who won the first Cricket World Cup?\"))\n",
    "print(chain.run(\"Who was the captain of the winning team?\"))\n",
    "print(chain.memory)\n",
    "print(chain.memory.buffer)\n",
    "\n",
    "\n",
    "\n",
    "From langchain import ConversationChain\n",
    "Convo = ConversationChain(llm=OpenAI(temperature=0.6))\n",
    "print(convo.prompt.template)\n",
    "\n",
    "convo.run(“who won the first world cup in cricket”)\n",
    "convo.run(“what is 6+6”)\n",
    "\n",
    "print(convo.memory)\n",
    "print(convo.memory.buffer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "Convo = ConversationChain(llm=OpenAI(temperature=0.6), memory= memory)\n",
    "print(convo.prompt.template)\n",
    "\n",
    "convo.run(“who won the first world cup in cricket”)\n",
    "convo.run(“what is 6+6”)\n",
    "\n",
    "print(convo.memory)\n",
    "print(convo.memory.buffer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This adds conversation history to the chain's memory.\n",
    "7.2 Conversational Buffer Window Memory (for cost optimization):\n",
    "To limit memory size and reduce costs, use ConversationBufferWindowMemory:\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) # remember only the last conversation\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "# ... (Run multiple conversations here)\n",
    "\n",
    "\n",
    "This example limits the memory to the last one conversation turn.\n",
    "This comprehensive summary covers all aspects of the LangChain crash course video, including code snippets, explanations, and key concepts. Remember to replace placeholders like \"YOUR_API_KEY_HERE\" with your actual keys.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "======================================================\n",
    "\n",
    "LANGCHAINN DOCUEMENTATION\n",
    "\n",
    "Chat Models Overview\n",
    "Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine-tuning for every scenario.\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output.\n",
    "Features of New Generation Chat Models\n",
    "Tool Calling\n",
    "Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\n",
    "Structured Output\n",
    "A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
    "Multimodality\n",
    "The ability to work with data other than text; for example, images, audio, and video.\n",
    "LangChain Features\n",
    "LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\n",
    "Integrations\n",
    "Integrations with many chat model providers: e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq.\n",
    "Message Formats: Use either LangChain's messages format or OpenAI format.\n",
    "Standard Tool Calling API: Standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\n",
    "Standard API for Structuring Outputs: Via the with_structured_output method.\n",
    "Support for Async Programming, Efficient Batching, Rich Streaming API.\n",
    "Integration with LangSmith: For monitoring and debugging production-grade applications based on LLMs.\n",
    "Additional Features: Standardized token usage, rate limiting, caching, and more.\n",
    "Types of Integrations\n",
    "Official Models: Supported by LangChain and/or model provider. Found in langchain-<provider> packages.\n",
    "Community Models: Mostly contributed and supported by the community. Found in the langchain-community package.\n",
    "Naming Convention\n",
    "LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\n",
    "Note\n",
    "Models that do not include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\n",
    "Interface\n",
    "LangChain chat models implement the BaseChatModel interface. Because BaseChatModel also implements the Runnable Interface, chat models support a standard streaming interface, async programming, optimized batching, and more.\n",
    "Key Methods\n",
    "invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "stream: A method that allows you to stream the output of a chat model as it is generated.\n",
    "batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "with_structured_output: A wrapper around the invoke method for models that natively support structured output.\n",
    "Inputs and Outputs\n",
    "Modern LLMs are typically accessed through a chat model interface that takes messages as input and returns messages as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\n",
    "Message Formats\n",
    "LangChain Message Format: LangChain's own message format, which is used by default and is used internally by LangChain.\n",
    "OpenAI's Message Format: OpenAI's message format.\n",
    "Standard Parameters\n",
    "Parameter\n",
    "Description\n",
    "model\n",
    "The name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\").\n",
    "temperature\n",
    "Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.\n",
    "timeout\n",
    "The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn’t hang indefinitely.\n",
    "max_tokens\n",
    "Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.\n",
    "stop\n",
    "Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.\n",
    "max_retries\n",
    "The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n",
    "api_key\n",
    "The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.\n",
    "base_url\n",
    "The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.\n",
    "rate_limiter\n",
    "An optional BaseRateLimiter to space out requests to avoid exceeding rate limits. See rate-limiting below for more details.\n",
    "\n",
    "Important Notes\n",
    "Standard parameters only apply to model providers that expose parameters with the intended functionality.\n",
    "Standard parameters are currently only enforced on integrations that have their own integration packages (e.g., langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.\n",
    "Chat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model, head to the respective API reference for that model.\n",
    "Tool Calling\n",
    "Chat models can call tools to perform tasks such as fetching data from a database, making API requests, or running custom code. Please see the tool calling guide for more information.\n",
    "Structured Outputs\n",
    "Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely useful for information extraction tasks. Please read more about the technique in the structured outputs guide.\n",
    "Multimodality\n",
    "Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as multimodality.\n",
    "Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\n",
    "Context Window\n",
    "A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\n",
    "If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the memory.\n",
    "The size of the input is measured in tokens which are the unit of processing that the model uses.\n",
    "Advanced Topics\n",
    "Rate-Limiting\n",
    "Many chat model providers impose a limit on the number of requests that can be made in a given time period.\n",
    "If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n",
    "Options to Deal with Rate Limits\n",
    "Try to avoid hitting rate limits by spacing out requests: Chat models accept a rate_limiter parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the how to handle rate limits for more information on how to use this feature.\n",
    "Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a max_retries parameter that can be used to control the number of retries. See the standard parameters section for more information.\n",
    "Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n",
    "Caching\n",
    "Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\n",
    "The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the exact inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\n",
    "An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\n",
    "A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an embedding model to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.\n",
    "However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\n",
    "Please see the how to cache chat model responses guide for more details.\n",
    "Messages\n",
    "Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
    "Each message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\n",
    "LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
    "What is Inside a Message?\n",
    "A message typically consists of the following pieces of information:\n",
    "Role: The role of the message (e.g., \"user\", \"assistant\").\n",
    "Content: The content of the message (e.g., text, multimodal data).\n",
    "Additional Metadata: id, name, token usage, and other model-specific metadata.\n",
    "Role\n",
    "Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n",
    "Role\n",
    "Description\n",
    "system\n",
    "Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.\n",
    "user\n",
    "Represents input from a user interacting with the model, usually in the form of text or other interactive input.\n",
    "assistant\n",
    "Represents a response from the model, which can include text or a request to invoke tools.\n",
    "tool\n",
    "A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.\n",
    "function (legacy)\n",
    "This is a legacy role, corresponding to OpenAI's legacy function-calling API. tool role should be used instead.\n",
    "\n",
    "Content\n",
    "The content of a message text or a list of dictionaries representing multimodal data (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\n",
    "Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\n",
    "For more information see:\n",
    "SystemMessage: for content which should be passed to direct the conversation\n",
    "HumanMessage: for content in the input from the user.\n",
    "AIMessage: for content in the response from the model.\n",
    "Multimodality: for more information on multimodal content.\n",
    "Other Message Data\n",
    "Depending on the chat model provider, messages can include other data such as:\n",
    "ID: An optional unique identifier for the message.\n",
    "Name: An optional name property which allows differentiate between different entities/speakers with the same role. Not all models support this!\n",
    "Metadata: Additional information about the message, such as timestamps, token usage, etc.\n",
    "Tool Calls: A request made by the model to call one or more tools. See tool calling for more information.\n",
    "Conversation Structure\n",
    "The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\n",
    "For example, a typical conversation structure might look like this:\n",
    "User Message: \"Hello, how are you?\"\n",
    "Assistant Message: \"I'm doing well, thank you for asking.\"\n",
    "User Message: \"Can you tell me a joke?\"\n",
    "Assistant Message: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "Please read the chat history guide for more information on managing chat history and ensuring that the conversation structure is correct.\n",
    "LangChain Messages\n",
    "LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
    "LangChain messages are Python objects that subclass from a BaseMessage.\n",
    "The five main message types are:\n",
    "SystemMessage: corresponds to system role\n",
    "HumanMessage: corresponds to user role\n",
    "AIMessage: corresponds to assistant role\n",
    "AIMessageChunk: corresponds to assistant role, used for streaming responses\n",
    "ToolMessage: corresponds to tool role\n",
    "Other important messages include:\n",
    "RemoveMessage: does not correspond to any role. This is an abstraction, mostly used in LangGraph to manage chat history.\n",
    "Legacy FunctionMessage: corresponds to the function role in OpenAI's legacy function-calling API.\n",
    "You can find more information about messages in the API Reference.\n",
    "SystemMessage\n",
    "A SystemMessage is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\").\n",
    "Different chat providers may support system message in one of the following ways:\n",
    "Through a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\n",
    "Through a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\n",
    "No support for system messages: Some models do not support system messages at all.\n",
    "Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider’s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\n",
    "If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the langchain-community package) it is recommended to check the specific documentation for that model.\n",
    "HumanMessage\n",
    "The HumanMessage corresponds to the \"user\" role. A human message represents input from a user interacting with the model.\n",
    "Text Content\n",
    "Most chat models expect the user input to be in the form of text.\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hello, how are you?\")])\n",
    "\n",
    "API Reference: HumanMessage\n",
    "Tip\n",
    "When invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. This is mostly useful for quick testing.\n",
    "model.invoke(\"Hello, how are you?\")\n",
    "\n",
    "Multi-modal Content\n",
    "Some chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.\n",
    "Please see the multimodality guide for more information.\n",
    "AIMessage\n",
    "AIMessage is used to represent a message with the role \"assistant\". This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.\n",
    "from langchain_core.messages import HumanMessage\n",
    "ai_message = model.invoke([HumanMessage(\"Tell me a joke\")])\n",
    "ai_message # <-- AIMessage\n",
    "\n",
    "API Reference: HumanMessage\n",
    "An AIMessage has the following attributes. The attributes which are standardized are the ones that LangChain attempts to standardize across different chat model providers. raw fields are specific to the model provider and may vary.\n",
    "Attribute\n",
    "Standardized/Raw\n",
    "Description\n",
    "content\n",
    "Raw\n",
    "Usually a string, but can be a list of content blocks. See content for details.\n",
    "tool_calls\n",
    "Standardized\n",
    "Tool calls associated with the message. See tool calling for details.\n",
    "invalid_tool_calls\n",
    "Standardized\n",
    "Tool calls with parsing errors associated with the message. See tool calling for details.\n",
    "usage_metadata\n",
    "Standardized\n",
    "Usage metadata for a message, such as token counts. See Usage Metadata API Reference.\n",
    "id\n",
    "Standardized\n",
    "An optional unique identifier for the message, ideally provided by the provider/model that created the message.\n",
    "response_metadata\n",
    "Raw\n",
    "Response metadata, e.g., response headers, logprobs, token counts.\n",
    "\n",
    "Content\n",
    "The content property of an AIMessage represents the response generated by the chat model.\n",
    "The content is either:\n",
    "text: the norm for virtually all chat models.\n",
    "A list of dictionaries: Each dictionary represents a content block and is associated with a type.\n",
    "Used by Anthropic for surfacing agent thought process when doing tool calling.\n",
    "Used by OpenAI for audio outputs. Please see multi-modal content for more information.\n",
    "Important\n",
    "The content property is not standardized across different chat model providers, mostly because there are still few examples to generalize from.\n",
    "AIMessageChunk\n",
    "It is common to stream responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.\n",
    "It is returned from the stream, astream, and astream_events methods of the chat model.\n",
    "For example,\n",
    "for chunk in model.stream([HumanMessage(\"what color is the sky?\")]):\n",
    "    print(chunk)\n",
    "\n",
    "AIMessageChunk follows nearly the same structure as AIMessage, but uses a different ToolCallChunk to be able to stream tool calling in a standardized manner.\n",
    "Aggregating\n",
    "AIMessageChunks support the + operator to merge them into a single AIMessage. This is useful when you want to display the final response to the user.\n",
    "ai_message = chunk1 + chunk2 + chunk3 + ...\n",
    "\n",
    "ToolMessage\n",
    "This represents a message with role \"tool\", which contains the result of calling a tool. In addition to role and content, this message has:\n",
    "a tool_call_id field: which conveys the id of the call to the tool that was called to produce this result.\n",
    "an artifact field: which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\n",
    "Please see tool calling for more information.\n",
    "RemoveMessage\n",
    "This is a special message type that does not correspond to any roles. It is used for managing chat history in LangGraph.\n",
    "Please see the following for more information on how to use the RemoveMessage:\n",
    "Memory conceptual guide\n",
    "How to delete messages\n",
    "OpenAI Format​\n",
    "Inputs​\n",
    "Chat models also accept OpenAI's format as inputs to chat models:\n",
    "chat_model.invoke([\n",
    "   {\n",
    "       \"role\": \"user\",\n",
    "       \"content\": \"Hello, how are you?\",\n",
    "   },\n",
    "   {\n",
    "       \"role\": \"assistant\",\n",
    "       \"content\": \"I'm doing well, thank you for asking.\",\n",
    "   },\n",
    "   {\n",
    "       \"role\": \"user\",\n",
    "       \"content\": \"Can you tell me a joke?\",\n",
    "   }\n",
    "])\n",
    "Outputs​\n",
    "At the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you need OpenAI format for the output as well.\n",
    "The convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format.\n",
    "\n",
    "Summary 2\n",
    "LangChain Chat Models: A Comprehensive Summary\n",
    "This document summarizes key aspects of LangChain's chat model functionality, including features, integrations, interface, key methods, inputs/outputs, standard parameters, advanced topics (rate-limiting, caching), and message structure.\n",
    "I. Overview of LangChain Chat Models\n",
    "LangChain provides a unified interface for interacting with various Large Language Models (LLMs), primarily through a chat model interface. Modern LLMs are accessed via a chat model interface, taking a list of messages as input and returning a message as output. LangChain supports a wide range of providers (Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Older LLMs, which use a string input/string output interface, are also supported but generally discouraged.\n",
    "Key Features of LangChain's Chat Model Support:\n",
    "Consistent Interface: Provides a standardized way to interact with diverse chat models.\n",
    "Tool Calling: Native support for integrating LLMs with external tools and APIs.\n",
    "Structured Output: Ability to receive responses in structured formats like JSON.\n",
    "Multimodality: Support for various data types beyond text (images, audio, video - support varies by model).\n",
    "Monitoring & Debugging: Integration with LangSmith for production-level applications.\n",
    "Asynchronous Programming, Batching, Streaming: Enhanced efficiency and real-time feedback.\n",
    "Official and Community Models: Access to models officially supported by LangChain and community-contributed models. Official models are found in langchain-<provider> packages; community models in langchain-community.\n",
    "Naming Convention: LangChain chat models have class names prefixed with \"Chat\" (e.g., ChatOllama, ChatOpenAI). Models without \"Chat\" or with \"LLM\" as a suffix are typically older, string-based models.\n",
    "II. LangChain Chat Model Interface\n",
    "LangChain chat models adhere to the BaseChatModel interface, which extends the Runnable interface. This enables streaming, async programming, and optimized batching.\n",
    "Key Methods:\n",
    "invoke(messages): Primary method; takes a list of messages as input and returns a list of messages as output.\n",
    "stream(messages): Streams the model's output as it's generated.\n",
    "batch(messages): Processes multiple requests efficiently.\n",
    "bind_tools(tools): Binds tools to the model for tool calling.\n",
    "with_structured_output(schema): Wraps invoke for structured output.\n",
    "Note: The terms \"LLM\" and \"Chat Model\" are often used interchangeably in the documentation, as most modern LLMs are accessed via a chat interface.\n",
    "III. Inputs and Outputs\n",
    "Message Format: LangChain supports two message formats:\n",
    "LangChain Message Format: LangChain's default, internal format.\n",
    "OpenAI Message Format: OpenAI's message format. Conversion between formats is possible using convert_to_openai_messages.\n",
    "Message Structure: Messages consist of:\n",
    "Role: (\"system\", \"user\", \"assistant\", \"tool\")\n",
    "Content: Text or multimodal data.\n",
    "Metadata: Optional additional information (ID, name, token usage, etc.).\n",
    "IV. Standard Parameters\n",
    "Many chat models use standardized parameters:\n",
    "Parameter\n",
    "Description\n",
    "model\n",
    "Model name/identifier (e.g., \"gpt-3.5-turbo\").\n",
    "temperature\n",
    "Controls randomness of output (0.0 = deterministic, 1.0 = creative).\n",
    "timeout\n",
    "Maximum response time (seconds).\n",
    "max_tokens\n",
    "Maximum number of tokens in the response.\n",
    "stop\n",
    "Stop sequences to halt generation.\n",
    "max_retries\n",
    "Maximum retry attempts for failed requests.\n",
    "api_key\n",
    "API key for authentication.\n",
    "base_url\n",
    "API endpoint URL.\n",
    "rate_limiter\n",
    "BaseRateLimiter for controlling request rate.\n",
    "\n",
    "Important Notes:\n",
    "Not all providers support all parameters.\n",
    "Standard parameters are primarily enforced on models with dedicated integration packages.\n",
    "V. Advanced Topics\n",
    "A. Rate-Limiting\n",
    "Strategies for handling rate limits:\n",
    "Spacing out requests: Use rate_limiter parameter during initialization.\n",
    "Handling rate limit errors: Use max_retries parameter.\n",
    "Fallback to another model: Switch to a non-rate-limited model.\n",
    "B. Caching\n",
    "Caching chat model responses is complex due to the low likelihood of exact input repetition. Semantic caching (caching based on meaning) is an alternative, but introduces dependencies and accuracy concerns. Caching may be beneficial for frequently asked questions.\n",
    "VI. LangChain Messages\n",
    "LangChain provides a unified message format with several core message types:\n",
    "SystemMessage: Provides instructions or context to the model. Support varies across providers.\n",
    "HumanMessage: Represents user input (text or multimodal data). Example: model.invoke([HumanMessage(content=\"Hello, how are you?\")])\n",
    "AIMessage: Represents the model's response (text or tool calls). Example: ai_message = model.invoke([HumanMessage(\"Tell me a joke\")])\n",
    "Attributes: content (text or list of dictionaries for multimodal data), tool_calls, invalid_tool_calls, usage_metadata, id, response_metadata.\n",
    "AIMessageChunk: Used for streaming responses. Supports + operator for aggregation.\n",
    "ToolMessage: Represents the result of a tool invocation.\n",
    "RemoveMessage: Used for managing chat history in LangGraph.\n",
    "Legacy FunctionMessage: Legacy type; use ToolMessage instead.\n",
    "VII. OpenAI Format\n",
    "LangChain can accept and (partially) output messages in OpenAI's format:\n",
    "Input Example:\n",
    "chat_model.invoke([\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you for asking.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"}\n",
    "])\n",
    "\n",
    "Output: Output from the model is in LangChain's message format; conversion to OpenAI format is required if needed.\n",
    "VIII. Tool Calling, Structured Outputs, and Multimodality\n",
    "These features are briefly mentioned but detailed explanations are deferred to separate guides within the LangChain documentation. In short:\n",
    "Tool Calling: Enables LLMs to interact with external services.\n",
    "Structured Outputs: Allows responses in specific formats (e.g., JSON).\n",
    "Multimodality: Supports input/output of data types beyond text (limited support currently).\n",
    "IX. Context Window\n",
    "The context window limits the input size (measured in tokens) a model can process. Exceeding the limit can cause errors. Managing conversation history within the context window is crucial for coherent dialogues.\n",
    "This structured summary provides a comprehensive overview of LangChain's chat model capabilities. Refer to the original documentation for detailed explanations and further examples\n",
    "\n",
    "\n",
    "\n",
    "Structured Summary of the Provided Text\n",
    "Introduction to Chat History\n",
    "Chat History:\n",
    "A record of the conversation between the user and the chat model.\n",
    "Maintains context and state throughout the conversation.\n",
    "Each message is associated with a specific role: \"user\", \"assistant\", \"system\", or \"tool\".\n",
    "Conversation Patterns\n",
    "Typical Conversation Flow:\n",
    "System Message: Sets the context for the conversation.\n",
    "User Message: Contains the user's input.\n",
    "Assistant Message: Contains the model's response.\n",
    "Conversation Patterns:\n",
    "Back-and-Forth Conversation: Alternating messages between the user and the assistant.\n",
    "Agentic Workflow: The assistant invokes tools to perform specific tasks.\n",
    "Managing Chat History\n",
    "Key Guidelines:\n",
    "Input Size Limit: Manage chat history to avoid exceeding the context window.\n",
    "Conversation Structure:\n",
    "The first message should be a \"user\" or \"system\" message.\n",
    "The last message should be a \"user\" or \"tool\" message.\n",
    "A \"tool\" message should follow an \"assistant\" message requesting the tool invocation.\n",
    "Tip:\n",
    "Understanding correct conversation structure is essential for implementing memory in chat models.\n",
    "Tool Abstraction in LangChain\n",
    "Key Concepts:\n",
    "Tools encapsulate a function and its schema.\n",
    "Tools can be passed to chat models that support tool calling.\n",
    "Tool Interface:\n",
    "BaseTool Class: Subclass of the Runnable Interface.\n",
    "Key Attributes:\n",
    "name: The name of the tool.\n",
    "description: A description of what the tool does.\n",
    "args: JSON schema for the tool's arguments.\n",
    "Key Methods:\n",
    "invoke: Invokes the tool with the given arguments.\n",
    "ainvoke: Asynchronously invokes the tool.\n",
    "Creating Tools Using the @tool Decorator\n",
    "Code Snippet:\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "Usage:\n",
    "Direct Invocation:\n",
    "multiply.invoke({\"a\": 2, \"b\": 3})\n",
    "\n",
    "\n",
    "Inspecting Tool Schema:\n",
    "print(multiply.name) # multiply\n",
    "print(multiply.description) # Multiply two numbers.\n",
    "print(multiply.args)\n",
    "# {\n",
    "# 'type': 'object',\n",
    "# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}},\n",
    "# 'required': ['a', 'b']\n",
    "# }\n",
    "\n",
    "\n",
    "Note:\n",
    "Direct interaction with tools might not be necessary when using pre-built LangChain components.\n",
    "Configuring the Schema\n",
    "Additional Options:\n",
    "Modify name, description, or parse the function's doc-string to infer the schema.\n",
    "Tool Artifacts\n",
    "Code Snippet:\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def some_tool(...) -> Tuple[str, Any]:\n",
    "    \"\"\"Tool that does something.\"\"\"\n",
    "    ...\n",
    "    return 'Message for chat model', some_artifact\n",
    "\n",
    "Special Type Annotations\n",
    "Type Annotations:\n",
    "InjectedToolArg: Hides the argument from the tool's schema.\n",
    "RunnableConfig: Passes the RunnableConfig object to the tool.\n",
    "InjectedState: Passes the overall state of the LangGraph graph to the tool.\n",
    "InjectedStore: Passes the LangGraph store object to the tool.\n",
    "Annotated: Adds a description to the argument exposed in the tool's schema.\n",
    "Example:\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "\n",
    "@tool\n",
    "def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:\n",
    "    \"\"\"Tool that processes input data.\"\"\"\n",
    "    return f\"User {user_id} processed {input_data}\"\n",
    "\n",
    "RunnableConfig Example:\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "@tool\n",
    "async def some_func(..., config: RunnableConfig) -> ...:\n",
    "    \"\"\"Tool that does something.\"\"\"\n",
    "    # do something with config\n",
    "    ...\n",
    "\n",
    "await some_func.ainvoke(..., config={\"configurable\": {\"value\": \"some_value\"}})\n",
    "\n",
    "Best Practices\n",
    "Designing Tools:\n",
    "Well-named, correctly-documented, and properly type-hinted tools are easier for models to use.\n",
    "Design simple and narrowly scoped tools.\n",
    "Use chat models that support tool-calling APIs.\n",
    "Toolkits\n",
    "Interface:\n",
    "get_tools Method: Returns a list of tools.\n",
    "Code Snippet:\n",
    "# Initialize a toolkit\n",
    "toolkit = ExampleTookit(...)\n",
    "\n",
    "# Get list of tools\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "Conceptual Overview of Tool Calling\n",
    "Key Concepts:\n",
    "Tool Creation: Use the @tool decorator.\n",
    "Tool Binding: Connect the tool to a model that supports tool calling.\n",
    "Tool Calling: The model decides when to call a tool.\n",
    "Tool Execution: Execute the tool using the arguments provided by the model.\n",
    "Recommended Usage:\n",
    "# Tool creation\n",
    "tools = [my_tool]\n",
    "# Tool binding\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "# Tool calling\n",
    "response = model_with_tools.invoke(user_input)\n",
    "\n",
    "Example:\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = tool_calling_model.bind_tools([multiply])\n",
    "\n",
    "Tool Calling Example:\n",
    "result = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n",
    "result.tool_calls\n",
    "{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}\n",
    "\n",
    "Further Reading\n",
    "Conceptual Guide on Tools\n",
    "Model Integrations that Support Tool Calling\n",
    "How-to Guide on Tool Calling\n",
    "LangGraph Documentation on Using ToolNode\n",
    "Best Practices for Tool Design\n",
    "Models with explicit tool-calling APIs perform better.\n",
    "Tools with well-chosen names and descriptions are easier for models to use.\n",
    "Simple, narrowly scoped tools are easier for models to use.\n",
    "Avoid asking the model to select from a large list of tools.\n",
    "Conclusion\n",
    "The text provides a comprehensive overview of managing chat history, creating and using tools in LangChain, and best practices for designing tools for chat models. It includes code snippets, examples, and tips to ensure a thorough understanding of the concepts.\n",
    "summary 2\n",
    "Chat History and Conversation Patterns\n",
    "Chat history is a sequential record of messages between a user and a chat model, maintaining context and state. Each message is tagged with a role (\"user,\" \"assistant,\" \"system,\" or \"tool\").\n",
    "Conversation Patterns:\n",
    "Most conversations begin with a system message setting the context, followed by a user message and an assistant's response. The assistant might directly respond or invoke a tool for specific tasks. Conversations typically alternate between user-assistant exchanges and assistant-tool interactions (agentic workflow).\n",
    "Managing Chat History:\n",
    "Chat models have input size limits; therefore, managing and trimming chat history is crucial to avoid exceeding the context window. Maintain correct conversation structure:\n",
    "The first message is either \"user\" or \"system,\" followed by \"user\" then \"assistant.\"\n",
    "The last message is either \"user\" or a \"tool\" message (tool call result).\n",
    "\"tool\" messages only follow \"assistant\" messages requesting tool invocation.\n",
    "Tip: Correct conversation structure is essential for proper memory implementation in chat models.\n",
    "Tools and the @tool Decorator\n",
    "The LangChain tool abstraction associates a Python function with a schema defining its name, description, and arguments. Tools can be passed to chat models supporting tool calling.\n",
    "Key Concepts:\n",
    "Tools encapsulate functions and their schemas for chat models. The @tool decorator simplifies tool creation:\n",
    "Infers tool name, description, and arguments (customization supported).\n",
    "Supports tools returning artifacts (images, dataframes, etc.).\n",
    "Hides input arguments from the schema using injected tool arguments.\n",
    "Tool Interface (BaseTool):\n",
    "The BaseTool class (subclass of Runnable interface) defines the tool interface:\n",
    "name: Tool name.\n",
    "description: Tool description.\n",
    "args: JSON schema for tool arguments.\n",
    "invoke(): Invokes the tool with given arguments.\n",
    "ainvoke(): Asynchronous invocation (Langchain async programming).\n",
    "Creating Tools with @tool:\n",
    "The recommended method is using the @tool decorator.\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "   \"\"\"Multiply two numbers.\"\"\"\n",
    "   return a * b\n",
    "\n",
    "This code defines a multiply tool using the @tool decorator. The docstring provides the description.\n",
    "API Reference: tool\n",
    "Note: Other methods (subclassing BaseTool or using StructuredTool) exist but the @tool decorator is generally preferred.\n",
    "Using the Tool:\n",
    "multiply.invoke({\"a\": 2, \"b\": 3})\n",
    "\n",
    "This invokes the multiply tool.\n",
    "Inspecting the Tool:\n",
    "print(multiply.name)       # multiply\n",
    "print(multiply.description) # Multiply two numbers.\n",
    "print(multiply.args) \n",
    "# {\n",
    "# 'type': 'object', \n",
    "# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, \n",
    "# 'required': ['a', 'b']\n",
    "# }\n",
    "\n",
    "This demonstrates how to access the tool's properties.\n",
    "Note: Direct tool interaction might not be needed with pre-built LangChain components but is valuable for debugging, testing, and custom LangGraph workflows.\n",
    "Configuring the Schema:\n",
    "The @tool decorator allows schema configuration (name, description, docstring parsing). Refer to the @tool API reference and the custom tools guide for details.\n",
    "Tool Artifacts:\n",
    "Tools' outputs can be fed back to the model. Sometimes, artifacts (custom objects, dataframes, images) should be accessible downstream but not exposed to the model.\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def some_tool(...) -> Tuple[str, Any]:\n",
    "    \"\"\"Tool that does something.\"\"\"\n",
    "    ...\n",
    "    return 'Message for chat model', some_artifact \n",
    "\n",
    "This example uses response_format=\"content_and_artifact\" to handle artifacts. See \"how to return artifacts from tools\" for more details.\n",
    "Special Type Annotations:\n",
    "Special type annotations control tool runtime behavior:\n",
    "InjectedToolArg: Arguments injected manually at runtime (hidden from schema).\n",
    "RunnableConfig: Pass RunnableConfig object to the tool.\n",
    "InjectedState: Pass LangGraph graph state.\n",
    "InjectedStore: Pass LangGraph store object.\n",
    "Annotated[..., \"string literal\"]: Adds a description to an argument exposed in the schema.\n",
    "InjectedToolArg:\n",
    "Hides arguments from the schema, allowing runtime injection.\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "\n",
    "@tool\n",
    "def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:\n",
    "    \"\"\"Tool that processes input data.\"\"\"\n",
    "    return f\"User {user_id} processed {input_data}\"\n",
    "\n",
    "API Reference: tool | InjectedToolArg\n",
    "See \"how to pass runtime values to tools\" for details on InjectedToolArg.\n",
    "RunnableConfig:\n",
    "Allows passing custom runtime values.\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "@tool\n",
    "async def some_func(..., config: RunnableConfig) -> ...:\n",
    "    \"\"\"Tool that does something.\"\"\"\n",
    "    # do something with config\n",
    "    ...\n",
    "\n",
    "await some_func.ainvoke(..., config={\"configurable\": {\"value\": \"some_value\"}})\n",
    "\n",
    "config is not part of the schema and is injected at runtime.\n",
    "Note: Manual RunnableConfig propagation might be needed in Python 3.9/3.10 async environments (not an issue in Python 3.11). See \"Propagation RunnableConfig\" for details.\n",
    "InjectedState and InjectedStore: See respective documentation for details.\n",
    "Best Practices for Tool Design\n",
    "Use clear names, documentation, and type hints.\n",
    "Design simple, narrowly scoped tools.\n",
    "Use chat models with tool-calling APIs.\n",
    "Toolkits\n",
    "Toolkits group tools for specific tasks. They expose a get_tools() method returning a list of tools.\n",
    "Tool Calling\n",
    "Many AI applications interact with systems (databases, APIs) requiring structured input (tool calling, function calling).\n",
    "Conceptual Overview:\n",
    "Tool Creation: Create tools using @tool.\n",
    "Tool Binding: Connect tools to a tool-calling model.\n",
    "Tool Calling: The model decides when to call a tool.\n",
    "Tool Execution: The tool executes using model-provided arguments.\n",
    "Recommended Usage (Pseudo-code):\n",
    "# Tool creation\n",
    "tools = [my_tool]\n",
    "# Tool binding\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "# Tool calling \n",
    "response = model_with_tools.invoke(user_input)\n",
    "\n",
    "Tool Creation (using @tool):\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "API Reference: tool\n",
    "Tool Binding:\n",
    "LangChain provides a standardized interface (bind_tools()) for connecting tools to models.\n",
    "model_with_tools = model.bind_tools(tools_list)\n",
    "\n",
    "Example:\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = tool_calling_model.bind_tools([multiply])\n",
    "\n",
    "Tip: See the model integration page for tool-calling model providers.\n",
    "Tool Calling:\n",
    "The model decides when to use a tool based on input relevance.\n",
    "Example:\n",
    "result = llm_with_tools.invoke(\"Hello world!\") # No tool call\n",
    "result = llm_with_tools.invoke(\"What is 2 multiplied by 3?\") # Tool call\n",
    "result.tool_calls # Contains tool call information\n",
    "\n",
    "Tool Execution:\n",
    "Tools implement the Runnable interface; they can be invoked directly (tool.invoke(args)). LangGraph provides components (e.g., ToolNode) for tool invocation.\n",
    "Best Practices for Tool Calling\n",
    "Use models with explicit tool-calling APIs.\n",
    "Use well-named and described tools.\n",
    "Use simple, narrowly scoped tools.\n",
    "Avoid large tool lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
